[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Screencasts",
    "section": "",
    "text": "If you’re looking for real-world examples of live data analyses, you’ve come to the right place.\nDavid Robinson, a highly experienced Data Scientist, has recorded many screencasts where he analyses data that he’s never seen before. These are fantastic examples of how to think about approaching an analysis. You couldn’t ask for a better mentor!\nThe recordings were done as part of a weekly R programming challenge called TidyTuesday. All code is shared and all datasets are publicly available."
  },
  {
    "objectID": "index.html#all-recordings",
    "href": "index.html#all-recordings",
    "title": "R Screencasts",
    "section": "All recordings",
    "text": "All recordings"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "David Robinson for recording the screencasts.\nAlex Cookson and Eric Fletcher for painstakingly time-stamping and describing all the content.\nOscar Baruffa for creating this website collating it all.\nThomas Mock for maintaining the TidyTuesday challenge.\nCountless others for R, RStudio, Quarto and a gazillion packages that make this all possible.\n\nIf you’d like to keep up to date with this website and other resources from Oscar Baruffa, sign up to the newsletter for posts about R, data and careers."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Not on Twitter but keen to try it out? The free book Twitter for R programmers will get you started.\nWant to learn R? Here’s some free resources to get you started - one book and one video course.\nWant even more R? Check out the collection of over 250 free books covering a wide variety of fields and topics at Big Book of R."
  },
  {
    "objectID": "content_pages/Chopped.html",
    "href": "content_pages/Chopped.html",
    "title": "Chopped",
    "section": "",
    "text": "Notable topics: Data manipulation, Modeling (Linear Regression, Random Forest, and Natural Spline)\nRecorded on: 2020-08-24\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Chopped.html#screencast",
    "href": "content_pages/Chopped.html#screencast",
    "title": "Chopped",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/Chopped.html#timestamps",
    "href": "content_pages/Chopped.html#timestamps",
    "title": "Chopped",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:20\n    \n    geom_histogramggplot\n    \n      Use geom_histogram to visualize the distribution of episode ratings.\n\n    \n  \n  \n    \n      0:6:30\n    \n    geom_pointgeom_lineggplot\n    \n      Use geom_point and geom_line with color = factor(season) to visualize the episode rating for every episode.\n\n    \n  \n  \n    \n      0:7:15\n    \n    group_bysummarizedplyr\n    \n      Use group_by and summarize to show the average rating for each season and the number of episodes in each season.\n\n    \n  \n  \n    \n      0:7:15\n    \n    geom_linegeom_pointggplot\n    \n      Continuing from previous row:\nUse geom_line and geom_point with size = n_episodes  to visualize the average rating for each season with point size indicating the total number of episodes (larger = more episodes, smaller = fewer episodes).\n\n    \n  \n  \n    \n      0:10:55\n    \n    fct_reorderforcats\n    \n      Use fct_reorder  to reorder the episode_name factor levels by sorting along the episode_rating variable.\n\n    \n  \n  \n    \n      0:10:55\n    \n    gnemonolarrangegplotdplyrglue\n    \n      Use geom_point to visualize the top episodes by rating.\nUse the 'glue' package to place season number and episode number before episode name on the y axis.\n\n    \n  \n  \n    \n      0:15:20\n    \n    pivot_longerseparate_rowstidyr\n    \n      Use pivot_longer to combine ingredients into one single column.\nUse separate_rows with sep = \", \"  to separate out the ingredients with each ingredient getting its own row.\n\n    \n  \n  \n    \n      0:18:10\n    \n    fct_lumpfct_reorderforcats\n    \n      Use fct_lump  to lump ingredients together except for the 10 most frequent.\nUse fct_reorder to reorder ingredient factor levels by sorting against n.\n\n    \n  \n  \n    \n      0:18:10\n    \n    geom_colggplot\n    \n      Use geom_col to create a stacked bar plot to visualize the most common ingredients by course.\n\n    \n  \n  \n    \n      0:19:45\n    \n    fct_relevelforcats\n    \n      Use fct_relevel  to reorder course factor levels to appetizer, entree, dessert.\n\n    \n  \n  \n    \n      0:21:00\n    \n    fct_revscale_fill_discreteforcatsggplot\n    \n      Use fct_rev and scale_fill_discrete with guide = guide_legend(reverse = TRUE) to reorder the segments within the stacked bar plot.\n\n    \n  \n  \n    \n      0:23:20\n    \n    add_countdistinctpairwise_corwidyrdplyr\n    \n      Use the widyr package and pairwise_cor to find out what ingredients appear together.\nMentioned: David Robinson - The {widyr} Package YouTube Talk at 2020 R Conference\n\n    \n  \n  \n    \n      0:26:20\n    \n    ggraphgeom_edge_linkgeom_node_pointgeom_node_textwidyrggraphtidygraph\n    \n      Use ggraph , geom_edge_link, geom_node_point, geom_node_text to create an ingredient network diagram to show their makeup and how they interact.\n\n    \n  \n  \n    \n      0:28:00\n    \n    pairwise_countwidyr\n    \n      Use pairwise_count from widyr to count the number of times each pair of items appear together within a group defined by feature.\n\n    \n  \n  \n    \n      0:30:15\n    \n    unitepairwise_counttidyrwidyr\n    \n      Use unite from the tidyr package in order to paste together the episode_course and series_episode columns into one column to figure out if any pairs of ingredients appear together in the same course across episodes.\n\n    \n  \n  \n    \n      0:31:55\n    \n    summarizeminmeanmaxdplyrbase\n    \n      Use summarize with min, mean, max, and n()to create thefirst_season, avg_season, last_seasonandn_appearances` variables.\n\n    \n  \n  \n    \n      0:34:35\n    \n    slicetaildplyrbase\n    \n      Use slice with tail to get the n ingredients that appear in early and late seasons.\n\n    \n  \n  \n    \n      0:35:40\n    \n    semi_joingeom_boxplotfct_reorderdplyrggplotforcats\n    \n      Use geom_boxplot to visualize the distribution of each ingredient across all seasons.\n\n    \n  \n  \n    \n      0:36:50\n    \n    pivot_widerlmlinear_regset_enginefitinitial_splittrainingplotbasevfold_cvfit_resamplestune_gridcollect_metricsgeom_linetidyrand_forestclean_namesstep_nstune_gridcollect_metricsprepjuicetidymodelsstatsrsampleggplotbroomparsnipjanitor\n    \n      Fit predictive models (linear regression , random forest, and natural spline) to determine if episode rating is explained by the ingredients or season.\nUse pivot_wider with values_fill = list(value = 0)) with 1 indicating ingredient was used and 0 indicating it wasn't used.\n\n    \n  \n  \n    \n      1:17:25\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/Cocktails.html",
    "href": "content_pages/Cocktails.html",
    "title": "Cocktails",
    "section": "",
    "text": "Notable topics: Pairwise correlation, Network diagram, Principal component analysis (PCA)\nRecorded on: 2020-05-25\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Cocktails.html#screencast",
    "href": "content_pages/Cocktails.html#screencast",
    "title": "Cocktails",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/Cocktails.html#timestamps",
    "href": "content_pages/Cocktails.html#timestamps",
    "title": "Cocktails",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:6:20\n    \n    fct_reorderforcats\n    \n      Use fct_reorder from the forcats package to reorder the ingredient factor levels along n.\n\n    \n  \n  \n    \n      0:7:40\n    \n    fct_lumpforcats\n    \n      Use fct_lump from the forcats package to lump together all the levels except the n most frequent in the category and ingredient variables.\n\n    \n  \n  \n    \n      0:11:30\n    \n    pairwise_corwidyr\n    \n      Use pairwise_cor from the widyr package to find the correlation between the ingredients.\n\n    \n  \n  \n    \n      0:16:00\n    \n    reorder_withinscale_x_reorderedtidytext\n    \n      Use reorder_within from the tidytext package with scale_x_reordered  to reorder the the columns in each facet.\n\n    \n  \n  \n    \n      0:19:45\n    \n    graph_from_data_frameggraphgeom_edge_linkgeom_node_pointgeom_node_labeligraphggraph\n    \n      Use the ggraph and igraph packages to create a network diagram\n\n    \n  \n  \n    \n      0:25:15\n    \n    extracttidyr\n    \n      Use extract from the tidyr package with regex = (.*) oz to create a new variable amount which doesn't include the oz.\n\n    \n  \n  \n    \n      0:26:40\n    \n    extracttidyr\n    \n      Use extract with regex to turn the strings in the new amount variable into separate columns for the ones, numerator, and denominator.\n\n    \n  \n  \n    \n      0:28:53\n    \n    replace_natidyr\n    \n      Use replace_na from the tidyr package to replace NA with zeros in the ones, numberator, and denominator columns. David ends up reaplcing the zero in the denominator column with ones in order for the calculation to work.\n\n    \n  \n  \n    \n      0:31:49\n    \n    geom_text_repelggrepel\n    \n      Use geom_text_repel from the ggrepel package to add ingredient labels to the geom_point plot.\n\n    \n  \n  \n    \n      0:32:30\n    \n    na_ifdplyr\n    \n      Use na_if from the dplyr package to replace zeros with NA\n\n    \n  \n  \n    \n      0:34:25\n    \n    scale_size_continuousggplot2\n    \n      Use scale_size_continuous with labels = percent_format() to convert size legend values to percent.\n\n    \n  \n  \n    \n      0:36:35\n    \n    graph_from_data_framegeom_node_pointigraphggraph\n    \n      Change the size of the points in the network diagram proportional to n using vertices = ingredient_info within graph_from_data_frame and aes(size = n) within geom_node_point.\n\n    \n  \n  \n    \n      0:48:05\n    \n    widely_svdtop_nabsgeom_colreorder_withinscale_y_reorderedfacet_wrapwidyr\n    \n      Use widely_svd from the widyr package to perform principle component analysis on the ingredients.\n\n    \n  \n  \n    \n      0:52:32\n    \n    paste0base\n    \n      Use paste0  to concatenate PC and dimension in the facet panel titles.\n\n    \n  \n  \n    \n      0:57:00\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/Dolphins.html",
    "href": "content_pages/Dolphins.html",
    "title": "Dolphins",
    "section": "",
    "text": "Notable topics: Survival analysis\nRecorded on: 2018-12-17\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/Dolphins.html#screencast",
    "href": "content_pages/Dolphins.html#screencast",
    "title": "Dolphins",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/Dolphins.html#timestamps",
    "href": "content_pages/Dolphins.html#timestamps",
    "title": "Dolphins",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:6:25\n    \n    \n    \n      Using year function from lubridate package to simplify calculating age of dolphins\n\n    \n  \n  \n    \n      0:8:30\n    \n    countfct_lump\n    \n      Combining count and fct_lump functions to get counts of top 5 species (with other species lumped in \"Other\")\n\n    \n  \n  \n    \n      0:9:55\n    \n    \n    \n      Creating boxplot of species and age\n\n    \n  \n  \n    \n      0:11:50\n    \n    \n    \n      Dealing with different types of NA (double, logical) (he doesn't get it in this case, but it's still useful)\n\n    \n  \n  \n    \n      0:15:30\n    \n    \n    \n      Adding acquisition type as colour dimension to histogram\n\n    \n  \n  \n    \n      0:16:00\n    \n    geom_area\n    \n      Creating a spinogram of acquisition type over time (alternative to histogram) using geom_area\n\n    \n  \n  \n    \n      0:17:25\n    \n    %/%\n    \n      Binning year into decade using truncated division operator %/%\n\n    \n  \n  \n    \n      0:19:10\n    \n    complete\n    \n      Fixing annoying triangular gaps in spinogram using complete function to fill in gaps in data\n\n    \n  \n  \n    \n      0:21:15\n    \n    fct_reorder\n    \n      Using fct_reorder function to reorder acquisition type (bigger categories are placed on the bottom of the spinogram)\n\n    \n  \n  \n    \n      0:23:25\n    \n    geom_vline\n    \n      Adding vertical dashed reference line using geom_vline function\n\n    \n  \n  \n    \n      0:24:05\n    \n    \n    \n      Starting analysis of acquisition location\n\n    \n  \n  \n    \n      0:27:05\n    \n    regex_left_joinfuzzyjoin\n    \n      Matching messy text data with regex to aggregate into a few categories variables with fuzzyjoin package\n\n    \n  \n  \n    \n      0:31:30\n    \n    distinct\n    \n      Using distinct function's .keep_all argument to keep only one row per animal ID\n\n    \n  \n  \n    \n      0:33:10\n    \n    coalesce\n    \n      Using coalesce function to conditionally replace NAs (same functionality as SQL verb)\n\n    \n  \n  \n    \n      0:40:00\n    \n    \n    \n      Starting survival analysis\n\n    \n  \n  \n    \n      0:46:25\n    \n    survfitsurvival\n    \n      Using survfit function from survival package to get a baseline survival curve (i.e., not regressed on any independent variables)\n\n    \n  \n  \n    \n      0:47:30\n    \n    \n    \n      Fixing cases where death year is before birth year\n\n    \n  \n  \n    \n      0:48:30\n    \n    \n    \n      Fixing specification of survfit model to better fit the format of our data (right-censored data)\n\n    \n  \n  \n    \n      0:50:10\n    \n    \n    \n      Built-in plot of baseline survival model (estimation of percentage survival at a given age)\n\n    \n  \n  \n    \n      0:50:30\n    \n    tidybroom\n    \n      Using broom package to tidy the survival model data (which is better for ggplot2 plotting)\n\n    \n  \n  \n    \n      0:52:20\n    \n    \n    \n      Fitting survival curve based on sex\n\n    \n  \n  \n    \n      0:54:25\n    \n    \n    \n      Cox proportional hazards model (to investigate association of survival time and one or more predictors)\n\n    \n  \n  \n    \n      0:55:50\n    \n    \n    \n      Explanation of why dolphins with unknown sex likely have a systematic bias with their data\n\n    \n  \n  \n    \n      0:57:25\n    \n    \n    \n      Investigating whether being born in captivity is associated with different survival rates\n\n    \n  \n  \n    \n      1:00:10\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/Friends.html",
    "href": "content_pages/Friends.html",
    "title": "Friends",
    "section": "",
    "text": "Notable topics: Data Manipulation, Linear Modeling, Pairwise Correlation, Text Mining\nRecorded on: 2020-09-07\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/Friends.html#screencast",
    "href": "content_pages/Friends.html#screencast",
    "title": "Friends",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/Friends.html#timestamps",
    "href": "content_pages/Friends.html#timestamps",
    "title": "Friends",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:7:30\n    \n    countdplyr\n    \n      Use dplyr package's count function to count the unique values of multiple variables.\n\n    \n  \n  \n    \n      0:9:35\n    \n    geom_colfct_reorderggplotforcats\n    \n      Use geom_col to show how many lines of dialogue there is for each character. Use fct_reorder to reorder the speaker factor levels by sorting along n.\n\n    \n  \n  \n    \n      0:12:07\n    \n    semi_joindplyr\n    \n      Use semi_join to join friends dataset with main_cast with by = \"speaker returning all rows from friends with a match in main_cast.\n\n    \n  \n  \n    \n      0:12:30\n    \n    uniteinner_joingluefct_reordertidyrglueforcats\n    \n      Use unite to create the episode_number variable which pastes together season and episode with sep = \".\".\nThen, use inner_join to combine above dataset with friends_info with by = c(\"season\", \"episode\").\nThen, use mutate and the glue package instead to combine { season }.{ episode } { title }.\nThen use fct_reorder(episode_title, season + .001 * episode)  to order it by season first then episode.\n\n    \n  \n  \n    \n      0:15:45\n    \n    geom_pointas.integergeom_textgeom_lineggplotbase\n    \n      Use geom_point to visualize episode_title and us_views_millions.\nUse as.integer to change episode_title to integer class.\nAdd labels to geom_point using geom_text with check_overlap = TRUE so text that overlaps previous text in the same layer will not be plotted.\n\n    \n  \n  \n    \n      0:19:95\n    \n    geom_pointas.integergeom_textgeom_lineggplotbase\n    \n      Run the above plot again using imdb_rating instead of us_views_millions\n\n    \n  \n  \n    \n      0:21:35\n    \n    semi_joingeom_boxplotcoord_flipfct_reordercompletefillscale_x_log10dplyrggplotforcatstidyrtidyr\n    \n      Ahead of modeling:\nUse geom_boxplot to visualize the distribution of speaking for main characters.\nUse the complete function with fill = list(n = 0) to replace existing explicit missing values in the data set.\nDemonstration of how to account for missing imdb_rating values using the fill function with .direction = \"downup\" to keep the imdb rating across the same title.\n\n    \n  \n  \n    \n      0:26:45\n    \n    semi_joinsummarizeadd_countgeom_boxplotgeom_smoothgeom_pointdplyrggplot\n    \n      Ahead of modeling:\nUse summarize with cor(log2(n), imdb_rating) to find the correlation between speaker and imdb rating -- the fact that the correlation is positive for all speakers gives David a suspicion that some episodes are longer than others because they're in 2 parts with higher ratings due to important moments. David addresses this confounding factor by including percentage of lines instead of number of lines.\nVisualize results with geom_boxplot, geom_point with geom_smooth.\n\n    \n  \n  \n    \n      0:34:05\n    \n    spreadacrosssemi_joinlmaovtidyrdplyrstats\n    \n      Use a linear model to predict imdb rating based on various variables.\n\n    \n  \n  \n    \n      0:42:00\n    \n    unnest_tokensanti_joinbind_log_oddssemi_joingeom_colscale_y_reorderedtidytexttidyloggplot\n    \n      Use the tidytext and tidylo packages to see what words are most common amongst characters, and whether they are said more times than would be expected by chance.\nUse geom_col to visualize the most overrepresented words per character according to log_odds_weighted.\n\n    \n  \n  \n    \n      0:54:15\n    \n    unitesemi_joinpairwise_corrwidyrtidyr\n    \n      Use the widyr package and pairwise correlation to determine which characters tend to appear in the same scences together?\nUse geom_col to visualize the correlation between characters.\n\n    \n  \n  \n    \n      1:00:25\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/african-american-achievements.html",
    "href": "content_pages/african-american-achievements.html",
    "title": "African-American Achievements",
    "section": "",
    "text": "Notable topics: plotly interactive timeline, Wikipedia web scraping\nRecorded on: 2020-06-08\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/african-american-achievements.html#screencast",
    "href": "content_pages/african-american-achievements.html#screencast",
    "title": "African-American Achievements",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/african-american-achievements.html#timestamps",
    "href": "content_pages/african-american-achievements.html#timestamps",
    "title": "African-American Achievements",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:8:20\n    \n    fct_reorderforcats\n    \n      Use  fct_reorder from the forcats package to reorder the category factor levels by sorting along  n.\n\n    \n  \n  \n    \n      0:11:35\n    \n    str_removestringr\n    \n      Use str_remove from the stringr package to remove anything after a bracket or parenthesis from the person variable with the regular expression \"[\\\\[\\\\(].*\" David then discusses how web scraping may be a better option than parsing the strings.\n\n    \n  \n  \n    \n      0:12:25\n    \n    str_trimstringr\n    \n      Use str_trim from the stringr package to remove the whitespace from the person variable. David then discusses how web scraping may be a better option than parsing the strings.\n\n    \n  \n  \n    \n      0:15:50\n    \n    ggplotlyplotly\n    \n      Create an interactive plotly timeline.\n\n    \n  \n  \n    \n      0:18:20\n    \n    ylimggplot2\n    \n      Use ylim(c(-.1, 1)) to set scale limits moving the geom_point to the bottom of the graph.\n\n    \n  \n  \n    \n      0:19:30\n    \n    paste0base\n    \n      Use paste0 from base R to concatenate the accomplishment and person with \": \" in between the two displayed in the timeline hover label.\n\n    \n  \n  \n    \n      0:20:30\n    \n    aesggplot2\n    \n      Set y to category in ggplot aesthetics to get 8 separate timelines on one plot, one for each category. Doing this allows David to remove the ylim mentioned above.\n\n    \n  \n  \n    \n      0:22:25\n    \n    tooltipplotly\n    \n      Use the plotly tooltip = text parameter to get just a single line of text in the plotly hover labels.\n\n    \n  \n  \n    \n      0:26:05\n    \n    glueglue\n    \n      Use glue from the glue package to reformat text with \\n included so that the single line of text can now be broken up into 2 separate lines in the hover labels.\n\n    \n  \n  \n    \n      0:33:55\n    \n    separate_rowstidyr\n    \n      Use separate_rows from the tidyr package to separate the occupation_s variable from the science dataset into multiple columns delimited by a semicolon with sep = \"; \"\n\n    \n  \n  \n    \n      0:34:25\n    \n    str_to_titlestringr\n    \n      Use str_to_title from the stringr package to conver the case to title case in the occupation_s variable.\n\n    \n  \n  \n    \n      0:35:15\n    \n    str_detectstringr\n    \n      Use str_detect from the stringr package to detect the presence of statistician from within the occupation_s variable with regex(\"statistician\", ignore_case = TRUE) to perform a case-insensitive search.\n\n    \n  \n  \n    \n      0:41:55\n    \n    read_htmlhtml_nodeshtml_tablesetNamesrvest\n    \n      Use the rvest package with Selector Gadget  to scrape additional information about the individual from their Wikipedia infobox.\n\n    \n  \n  \n    \n      0:49:15\n    \n    mappossiblyread_htmlpurrr\n    \n      Use map and possibly from the purrr package to separate out the downloading of data from parsing the useful information. David then turns the infobox extraction step into an anonymous function using .%>% dot-pipe.\n\n    \n  \n  \n    \n      0:58:40\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/african-american-history.html",
    "href": "content_pages/african-american-history.html",
    "title": "African-American History",
    "section": "",
    "text": "Notable topics: Network diagram, Wordcloud\nRecorded on: 2020-06-15\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/african-american-history.html#screencast",
    "href": "content_pages/african-american-history.html#screencast",
    "title": "African-American History",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/african-american-history.html#timestamps",
    "href": "content_pages/african-american-history.html#timestamps",
    "title": "African-American History",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:6:55\n    \n    fct_lumpforcats\n    \n      Use fct_lump from the forcats package to lump together all the factor levels in ship_name except the n most frequent. Used within filter with ! = \"Other\" to remove other.\n\n    \n  \n  \n    \n      0:8:00\n    \n    fct_reorderforcats\n    \n      use fct_reorder from the forcats package to reorder the ship_name factor levels y sorting along the n_slaves_arrived variable.\n\n    \n  \n  \n    \n      0:10:20\n    \n    geom_vlineggplot2\n    \n      Add geom_vline to geom_histogram to annotate the plot with a vertical line indicating the Revolutionary War and the Civil War.\n\n    \n  \n  \n    \n      0:13:00\n    \n    countdplyr\n    \n      Use truncated division within count to create a new decade variable equal to 10 * (year_arrival %/% 10))\n\n    \n  \n  \n    \n      0:17:20\n    \n    str_truncstringr\n    \n      Use str_trunc from the stringr package to truncate the titles in each facet panel accounting for the slave ports with really long names.\n\n    \n  \n  \n    \n      0:18:05\n    \n    themeggplot2\n    \n      Another option for accounting for long titles in the facet panels is to use strip.text within theme with element_text(size = 6)\n\n    \n  \n  \n    \n      0:26:55\n    \n    ggraphgeom_edge_linkgeom_node_pointgeom_node_textggraph\n    \n      Use the ggraph package to create a network diagram using port_origin and port_arrival.\n\n    \n  \n  \n    \n      0:29:05\n    \n    arrowgrid\n    \n      Use arrow from the grid package to add directional arrows to the points in the network diagram.\n\n    \n  \n  \n    \n      0:29:40\n    \n    scale_edge_size_continuousggraph\n    \n      Use scale_width_size_continuous from the ggraph packge to adjust the size of the points in the network diagram.\n\n    \n  \n  \n    \n      0:35:25\n    \n    summarizemeandplyr\n    \n      Within summarize use mean(n_slaves_arrived, na.rm = TRUE) * n()) to come up with an estimated total numer of slaves since 49% of the data is missing.\n\n    \n  \n  \n    \n      0:48:20\n    \n    geom_colfacet_wrapggplot2\n    \n      Create a faceted stacked percent barplot (spinogram) showing the percentage of black_free, black_slaves, white, and other for each region.\n\n    \n  \n  \n    \n      0:51:00\n    \n    wordcloudgeom_text_wordcloudwordcloudggwordcloud\n    \n      Use the wordcloud package to create a wordcloud with the african_names dataset. David hsa issues with the wordcloud package and opts to use ggwordcloud instead. Also, mentions the worldcloud2 package.\n\n    \n  \n  \n    \n      0:55:20\n    \n    fct_recodeforcats\n    \n      Use fct_recode from the forcats package to change the factor levels for the gender variable while renaming Man = \"Boy\" and Woman = \"Girl\"\n\n    \n  \n  \n    \n      0:57:20\n    \n    reorder_withintidytext\n    \n      Use reorder_within from the tidytext package to reorder the geom_col by n within gender variable for each facet panel.\n\n    \n  \n  \n    \n      0:59:00\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/animal-crossing.html",
    "href": "content_pages/animal-crossing.html",
    "title": "Animal Crossing",
    "section": "",
    "text": "Notable topics: Topic modelling (stm package)\nRecorded on: 2020-05-04\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/animal-crossing.html#screencast",
    "href": "content_pages/animal-crossing.html#screencast",
    "title": "Animal Crossing",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/animal-crossing.html#timestamps",
    "href": "content_pages/animal-crossing.html#timestamps",
    "title": "Animal Crossing",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:05\n    \n    \n    \n      Starting text analysis of critic reviews of Animal Crossing\n\n    \n  \n  \n    \n      0:7:50\n    \n    floor_datelubridate\n    \n      Using floor_date function from lubridate package to round dates down to nearest month (then week)\n\n    \n  \n  \n    \n      0:9:00\n    \n    unnest_tokensanti_jointidytext\n    \n      Using unnest_tokens function and anti_join functions from tidytext package to break reviews into individual words and remove stop words\n\n    \n  \n  \n    \n      0:10:35\n    \n    \n    \n      Taking the average rating associated with individual words (simple approach to gauge sentiment)\n\n    \n  \n  \n    \n      0:12:30\n    \n    geom_line\n    \n      Using geom_line and geom_point to graph ratings over time\n\n    \n  \n  \n    \n      0:14:40\n    \n    mean\n    \n      Using mean function and logical statement to calculate percentages that meet a certain condition\n\n    \n  \n  \n    \n      0:22:30\n    \n    geom_text\n    \n      Using geom_text to visualize what words are associated with positive/negative reviews\n\n    \n  \n  \n    \n      0:27:00\n    \n    \n    \n      Disclaimer that this exploration is not text regression -- wine ratings screencast is a good resource for that\n\n    \n  \n  \n    \n      0:28:30\n    \n    \n    \n      Starting to do topic modelling\n\n    \n  \n  \n    \n      0:30:45\n    \n    stmstm\n    \n      Explanation of stm function from stm package\n\n    \n  \n  \n    \n      0:34:30\n    \n    stmstm\n    \n      Explanation of stm function's output (topic modelling output)\n\n    \n  \n  \n    \n      0:36:55\n    \n    \n    \n      Changing the number of topics from 4 to 6\n\n    \n  \n  \n    \n      0:37:40\n    \n    \n    \n      Explanation of how topic modelling works conceptually\n\n    \n  \n  \n    \n      0:40:55\n    \n    tidybroom\n    \n      Using tidy function from broom package to find which \"documents\" (reviews) were the \"strongest\" representation of each topic\n\n    \n  \n  \n    \n      0:44:50\n    \n    \n    \n      Noting that there might be a scraping issue resulting in review text being repeated\n\n    \n  \n  \n    \n      0:46:05\n    \n    str_sub\n    \n      (Unsuccessfully) Using str_sub function to help fix repeated review text by locating where in the review text starts being repeated\n\n    \n  \n  \n    \n      0:48:20\n    \n    str_replacemap2\n    \n      (Unsuccessfully) Using str_replace and map2_chr functions, as well as regex cpaturing groups to fix repeated text\n\n    \n  \n  \n    \n      0:52:00\n    \n    \n    \n      Looking at the association between review grade and gamma of the topic model (how \"strong\" a review represents a topic)\n\n    \n  \n  \n    \n      0:53:55\n    \n    cor\n    \n      Using cor function with method = \"spearman\" to calculate correlation based on rank instead of actual values\n\n    \n  \n  \n    \n      0:57:35\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/art-collections.html",
    "href": "content_pages/art-collections.html",
    "title": "Art Collections",
    "section": "",
    "text": "Notable topics: geom_area plot, distributions, calculating area (square meters) and ratio (width / height)\nRecorded on: 2021-01-11\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/art-collections.html#screencast",
    "href": "content_pages/art-collections.html#screencast",
    "title": "Art Collections",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/art-collections.html#timestamps",
    "href": "content_pages/art-collections.html#timestamps",
    "title": "Art Collections",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:55\n    \n    clean_namesjanitor\n    \n      Using clean_names to convert variable names from camelcase to snakecase.\n\n    \n  \n  \n    \n      0:4:05\n    \n    fct_reordergeom_colforcatsggplot2\n    \n      Use fct_reorder to reorder geom_col columns in ascending order.\n\n    \n  \n  \n    \n      0:4:50\n    \n    extractseparatetidyr\n    \n      \"Use extract to extract a character column into multiple columns using the regular expression \"\"(.*) on (.*)\"\" at 6:05 David decides to change this to: Use separate with sep = \"\" on \"\" and fill = \"\"left\"\" and extra = \"\"merge\"\" to control what happens when there are not enoughor too many pieces. at 7:10 David decides to change to fill = \"\"right\"\".\"\n\n    \n  \n  \n    \n      0:7:50\n    \n    replace_natidyr\n    \n      Use replace_na to replace NAs with specified values. In this case replace them with Missing.\n\n    \n  \n  \n    \n      0:10:25\n    \n    fct_lumpfilterforcatsdplyr\n    \n      \"Use fct_lump to lump artist and medium levels except for the n most frequent. at 11:30 David decides to use filter(fct_lump(artist, 16) != \"\"Other\"\") to get rid of the artist Other category. \"\n\n    \n  \n  \n    \n      0:13:55\n    \n    geom_areaggplot2\n    \n      \"Create a geom_area plot to show the distribution of paintings by medium over time. At 15:35 David decides to change from count to percentage to make it easier to show the difference in composition using mutate(pct = n / sum).\"\n\n    \n  \n  \n    \n      0:14:20\n    \n    countroundbasedplyr\n    \n      Bucket year variable into decades using round(year -1) to round the year to the nearest 10.\n\n    \n  \n  \n    \n      0:16:35\n    \n    scale_y_continuousscales\n    \n      Use scale_y_continuous(labels = scales::percent) to change y-axis labels to percent format.\n\n    \n  \n  \n    \n      0:18:35\n    \n    facet_wrapgeom_colggplot2\n    \n      Turn the geom_area plot into a faceted geom_col.\n\n    \n  \n  \n    \n      0:21:35\n    \n    mutategroup_bysummarizecompletedplyrtidyr\n    \n      \"Calculate the percentage of artists for each medium per decade. \"\n\n    \n  \n  \n    \n      0:29:20\n    \n    filtermutateggplotgeom_histogramscale_x_log10geom_vlinedplyrggplot2\n    \n      Calculate the distribution of the area (square meters) and ratio (width / height) of the art pieces.\n\n    \n  \n  \n    \n      0:38:25\n    \n    mutatecase_whengeom_areacompletedplyrggplot2\n    \n      Categorize the pieces by shape(landscape, portait, scquare) based on their ratio then plot using geom_area to look at the composition over time.\n\n    \n  \n  \n    \n      0:41:35\n    \n    group_bysummarizefilterggplotgeom_linegeom_pointdplyrggplot2\n    \n      Craete a line plot showing the median ratio by decade over time.\n\n    \n  \n  \n    \n      0:44:15\n    \n    group_bysummarizefilterggplotgeom_linegeom_pointdplyrggplot2\n    \n      Craete a line plot showing the median area by decade over time.\n\n    \n  \n  \n    \n      0:46:05\n    \n    mutatefilterggplotgeom_boxplotscale_y_log10dplyrggplot2\n    \n      Create a boxplot showing the distribution of area over time.\n\n    \n  \n  \n    \n      0:48:25\n    \n    group_bysummarizearrangedplyr\n    \n      Create various summary statistics for the artists such as avg_year, first_year, last_year, n_pieces, median_area, median_ratio`.\n\n    \n  \n  \n    \n      0:51:00\n    \n    filteradd_countmutateggplotgeom_boxplotscale_x_log10geom_vlinegluedplyrggplot2glue\n    \n      Create a boxplot showing the distribution of ratio over time for n amount of artists. Use glue to concatonate number of pieces for each artist ont he y axis.\n\n    \n  \n  \n    \n      0:56:20\n    \n    filteradd_countmutateggplotgeom_boxplotscale_x_log10geom_vlinegluedplyrggplot2glue\n    \n      Create a boxplot showing the distribution of ratio over time for each medium. Use glue to concatonate number of pieces for each medium on the y axis.\n\n    \n  \n  \n    \n      0:59:10\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/australian-animal-outcomes.html",
    "href": "content_pages/australian-animal-outcomes.html",
    "title": "Australian Animal Outcomes",
    "section": "",
    "text": "Notable topics: Data manipulation, Web Scraping (rvest package) and SelectorGadget, Animated Choropleth Map\nRecorded on: 2020-07-20\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/australian-animal-outcomes.html#screencast",
    "href": "content_pages/australian-animal-outcomes.html#screencast",
    "title": "Australian Animal Outcomes",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/australian-animal-outcomes.html#timestamps",
    "href": "content_pages/australian-animal-outcomes.html#timestamps",
    "title": "Australian Animal Outcomes",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:1:20\n    \n    use_tidytemplatetidytuesdayR\n    \n      Using use_tidytemplate to open the project dataset with the package's tidytemplate Rmd\n\n    \n  \n  \n    \n      0:4:30\n    \n    renamedplyr\n    \n      Using rename to rename Total column to total\n\n    \n  \n  \n    \n      0:6:20\n    \n    fct_reorderforcats\n    \n      Using fct_reorder to reorder stacked barplot with weight = sum\n\n    \n  \n  \n    \n      0:7:00\n    \n    fct_lumpforcats\n    \n      Using fct_lump with w = n to lump together outcome factor levels displaying the most frequenct with rest lumped into other\n\n    \n  \n  \n    \n      0:9:15\n    \n    fct_recodeforcats\n    \n      Using fct_recode to combine the factor level In Stock with Currently In Care\n\n    \n  \n  \n    \n      0:12:10\n    \n    fct_reorderforcats\n    \n      Using fct_reorder to reorder facet_wrap panels\n\n    \n  \n  \n    \n      0:13:03\n    \n    scale_y_continuousggplot2scales\n    \n      Using scale_y_continuous with labels = comma to separate digits with comma\n\n    \n  \n  \n    \n      0:14:10\n    \n    completetidyr\n    \n      Using complete to complete account for missing combinations of data where the value is 0 in the released column\n\n    \n  \n  \n    \n      0:16:10\n    \n    maxbase\n    \n      Using max (year) within filter to subset the data displaying only the most recent year\n\n    \n  \n  \n    \n      0:19:30\n    \n    pivot_longertidyr\n    \n      Using pivot_longer to pivot location variables from wide to long\n\n    \n  \n  \n    \n      0:21:45\n    \n    read_htmlhtml_nodesmaprvestjanitor\n    \n      Web Scaraping table from Wikipedia with SelectorGadget and Rvest\n\n    \n  \n  \n    \n      0:25:45\n    \n    str_to_upperstringr\n    \n      Using str_to_upper to upper case the values in the shorthand column\n\n    \n  \n  \n    \n      0:27:13\n    \n    parse_numberreadr\n    \n      Using parse_number to remove commas from population and area columns\n\n    \n  \n  \n    \n      0:28:55\n    \n    bind_rowsdplyr\n    \n      Using bind_rows to bind the two web scraped tables from Wikipedia together by row and column\n\n    \n  \n  \n    \n      0:29:35\n    \n    inner_joindplyr\n    \n      Using inner_join to combine the Wikipedia table with the original data set\n\n    \n  \n  \n    \n      0:29:47\n    \n    mutatedplyr\n    \n      Using mutate to create new per_capita_million column to show outcome on a per million people basis\n\n    \n  \n  \n    \n      0:37:25\n    \n    summarizedplyr\n    \n      Using summarize to create new column pct_euthanized showing percent of cats and dogs euthanized over time. Formula accounts for 0 values thus avoiding a resulting empty vector.\n\n    \n  \n  \n    \n      0:39:10\n    \n    scale_y_continuousggplot2scales\n    \n      Using scale_y_continuous with labels = percent to add percentage sign to y-axis values\n\n    \n  \n  \n    \n      0:42:45\n    \n    read_sfgeom_sfsf_simplifysfggplot2\n    \n      Create a choropleth map of Australia using an Australian States Shapefile using the sf and ggplot2 packages | Troubleshooting begins at 44:25 (downsizing / downsampling with sf_simplify)\n\n    \n  \n  \n    \n      0:55:45\n    \n    transition_manualgganimate\n    \n      Add animation to the map of Australia showing the percent of cats euthanized by region using gganimate\n\n    \n  \n  \n    \n      1:01:35\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/beach-volleyball.html",
    "href": "content_pages/beach-volleyball.html",
    "title": "Beach Volleyball",
    "section": "",
    "text": "Notable topics: Data cleaning, Logistic regression\nRecorded on: 2020-05-18\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/beach-volleyball.html#screencast",
    "href": "content_pages/beach-volleyball.html#screencast",
    "title": "Beach Volleyball",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/beach-volleyball.html#timestamps",
    "href": "content_pages/beach-volleyball.html#timestamps",
    "title": "Beach Volleyball",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:30\n    \n    pivot_longerdplyr\n    \n      Use pivot_longer from the dplyr package to pivot the data set from wide to long.\n\n    \n  \n  \n    \n      0:7:20\n    \n    mutate_atdplyr\n    \n      Use mutate_at from the dplyr package with starts_with to change the class to character for all columns that start with w_ and l_.\n\n    \n  \n  \n    \n      0:8:00\n    \n    separatetidyr\n    \n      Use separate from the tidyr package to separate the name variable into three columns with extra = merge and fill = right.\n\n    \n  \n  \n    \n      0:10:35\n    \n    renamedplyr\n    \n      Use rename from the dplyr package to rename w_player1, w_player2, l_player1, and l_player2.\n\n    \n  \n  \n    \n      0:12:50\n    \n    pivot_widerdplyr\n    \n      Use pivot_wider from the dplyr package to pivot the name variable from long to wide.\n\n    \n  \n  \n    \n      0:15:15\n    \n    str_to_upperstringr\n    \n      Use str_to_upper to convert the winner_loser w and l values to uppercase.\n\n    \n  \n  \n    \n      0:20:25\n    \n    row_numberdplyr\n    \n      Add unique row numbers for each match using mutate with row_number from the dplyr package.\n\n    \n  \n  \n    \n      0:21:20\n    \n    separate_rowstidyr\n    \n      Separate the score values into multiple rows using separate_rows from the tidyr package.\n\n    \n  \n  \n    \n      0:22:45\n    \n    separatetidyr\n    \n      Use separate from the tidyr package to actual scores into two columns, one for the winners score w_score and another for the losers score l_score.\n\n    \n  \n  \n    \n      0:23:45\n    \n    na_ifdplyr\n    \n      Use na_if from the dplyr package to change the Forfeit or other value from the score variable to NA.\n\n    \n  \n  \n    \n      0:24:35\n    \n    str_removestringr\n    \n      Use str_remove from the stringr package to remove scores that include retired.\n\n    \n  \n  \n    \n      0:25:25\n    \n    mutategroup_bysummarizedplyr\n    \n      Determine how many times the winners score w_score is  greter than the losers score l_score at least 1/3 of the time.\n\n    \n  \n  \n    \n      0:28:30\n    \n    summarizedplyr\n    \n      Use summarize from the dplyr package to create the summary statistics including the number of matches, winning percentage, date of first match, date of most recent match.\n\n    \n  \n  \n    \n      0:34:15\n    \n    type_convertreadr\n    \n      Use type_convert from the readr package to convert character class variables to numeric.\n\n    \n  \n  \n    \n      0:35:00\n    \n    summarize_alldplyr\n    \n      Use summarize_all from the dplyr package to calculate the calculate which fraction of the data is not NA.\n\n    \n  \n  \n    \n      0:42:00\n    \n    summarizeinner_joingeom_pointglmcbinddplyrggplot2\n    \n      Use summarize from the dplyr package to determine players number of matches, winning percentage, average attacks, average errors, average kills, average aces, average serve errors, and total rows with data for years prior to 2019.\nThe summary statistics are then used to answer how would we could predict if a player will win in 2019 using geom_point and logistic regression. Initially, David wanted to predict performance based on players first year performance. (NOTE - David mistakingly grouped by year and age. He cathces this around 1:02:00.)\n\n    \n  \n  \n    \n      0:49:25\n    \n    summarizeyearlubridate\n    \n      Use  year from the lubridate package within a group_by to determine the age for each play given their birthdate.\n\n    \n  \n  \n    \n      0:54:30\n    \n    \n    \n      Turn the summary statistics at timestamp 42:00 into a . DOT %>% PIPE function.\n\n    \n  \n  \n    \n      1:04:30\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/beer-production.html",
    "href": "content_pages/beer-production.html",
    "title": "Beer Production",
    "section": "",
    "text": "Notable topics: tidymetrics package demonstration, Animated map (gganimate package)\nRecorded on: 2020-03-31\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/beer-production.html#screencast",
    "href": "content_pages/beer-production.html#screencast",
    "title": "Beer Production",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/beer-production.html#timestamps",
    "href": "content_pages/beer-production.html#timestamps",
    "title": "Beer Production",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:25\n    \n    \n    \n      Asking, \"What ingredients are used in beer?\"\n\n    \n  \n  \n    \n      0:4:40\n    \n    filtermax\n    \n      Using filter and max functions to look at the most recent period of time\n\n    \n  \n  \n    \n      0:7:25\n    \n    pasteymd\n    \n      Using paste and ymd functions (ymd is from lubridate package) to convert year-month field into an date-formatted field\n\n    \n  \n  \n    \n      0:9:20\n    \n    \n    \n      Spotting potential missing or mis-parsed data\n\n    \n  \n  \n    \n      0:13:50\n    \n    \n    \n      Introducing the tidymetrics framework\n\n    \n  \n  \n    \n      0:14:45\n    \n    install_github\n    \n      Using install_github function to install tidymetrics from GitHub\n\n    \n  \n  \n    \n      0:15:25\n    \n    cross_by_dimensionstidymetrics\n    \n      Using cross_by_dimensions function from tidymetrics package to get aggregations at different levels of multiple dimensions\n\n    \n  \n  \n    \n      0:18:10\n    \n    cross_by_periodstidymetrics\n    \n      Using cross_by_periods function from tidymetrics package to also get aggregations for different intervals (e.g, month, quarter, year)\n\n    \n  \n  \n    \n      0:22:00\n    \n    use_metrics_scaffoldtidymetrics\n    \n      Using use_metrics_scaffold function from tidymetrics package to create framework for documenting dimensions in RMarkdown YAML header\n\n    \n  \n  \n    \n      0:24:00\n    \n    create_metricstidymetrics\n    \n      Using create_metrics function from tidymetrics package to save data as a tibble with useful metadata (good for visualizing interactively)\n\n    \n  \n  \n    \n      0:25:15\n    \n    preview_metricsshinymetrics\n    \n      Using preview_metric function from shinymetrics package (still under development as of 2020-04-24) to demonstrate shinymetrics\n\n    \n  \n  \n    \n      0:27:35\n    \n    shinymetrics\n    \n      Succesfuly getting shinymetrics to work\n\n    \n  \n  \n    \n      0:28:25\n    \n    shinymetrics\n    \n      Explanation of the shinymetrics bug David ran into\n\n    \n  \n  \n    \n      0:34:10\n    \n    parse_numberfct_lumpcoalesce\n    \n      Changing order of ordinal variable (e.g., \"1,000 to 10,000\" and \"10,000 to 20,000\") using the parse_number, fct_lump, and coalesce functions\n\n    \n  \n  \n    \n      0:41:25\n    \n    \n    \n      Asking, \"Where is beer produced?\"\n\n    \n  \n  \n    \n      0:46:45\n    \n    sf\n    \n      Looking up sf package documentation to refresh memory on how to draw state borders for a map\n\n    \n  \n  \n    \n      0:48:55\n    \n    match\n    \n      Using match function and state.abb vector (state abbreviations) from sf package to perform a lookup of state names\n\n    \n  \n  \n    \n      0:51:05\n    \n    geom_sfsf\n    \n      Using geom_sf function (and working through some hiccoughs) to create a choropleth map\n\n    \n  \n  \n    \n      0:52:30\n    \n    theme_mapggthemes\n    \n      Using theme_map function from ggthemes package to get more appropriate styling for maps\n\n    \n  \n  \n    \n      0:55:40\n    \n    \n    \n      Experimenting with how to get the legend to display in the bottom right corner\n\n    \n  \n  \n    \n      0:58:25\n    \n    gganimate\n    \n      Starting to build an animation of consumption patterns over time using gganimate package\n\n    \n  \n  \n    \n      1:03:40\n    \n    gganimate\n    \n      Getting the year being animated to show up in the title of a gganimate map\n\n    \n  \n  \n    \n      1:05:40\n    \n    \n    \n      Summary of screencast\n\n    \n  \n  \n    \n      1:06:50\n    \n    \n    \n      Spotting a mistake in a group_by call causing the percentages not to add up properly\n\n    \n  \n  \n    \n      1:09:10\n    \n    tidymetrics\n    \n      Brief extra overview of tidymetrics code"
  },
  {
    "objectID": "content_pages/beyonce-and-taylor-swift-lyrics.html",
    "href": "content_pages/beyonce-and-taylor-swift-lyrics.html",
    "title": "Beyonce and Taylor Swift Lyrics",
    "section": "",
    "text": "Notable topics: Text analysis, tf_idf, Log odds ratio, Diverging bar graph, Lollipop graph\nRecorded on: 2020-09-28\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/beyonce-and-taylor-swift-lyrics.html#screencast",
    "href": "content_pages/beyonce-and-taylor-swift-lyrics.html#screencast",
    "title": "Beyonce and Taylor Swift Lyrics",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/beyonce-and-taylor-swift-lyrics.html#timestamps",
    "href": "content_pages/beyonce-and-taylor-swift-lyrics.html#timestamps",
    "title": "Beyonce and Taylor Swift Lyrics",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:7:50\n    \n    fct_reorderforcats\n    \n      Use fct_reorder from the forcats package to reorder title factor levels by sorting along the sales variable in geom_col plot.\n\n    \n  \n  \n    \n      0:8:10\n    \n    labelsscales\n    \n      Use labels = dollar from the scales package to format the geom_col x-axis values as currency.\n\n    \n  \n  \n    \n      0:11:15\n    \n    rename_allstr_to_lowerdplyrstringr\n    \n      Use rename_all(str_to_lower) to convert variable names to lowercase.\n\n    \n  \n  \n    \n      0:12:45\n    \n    unnest_tokenstidytext\n    \n      Use unnest_tokens from the tidytext package to split the lyrics into one-lyric-per-row.\n\n    \n  \n  \n    \n      0:13:00\n    \n    anti_joindplyr\n    \n      Use anti_join from the tidytext package to find the most common words int he lyrics without stop_words.\n\n    \n  \n  \n    \n      0:15:15\n    \n    bind_tf_idftidytext\n    \n      Use bind_tf_idf from the tidytext package to determine tf - the proportion each word has in each album and idf - how specific each word is to each particular album.\n\n    \n  \n  \n    \n      0:17:45\n    \n    reorder_withinscale_y_reorderedslice_maxtidytextdplyr\n    \n      Use reorder_within with scale_y_reordered in order to reorder the bars within each facet panel. David replaces top_n with slice_max from the dplyr package in order to show the top 10 words with ties = FALSE.\n\n    \n  \n  \n    \n      0:20:45\n    \n    bind_log_oddstidylo\n    \n      Use bind_log_odds from the tidylo package to calculate the log odds ratio of album and words, that is how much more common is the word in a specific album than across all the other albums.\n\n    \n  \n  \n    \n      0:23:10\n    \n    filterstr_lengthdplyrstringr\n    \n      Use filter(str_length(word) <= 3) to come up with a list in order to remove common filler words like ah, uh, ha, ey, eeh, and huh.\n\n    \n  \n  \n    \n      0:27:00\n    \n    distinctmdystr_removedplyrlubridatestringr\n    \n      Use mdy from the lubridate package and str_remove(released, \" \\\\(.*)\")) from the stringr package to parse the dates in the released variable.\n\n    \n  \n  \n    \n      0:28:15\n    \n    inner_joinfct_recodedplyrforcats\n    \n      Use inner_join from the dplyr package to join taylor_swift_words with release_dates.\nDavid ends up having to use fct_recode since the albums reputation and folklore were nor lowercase in a previous table thus excluding them from the inner_join.\n\n    \n  \n  \n    \n      0:28:30\n    \n    fct_reordergeom_colforcatsggplot2\n    \n      Use fct_reorder from the forcats package to reorder album factor levels by sorting along the released variable to be used in the faceted geom_col.\n\n    \n  \n  \n    \n      0:34:40\n    \n    bind_rowsunnest_tokensdplyrtidytext\n    \n      Use bind_rows from hte dplyr package to bind ts with beyonce with unnest_tokens from the tidytext package to get one lyric per row per artist.\n\n    \n  \n  \n    \n      0:38:40\n    \n    bind_log_oddstidylo\n    \n      Use bind_log_odds to figure out which words are more likely to come from a Taylor Swift or Beyonce song?\n\n    \n  \n  \n    \n      0:41:10\n    \n    slice_maxgeom_colifelsefct_reorderdplyrggplot2forcats\n    \n      Use slice_max from the dplyr package to select the top 100 words by num_words_total and then the top 25 by log_odds_weighted. Results are used to create a diverging bar chart showing which words are most common between Beyonce and Taylor Swift songs.\n\n    \n  \n  \n    \n      0:44:40\n    \n    scale_x_continuousggplot2\n    \n      Use scale_x_continuous to make the log_odds_weighted scale more interpretable.\n\n    \n  \n  \n    \n      0:50:45\n    \n    geom_colgeom_pointgeom_vlineggplot2\n    \n      Take the previous plot and turn it into a lollipop graph with geom_point(aes(size = num_words_total, color = direction))\n\n    \n  \n  \n    \n      0:53:05\n    \n    ifelsebase\n    \n      Use ifelse to change the 1x value on the x-axis to same.\n\n    \n  \n  \n    \n      0:54:15\n    \n    pivot_widerclean_namesgeom_ablinegeom_pointslice_maxscale_y_log_10scale_x_log_10geom_texttidyrggplot2dplyr\n    \n      Create a geom_point with geom_abline to show the most popular words they use in common.\n\n    \n  \n  \n    \n      1:01:55\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/big-mac-index.html",
    "href": "content_pages/big-mac-index.html",
    "title": "Big Mac Index",
    "section": "",
    "text": "Notable topics: Data manipulation, Pairwise correlation\nRecorded on: 2020-12-21\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/big-mac-index.html#screencast",
    "href": "content_pages/big-mac-index.html#screencast",
    "title": "Big Mac Index",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/big-mac-index.html#timestamps",
    "href": "content_pages/big-mac-index.html#timestamps",
    "title": "Big Mac Index",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:45\n    \n    add_countdplyr\n    \n      Use the add_count function from the dplyr package with name = \"country_total\" to count the number of observations by group in the name variable.\n\n    \n  \n  \n    \n      0:5:55\n    \n    filtermaxdplyrbase\n    \n      Use filter from the dplyr package with country_total == max(country_total) to filter the data for countries where every data point is provided.\n\n    \n  \n  \n    \n      0:6:30\n    \n    renamedplyr\n    \n      Use the rename function from the dplyr package to rename the name variable to country_name.\n\n    \n  \n  \n    \n      0:6:45\n    \n    themeggplot2\n    \n      Use theme(legend.position = \"none\") to hide the legend generated by the geom_line plot.\n\n    \n  \n  \n    \n      0:7:00\n    \n    expand_limitsggplot2\n    \n      Use the expand_limits function from the ggplot2 package with y = 0 so that each facet panel has a y-axis that starts at the same point, in this case 0.\n\n    \n  \n  \n    \n      0:8:30\n    \n    fct_reorderforcats\n    \n      Reorder facet panels using the fct_reorder from the forcats package with a function passed in to the .fun argument to calculate the ratio between max and min values in the local_price variable.  At 12:00, David changes from using max and min to last and first to calculate the Big Mac inflation rate.\n\n    \n  \n  \n    \n      0:13:20\n    \n    scale_x_log10ggplot2\n    \n      Use scale_x_log10 from the ggplot2 package to change the breaks for the x-axis while also applying a log10 tranformation.\n\n    \n  \n  \n    \n      0:15:05\n    \n    geom_textpaste0ggplot2base\n    \n      Use geom_text from the from the ggplot2 with paste0 package to add labels to each bar in the plot indicating how many time X the price of a Big Mac increased from 2000 to 2020.\n\n    \n  \n  \n    \n      0:28:10\n    \n    geom_lineggplot2\n    \n      Add two lines to a plot using 2 geom_line  with  color = argument and y= argument to distinguish between the two lines.\n\n    \n  \n  \n    \n      0:34:05\n    \n    geom_hlineggplot2\n    \n      Use geom_hline from the ggplot2 package to add horizontal reference line to each facet panel.\n\n    \n  \n  \n    \n      0:35:40\n    \n    themeggplot2\n    \n      Use theme from the ggplot2 package with axis.text.x = element_text(angle = 90, hjust = 1) to rmake the x-axis labels horizontal in order to avoid overcrowding.\n\n    \n  \n  \n    \n      0:38:25\n    \n    geom_textgeom_text_repelggplot2ggrepel\n    \n      Use geom_text to add country names to each point in geom_point plot. David then opts to use geom_text_repel from the ggrepel package instead to avoid overcrowding.\n\n    \n  \n  \n    \n      0:38:40\n    \n    geom_smoothggplot2\n    \n      Use geom_smooth from the ggplot2 package with lm smoothing method to help show the linear trend when comparing gdp_dollar to usd_raw.\n\n    \n  \n  \n    \n      0:47:00\n    \n    transition_manualtransition_timegganimate\n    \n      Use the gganimate package to animate the GDP per capital versus adjusted big mac index relative to USD over time.\n\n    \n  \n  \n    \n      0:53:05\n    \n    str_to_upperstr_removestringr\n    \n      Use str_to_upper and str_remove to remove _adjusted from base_currency while uppercasing the characters that remain.\n\n    \n  \n  \n    \n      0:58:05\n    \n    pairwise_corwidyr\n    \n      Use pairwise_cor from the widyr package to perform pairwise correlation to figure out which countries Big Mac prices tend to move together over time.\n\n    \n  \n  \n    \n      1:00:50\n    \n    \n    \n      Screencast summary."
  },
  {
    "objectID": "content_pages/bird-collisions.html",
    "href": "content_pages/bird-collisions.html",
    "title": "Bird Collisions",
    "section": "",
    "text": "Notable topics: Bootstrapping\nRecorded on: 2019-05-02\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/bird-collisions.html#screencast",
    "href": "content_pages/bird-collisions.html#screencast",
    "title": "Bird Collisions",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/bird-collisions.html#timestamps",
    "href": "content_pages/bird-collisions.html#timestamps",
    "title": "Bird Collisions",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:45\n    \n    \n    \n      Analyzing when NAs appear in a dimension\n\n    \n  \n  \n    \n      0:7:30\n    \n    gather\n    \n      Looking at multiple categorical variable at the same time by gathering them into one column and eventually graphing each as a different facet\n\n    \n  \n  \n    \n      0:9:30\n    \n    \n    \n      Re-order facet graphs according to which ones have the fewest categories in them to ones that have the most\n\n    \n  \n  \n    \n      0:20:45\n    \n    \n    \n      Geometric mean for estimating counts when there are a lot of low values (1-3 bird collisions, in this case)\n\n    \n  \n  \n    \n      0:23:15\n    \n    \n    \n      Filling in \"blank\" observations where there were no observations made\n\n    \n  \n  \n    \n      0:27:00\n    \n    \n    \n      Using log+1 to convert a dimension with values of 0 into a log scale\n\n    \n  \n  \n    \n      0:29:00\n    \n    \n    \n      Adding confidence bounds for data using a geometric mean (where he first gets the idea of bootstrapping)\n\n    \n  \n  \n    \n      0:32:00\n    \n    \n    \n      Actual coding of bootstrap starts\n\n    \n  \n  \n    \n      0:38:30\n    \n    \n    \n      Adding confidence bounds using bootstrap data\n\n    \n  \n  \n    \n      0:42:00\n    \n    \n    \n      Investigating potential confounding variables\n\n    \n  \n  \n    \n      0:44:15\n    \n    \n    \n      Discussing approaches to dealing with confounding variables\n\n    \n  \n  \n    \n      0:46:45\n    \n    complete\n    \n      Using complete function to get explicit NA values"
  },
  {
    "objectID": "content_pages/board-game-reviews.html",
    "href": "content_pages/board-game-reviews.html",
    "title": "Board Game Reviews",
    "section": "",
    "text": "Notable topics: LASSO regression (glmnet package)\nRecorded on: 2019-03-14\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/board-game-reviews.html#screencast",
    "href": "content_pages/board-game-reviews.html#screencast",
    "title": "Board Game Reviews",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/board-game-reviews.html#timestamps",
    "href": "content_pages/board-game-reviews.html#timestamps",
    "title": "Board Game Reviews",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:50\n    \n    \n    \n      Starting EDA (exploratory data analysis) with counts of categorical variables\n\n    \n  \n  \n    \n      0:7:25\n    \n    scale_x_log10\n    \n      Specifying scale_x_log10 function's breaks argument to get sensisble tick marks for time on histogram\n\n    \n  \n  \n    \n      0:8:45\n    \n    geom_histogram\n    \n      Tweaking geom_histogram function's binwidth argument to get something that makes sense for log scale\n\n    \n  \n  \n    \n      0:10:10\n    \n    separate_rows\n    \n      Using separate_rows to break down comma-separated values for three different categorical variables\n\n    \n  \n  \n    \n      0:15:55\n    \n    top_n\n    \n      Using top_n to get top 20 observations from each of several categories (not quite right, fixed at 17:47)\n\n    \n  \n  \n    \n      0:16:15\n    \n    facet_wrap\n    \n      Troubleshooting various issues with facetted graph (e.g., ordering, values appearing in multiple categories)\n\n    \n  \n  \n    \n      0:19:55\n    \n    lm\n    \n      Starting prediction of average rating with a linear model\n\n    \n  \n  \n    \n      0:20:50\n    \n    \n    \n      Splitting data into train/test sets (training/holdout)\n\n    \n  \n  \n    \n      0:22:55\n    \n    \n    \n      Investigating relationship between max number of players and average rating (to determine if it should be in linear model)\n\n    \n  \n  \n    \n      0:25:05\n    \n    \n    \n      Exploring average rating over time (\"Do newer games tend to be rated higher/lower?\")\n\n    \n  \n  \n    \n      0:27:35\n    \n    \n    \n      Discussing necessity of controlling for year a game was published in the linear model\n\n    \n  \n  \n    \n      0:28:30\n    \n    \n    \n      Non-model approach to exploring relationship between game features (e.g., card game, made in Germany) on average rating\n\n    \n  \n  \n    \n      0:30:50\n    \n    geom_boxplot\n    \n      Using geom_boxplot function to create boxplot of average ratings for most common game features\n\n    \n  \n  \n    \n      0:34:05\n    \n    unite\n    \n      Using unite function to combine multiple variables into one\n\n    \n  \n  \n    \n      0:37:25\n    \n    \n    \n      Introducing Lasso regression as good option when you have many features likely to be correlated with one another\n\n    \n  \n  \n    \n      0:38:15\n    \n    \n    \n      Writing code to set up Lasso regression using glmnet and tidytext packages\n\n    \n  \n  \n    \n      0:40:05\n    \n    \n    \n      Adding average rating to the feature matrix (warning: method is messy)\n\n    \n  \n  \n    \n      0:41:40\n    \n    setdiff\n    \n      Using setdiff function to find games that are in one set, but not in another (while setting up matrix for Lasso regression)\n\n    \n  \n  \n    \n      0:44:15\n    \n    \n    \n      Spotting the error stemming from the step above (calling row names from the wrong data)\n\n    \n  \n  \n    \n      0:45:45\n    \n    glmnetglmnet\n    \n      Explaining what a Lasso regression does, including the penalty parameter lambda\n\n    \n  \n  \n    \n      0:48:35\n    \n    cv.glmnetglmnet\n    \n      Using a cross-validated Lasso model to choose the level of the penalty parameter (lambda)\n\n    \n  \n  \n    \n      0:51:35\n    \n    \n    \n      Adding non-categorical variables to the Lasso model to control for them (e.g., max number of players)\n\n    \n  \n  \n    \n      0:55:15\n    \n    unite\n    \n      Using unite function to combine multiple variables into one, separated by a colon\n\n    \n  \n  \n    \n      0:58:45\n    \n    \n    \n      Graphing the top 20 coefficients in the Lasso model that have the biggest effect on predicted average rating\n\n    \n  \n  \n    \n      1:00:55\n    \n    yardstick\n    \n      Mentioning the yardstick package as a way to evaluate the model's performance\n\n    \n  \n  \n    \n      1:01:15\n    \n    \n    \n      Discussing drawbacks of linear models like Lasso (can't do non-linear relationships or interaction effects)"
  },
  {
    "objectID": "content_pages/bob-ross-paintings.html",
    "href": "content_pages/bob-ross-paintings.html",
    "title": "Bob Ross Paintings",
    "section": "",
    "text": "Notable topics: Network graphs, Principal Component Analysis (PCA)\nRecorded on: 2019-08-11\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/bob-ross-paintings.html#screencast",
    "href": "content_pages/bob-ross-paintings.html#screencast",
    "title": "Bob Ross Paintings",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/bob-ross-paintings.html#timestamps",
    "href": "content_pages/bob-ross-paintings.html#timestamps",
    "title": "Bob Ross Paintings",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:40\n    \n    clean_namesjanitor\n    \n      Using clean_names function in janitor package to get field names to snake_case\n\n    \n  \n  \n    \n      0:1:50\n    \n    gather\n    \n      Using gather function to get wide elements into tall (tidy) format\n\n    \n  \n  \n    \n      0:2:35\n    \n    str_to_titlestr_replace\n    \n      Cleaning text (str_to_title, str_replace) to get into nicer-to-read format\n\n    \n  \n  \n    \n      0:3:30\n    \n    str_remove_all\n    \n      Using str_remove_all function to trim trimming quotation marks and backslashes\n\n    \n  \n  \n    \n      0:4:40\n    \n    extract\n    \n      Using extract function to extract the season number and episode number from episode field; uses regex capturing groups\n\n    \n  \n  \n    \n      0:14:00\n    \n    add_count\n    \n      Using add_count function's name argument to specify field's name\n\n    \n  \n  \n    \n      0:15:35\n    \n    \n    \n      Getting into whether the elements of Ross's paintings changed over time (e.g., are mountains more/less common over time?)\n\n    \n  \n  \n    \n      0:20:00\n    \n    broom\n    \n      Quick point: could have used logistic regression to see change over time of elements\n\n    \n  \n  \n    \n      0:21:10\n    \n    widyr\n    \n      Asking, \"What elements tends to appear together?\" prompting clustering analysis\n\n    \n  \n  \n    \n      0:22:15\n    \n    pairwise_corwidyr\n    \n      Using pairwise_cor to see which elements tend to appear together\n\n    \n  \n  \n    \n      0:22:50\n    \n    \n    \n      Discussion of a blind spot of pairwise correlation (high or perfect correlation on elements that only appear once or twice)\n\n    \n  \n  \n    \n      0:28:05\n    \n    \n    \n      Asking, \"What are clusters of elements that belong together?\"\n\n    \n  \n  \n    \n      0:28:30\n    \n    ggraphigraph\n    \n      Creating network plot using ggraph and igraph packages\n\n    \n  \n  \n    \n      0:30:15\n    \n    \n    \n      Reviewing network plot for interesting clusters (e.g., beach cluster, mountain cluster, structure cluster)\n\n    \n  \n  \n    \n      0:31:55\n    \n    \n    \n      Explanation of Principal Component Analysis (PCA)\n\n    \n  \n  \n    \n      0:34:35\n    \n    \n    \n      Start of actual PCA coding\n\n    \n  \n  \n    \n      0:34:50\n    \n    acastreshape2\n    \n      Using acast function to create matrix of painting titles x painting elements (initially wrong, corrected at 36:30)\n\n    \n  \n  \n    \n      0:36:55\n    \n    tcolSumscolMeans\n    \n      Centering the matrix data using t function (transpose of matrix), colSums function, and colMeans function\n\n    \n  \n  \n    \n      0:38:15\n    \n    svd\n    \n      Using svd function to performn singular value decomposition, then tidying with broom package\n\n    \n  \n  \n    \n      0:39:55\n    \n    \n    \n      Exploring one principal component to get a better feel for what PCA is doing\n\n    \n  \n  \n    \n      0:43:20\n    \n    reorder_withintidytext\n    \n      Using reorder_within function to re-order factors within a grouping\n\n    \n  \n  \n    \n      0:48:00\n    \n    \n    \n      Exploring different matrix names in PCA (u, v, d)\n\n    \n  \n  \n    \n      0:56:50\n    \n    \n    \n      Looking at top 6 principal components of painting elements\n\n    \n  \n  \n    \n      0:57:45\n    \n    \n    \n      Showing percentage of variation that each principal component is responsible for"
  },
  {
    "objectID": "content_pages/broadway-musicals.html",
    "href": "content_pages/broadway-musicals.html",
    "title": "Broadway Musicals",
    "section": "",
    "text": "Notable topics: Creating an interactive dashboard with shinymetrics and tidymetrics, moving windows, period aggregation\nRecorded on: 2020-04-27\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/broadway-musicals.html#screencast",
    "href": "content_pages/broadway-musicals.html#screencast",
    "title": "Broadway Musicals",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/broadway-musicals.html#timestamps",
    "href": "content_pages/broadway-musicals.html#timestamps",
    "title": "Broadway Musicals",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:8:15\n    \n    renamecross_by_periodstidymetrics\n    \n      Use the cross_by_periods  function from the tidymetrics package to aggregate data over time (month, quarter, and year) then visualize with geom_line.\n\n    \n  \n  \n    \n      0:14:00\n    \n    cross_by_periodstidymetrics\n    \n      Use the cross_by_periods  function from the tidymetrics  package with windows = c(28)) to create a 4-week rolling average across month, quarter, and year.\n\n    \n  \n  \n    \n      0:21:50\n    \n    use_metrics_scaffoldcreate_metricsshinymetricsTidaymetrics\n    \n      Create and interactive dashboard using the shinymetrics and tidymetrics packages.\n\n    \n  \n  \n    \n      0:25:00\n    \n    str_removestringr\n    \n      Use the str_remove function from the stringr package to remove matched pattern in a string.\n\n    \n  \n  \n    \n      0:25:20\n    \n    cross_by_dimensionstidymetrics\n    \n      Use the cross_by_dimensions function from the tidymetrics package which acts as an extended group_by that allows complete summaries across each individual dimension and possible combinations.\n\n    \n  \n  \n    \n      0:41:25\n    \n    shinybones\n    \n      Use the shinybones package to create an interactive dashboard to visualize all 3 metrics at the same time."
  },
  {
    "objectID": "content_pages/car-fuel-efficiency.html",
    "href": "content_pages/car-fuel-efficiency.html",
    "title": "Car Fuel Efficiency",
    "section": "",
    "text": "Notable topics: Natural splines for regression\nRecorded on: 2019-10-14\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/car-fuel-efficiency.html#screencast",
    "href": "content_pages/car-fuel-efficiency.html#screencast",
    "title": "Car Fuel Efficiency",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/car-fuel-efficiency.html#timestamps",
    "href": "content_pages/car-fuel-efficiency.html#timestamps",
    "title": "Car Fuel Efficiency",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:3:20\n    \n    selectsortcolnames\n    \n      Using select and sort and colnames functions to sort variables in alphabetical order\n\n    \n  \n  \n    \n      0:10:00\n    \n    geom_abline\n    \n      Adding geom_abline for y = x to a scatter plot for comparison\n\n    \n  \n  \n    \n      0:18:00\n    \n    geom_boxplot\n    \n      Visualising using geom_boxplot for mpg by vehicle class (size of car)\n\n    \n  \n  \n    \n      0:24:45\n    \n    \n    \n      Start of explanation of prediction goals\n\n    \n  \n  \n    \n      0:27:00\n    \n    sample_frac\n    \n      Creating train and test sets, along with trick using sample_frac function to randomly re-arrange all rows in a dataset\n\n    \n  \n  \n    \n      0:28:35\n    \n    geom_smooth\n    \n      First step of developing linear model: visually adding geom_smooth\n\n    \n  \n  \n    \n      0:30:00\n    \n    augment\n    \n      Using augment function to add extra variables from model to original dataset (fitted values and residuals, especially)\n\n    \n  \n  \n    \n      0:30:45\n    \n    \n    \n      Creating residuals plot and explaining what you want and don't want to see\n\n    \n  \n  \n    \n      0:31:50\n    \n    \n    \n      Explanation of splines\n\n    \n  \n  \n    \n      0:33:30\n    \n    \n    \n      Visualising effect of regressing using natural splines\n\n    \n  \n  \n    \n      0:35:10\n    \n    \n    \n      Creating a tibble to test different degrees of freedom (1:10) for natural splines\n\n    \n  \n  \n    \n      0:36:30\n    \n    unnest\n    \n      Using unnest function to get tidy versions of different models\n\n    \n  \n  \n    \n      0:37:55\n    \n    \n    \n      Visualising fitted values of all 6 different models at the same time\n\n    \n  \n  \n    \n      0:42:10\n    \n    glance\n    \n      Investigating whether the model got \"better\" as we added degrees of freedom to the natural splines, using the glance function\n\n    \n  \n  \n    \n      0:47:45\n    \n    \n    \n      Using ANOVA to perform a statistical test on whether natural splines as a group explain variation in MPG\n\n    \n  \n  \n    \n      0:48:30\n    \n    \n    \n      Exploring colinearity of dependant variables (displacement and cylinders)\n\n    \n  \n  \n    \n      0:55:10\n    \n    floor\n    \n      Binning years into every two years using floor function\n\n    \n  \n  \n    \n      0:56:40\n    \n    summarise_at\n    \n      Using summarise_at function to do quick averaging of multiple variables"
  },
  {
    "objectID": "content_pages/caribou-locations.html",
    "href": "content_pages/caribou-locations.html",
    "title": "Caribou Locations",
    "section": "",
    "text": "Notable topics: Maps with ggplot2, Calculating distance and speed with geosphere\nRecorded on: 2020-06-22\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/caribou-locations.html#screencast",
    "href": "content_pages/caribou-locations.html#screencast",
    "title": "Caribou Locations",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/caribou-locations.html#timestamps",
    "href": "content_pages/caribou-locations.html#timestamps",
    "title": "Caribou Locations",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:00\n    \n    summarizeacrossdplyr\n    \n      Use summarize and across to calculate the proportion of NA values in the individuals dataset. Note, you do not need to use list().\n\n    \n  \n  \n    \n      0:9:00\n    \n    ggplotbordersggplot2\n    \n      Use ggplot and borders from the ggplot2 package to create a map of Canada with deploy_on_longitude and deploy_on_latitude from the individuals dataset.\n\n    \n  \n  \n    \n      0:13:50\n    \n    read_sfsf\n    \n      Import Canada province shapefile using the sf package. [Unsuccessful]\n\n    \n  \n  \n    \n      0:25:00\n    \n    minmaxbase\n    \n      Use min and max from base r  within summarize to find out the start and end dates for each caribou in the locations dataset.\n\n    \n  \n  \n    \n      0:27:15\n    \n    samplegeom_pathbaseggplot2\n    \n      Use sample from base r to pick one single caribou at a time then use the subset with geom_path from ggplot2 to track the path a that caribou takes over time.  color = factor(floor_date(timestamp, \"quarter\") is used to color the path according to what quarter the observation occured in.\n\n    \n  \n  \n    \n      0:35:15\n    \n    as.Datefloor_datebaselubridate\n    \n      Use as.Date from base r and floor_date from the lubridate package to convert timestamp variable into quarters then facet_wrap the previous plot by quarter.\n\n    \n  \n  \n    \n      0:37:15\n    \n    difftimelagbase\n    \n      Within mutate, use as.numeric(difftime(timestamp, lag(timestamp), unit = \"hours\"))  from base r to figure out the gap in time between observations.\n\n    \n  \n  \n    \n      0:43:05\n    \n    distHaversinecbindgeosphere\n    \n      Use distHaversine from the geosphere package to calculate distance in km then convert it to speed in kph.\n\n    \n  \n  \n    \n      1:00:00\n    \n    \n    \n      Summary of dataset."
  },
  {
    "objectID": "content_pages/coffee-ratings.html",
    "href": "content_pages/coffee-ratings.html",
    "title": "Coffee Ratings",
    "section": "",
    "text": "Notable topics: Ridgeline plot, Pairwise correlation, Network plot, Singular value decomposition, Linear model\nRecorded on: 2020-07-06\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/coffee-ratings.html#screencast",
    "href": "content_pages/coffee-ratings.html#screencast",
    "title": "Coffee Ratings",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/coffee-ratings.html#timestamps",
    "href": "content_pages/coffee-ratings.html#timestamps",
    "title": "Coffee Ratings",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:08:15\n    \n    countmutatefct_lumpforcats\n    \n      Using fct_lump within count and then mutate to lump the variety of coffee together except for the most frequent\n\n    \n  \n  \n    \n      0:08:50\n    \n    geom_boxplotggplot2\n    \n      Create a geom_boxplot to visualize the variety and the distribution of total_cup_points\n\n    \n  \n  \n    \n      0:09:55\n    \n    geom_histogramggplot2\n    \n      Create a geom_histogram  to visualize the variety and the distribution of total_cup_points\n\n    \n  \n  \n    \n      0:11:40\n    \n    fct_reorderfct_reorder\n    \n      Using fct_reorder to reorder variety by sorting it along total_cup_points in ascending order\n\n    \n  \n  \n    \n      0:12:35\n    \n    summarizeacrossdplyr\n    \n      Using summarize with across to calculate the percent of missing data (NA) for each rating variable\n\n    \n  \n  \n    \n      0:15:20\n    \n    geom_colfct_lumpggplot2forcats\n    \n      Create a bar chart using geom_col with fct_lump to visualize the frequency of top countries\n\n    \n  \n  \n    \n      0:20:35\n    \n    pivot_longertidyr\n    \n      Using pivot_longer to pivot the rating metrics for wide format to long format\n\n    \n  \n  \n    \n      0:21:30\n    \n    geom_lineggplot2\n    \n      Create a geom_line chart to see if the sum of the rating categories equal to the total_cup_points column\n\n    \n  \n  \n    \n      0:23:10\n    \n    geom_density_ridgesggridges\n    \n      Create a geom_density_ridges chart to show the distribution of ratings across each rating metric\n\n    \n  \n  \n    \n      0:24:35\n    \n    summarizedplyr\n    \n      Using summarize with mean and sd to show the average rating per metric with its standard deviation\n\n    \n  \n  \n    \n      0:26:15\n    \n    pairwise_corwidyr\n    \n      Using pairwise_cor to find correlations amongst the rating metrics\n\n    \n  \n  \n    \n      0:27:20\n    \n    graph_from_data_frameggraphgeom_edge_linkgeom_node_pointgeom_node_textggraphigraph\n    \n      Create a network plot to show the clustering of the rating metrics\n\n    \n  \n  \n    \n      0:29:35\n    \n    widely_svdgeom_colreorder_withinscale_y_reorderedwidyrtidytext\n    \n      Using widely_svd to visualize the biggest source of variation with the rating metrics (Singular value decomposition)\n\n    \n  \n  \n    \n      0:37:40\n    \n    geom_histogramggplot2\n    \n      Create a geom_histogram to visualize the distribution of altitude\n\n    \n  \n  \n    \n      0:40:20\n    \n    pminbase\n    \n      Using pmin to set a maximum numeric altitude value of 3000\n\n    \n  \n  \n    \n      0:41:05\n    \n    geom_pointgeom_smoothggplot2\n    \n      Create a geom-point chart to visualize the correlation between altitude and quality (total_cup_points)\n\n    \n  \n  \n    \n      0:42:00\n    \n    summarizecordplyrstats\n    \n      Using summarize with cor to show the correlation between altitude and each rating metric\n\n    \n  \n  \n    \n      0:44:25\n    \n    lmgeom_pointtidymapgeom_errorbarhstatsbroompurrrggplot2\n    \n      Create a linear model lm for each rating metric then visualize the results using a geom_line chart to show how each kilometer of altitude contributes to the score\n\n    \n  \n  \n    \n      0:50:35\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/college-majors-and-income.html",
    "href": "content_pages/college-majors-and-income.html",
    "title": "College Majors and Income",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2018-10-14\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/college-majors-and-income.html#screencast",
    "href": "content_pages/college-majors-and-income.html#screencast",
    "title": "College Majors and Income",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/college-majors-and-income.html#timestamps",
    "href": "content_pages/college-majors-and-income.html#timestamps",
    "title": "College Majors and Income",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:45\n    \n    read_csv\n    \n      Using read_csv function to import data directly from Github to R (without cloning the repository)\n\n    \n  \n  \n    \n      0:7:20\n    \n    geom_histogramgeom_boxplot\n    \n      Creating a histogram (geom_histogram), then a boxplot (geom_boxplot), to explore the distribution of salaries\n\n    \n  \n  \n    \n      0:8:55\n    \n    fct_reorder\n    \n      Using fct_reorder function to sort boxplot of college majors by salary\n\n    \n  \n  \n    \n      0:9:35\n    \n    dollar_formatscales\n    \n      Using dollar_format function from scales package to convert scientific notation to dollar format (e.g., \"4e+04\" becomes \"$40,000\")\n\n    \n  \n  \n    \n      0:14:10\n    \n    geom_point\n    \n      Creating a dotplot (geom_point) of 20 top-earning majors (includes adjusting axis, using the colour aesthetic, and adding error bars)\n\n    \n  \n  \n    \n      0:17:45\n    \n    str_to_title\n    \n      Using str_to_title function to convert string from ALL CAPS to Title Case\n\n    \n  \n  \n    \n      0:20:45\n    \n    \n    \n      Creating a Bland-Altman graph to explore relationship between sample size and median salary\n\n    \n  \n  \n    \n      0:21:45\n    \n    geom_text_repelggrepel\n    \n      Using geom_text_repel function from ggrepel package to get text labels on scatter plot points\n\n    \n  \n  \n    \n      0:28:30\n    \n    count\n    \n      Using count function's wt argument to specify what should be counted (default is number of rows)\n\n    \n  \n  \n    \n      0:30:00\n    \n    \n    \n      Spicing up a dull bar graph by adding a redundant colour aesthetic (trick from Julia Silge)\n\n    \n  \n  \n    \n      0:36:20\n    \n    \n    \n      Starting to explore relationship between gender and salary\n\n    \n  \n  \n    \n      0:37:10\n    \n    geom_col\n    \n      Creating a stacked bar graph (geom_col) of gender breakdown within majors\n\n    \n  \n  \n    \n      0:40:15\n    \n    summarise_at\n    \n      Using summarise_at to aggregate men and women from majors into categories of majors\n\n    \n  \n  \n    \n      0:45:30\n    \n    geom_point\n    \n      Graphing scatterplot (geom_point) of share of women and median salary\n\n    \n  \n  \n    \n      0:47:10\n    \n    geom_smooth\n    \n      Using geom_smooth function to add a line of best fit to scatterplot above\n\n    \n  \n  \n    \n      0:48:40\n    \n    \n    \n      Explanation of why not to aggregate first when performing a statistical test (including explanation of Simpson's Paradox)\n\n    \n  \n  \n    \n      0:49:55\n    \n    geom_smooth\n    \n      Fixing geom_smooth so that we get one overall line while still being able to map to the colour aesthetic\n\n    \n  \n  \n    \n      0:51:10\n    \n    lm\n    \n      Predicting median salary from share of women with weighted linear regression (to take sample sizes into account)\n\n    \n  \n  \n    \n      0:56:05\n    \n    nesttidybroom\n    \n      Using nest function and tidy function from the broom package to apply a linear model to many categories at once\n\n    \n  \n  \n    \n      0:58:05\n    \n    p.adjust\n    \n      Using p.adjust function to adjust p-values to correct for multiple testing (using FDR, False Discovery Rate)\n\n    \n  \n  \n    \n      1:04:50\n    \n    \n    \n      Showing how to add an appendix to an RMarkdown file with code that doesn't run when compiled\n\n    \n  \n  \n    \n      1:09:00\n    \n    fct_lump\n    \n      Using fct_lump function to aggregate major categories into the top four and an \"Other\" category\n\n    \n  \n  \n    \n      1:10:05\n    \n    \n    \n      Adding sample size to the size aesthetic within the aes function\n\n    \n  \n  \n    \n      1:10:50\n    \n    ggplotlyplotly\n    \n      Using ggplotly function from plotly package to create an interactive scatterplot (tooltips appear when moused over)\n\n    \n  \n  \n    \n      1:15:55\n    \n    \n    \n      Exploring IQR (Inter-Quartile Range) of salaries by major"
  },
  {
    "objectID": "content_pages/cord-19-data-package.html",
    "href": "content_pages/cord-19-data-package.html",
    "title": "CORD-19 Data Package",
    "section": "",
    "text": "Notable topics: R package development and documentation-writing\nRecorded on: 2020-03-18\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/cord-19-data-package.html#screencast",
    "href": "content_pages/cord-19-data-package.html#screencast",
    "title": "CORD-19 Data Package",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/cord-19-data-package.html#timestamps",
    "href": "content_pages/cord-19-data-package.html#timestamps",
    "title": "CORD-19 Data Package",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:10\n    \n    \n    \n      Overview of JSON files with the data David will make a package of\n\n    \n  \n  \n    \n      0:3:05\n    \n    \n    \n      Starting to create a new package with \"New Project\" in RStudio\n\n    \n  \n  \n    \n      0:5:40\n    \n    \n    \n      Creating a file to reference the license for the dataset\n\n    \n  \n  \n    \n      0:7:25\n    \n    use_data_rawusethis\n    \n      Using use_data_raw function from usethis package to set up a folder structure and preliminary function for raw data\n\n    \n  \n  \n    \n      0:8:30\n    \n    \n    \n      Explanation that we want to limit the number of packages we load when building a package (e.g., no library(tidyverse) )\n\n    \n  \n  \n    \n      0:9:00\n    \n    use_packageusethis\n    \n      Using use_package function from usethis package to add \"Suggested packages\"\n\n    \n  \n  \n    \n      0:10:15\n    \n    \n    \n      Reviewing import and cleaning code already completed\n\n    \n  \n  \n    \n      0:14:55\n    \n    \n    \n      Using roxygen2 package to write documentation\n\n    \n  \n  \n    \n      0:19:35\n    \n    \n    \n      More documentation writing\n\n    \n  \n  \n    \n      0:24:50\n    \n    use_datausethis\n    \n      Using use_data function from usethis package to create a folder structure and datafile for (finished/cleaned) data\n\n    \n  \n  \n    \n      0:26:10\n    \n    \n    \n      Making a mistake clicking \"Install and Restart\" button on the \"Build\" tab (because of huge objects in the environment) (see 26:50 for alternative)\n\n    \n  \n  \n    \n      0:26:50\n    \n    load_alldevtools\n    \n      Using load_all function from devtrools package as an alternative to \"Install and Restart\" from above step\n\n    \n  \n  \n    \n      0:27:35\n    \n    documentdevtools\n    \n      Using document function from devtools package to process written documentation\n\n    \n  \n  \n    \n      0:32:20\n    \n    \n    \n      De-duplicating paper data in a way the keeps records that have fewer missing values than other records for the same paper\n\n    \n  \n  \n    \n      0:39:50\n    \n    use_datausethis\n    \n      Using use_data function with its overwrite argument to overwrite existing data\n\n    \n  \n  \n    \n      0:47:30\n    \n    \n    \n      Writing documentation for paragraphs data\n\n    \n  \n  \n    \n      0:57:55\n    \n    \n    \n      Testing an install of the package\n\n    \n  \n  \n    \n      0:59:30\n    \n    \n    \n      Adding link to code in documentation\n\n    \n  \n  \n    \n      1:03:00\n    \n    \n    \n      Writing examples of how to use the package (in documentation)\n\n    \n  \n  \n    \n      1:08:45\n    \n    \n    \n      Discussion of outstanding items that David hasn't done yet (e.g., readme, vignettes, tests)\n\n    \n  \n  \n    \n      1:09:20\n    \n    use_readme_rmdusethis\n    \n      Creating a simple readme, including examples, with use_readme_rmd function from usethis package\n\n    \n  \n  \n    \n      1:16:10\n    \n    knitknitr\n    \n      Using knit function from the knitr package to knit the readme into a markdown file\n\n    \n  \n  \n    \n      1:17:10\n    \n    \n    \n      Creating a GitHub repository to host the package (includes how to commit to a GitHub repo using RStudio's GUI)\n\n    \n  \n  \n    \n      1:18:15\n    \n    \n    \n      Explanation that version 0.0.0.9000 means that the package is in early development\n\n    \n  \n  \n    \n      1:20:30\n    \n    \n    \n      Actually creating the GitHub repository\n\n    \n  \n  \n    \n      1:22:25\n    \n    \n    \n      Overview of remaining tasks"
  },
  {
    "objectID": "content_pages/covid-19-open-research-dataset-cord-19.html",
    "href": "content_pages/covid-19-open-research-dataset-cord-19.html",
    "title": "COVID-19 Open Research Dataset (CORD-19)",
    "section": "",
    "text": "Notable topics: JSON formatted data\nRecorded on: 2020-03-17\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/covid-19-open-research-dataset-cord-19.html#screencast",
    "href": "content_pages/covid-19-open-research-dataset-cord-19.html#screencast",
    "title": "COVID-19 Open Research Dataset (CORD-19)",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/covid-19-open-research-dataset-cord-19.html#timestamps",
    "href": "content_pages/covid-19-open-research-dataset-cord-19.html#timestamps",
    "title": "COVID-19 Open Research Dataset (CORD-19)",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:0:55\n    \n    \n    \n      Disclaimer that David's not an epidemiologist\n\n    \n  \n  \n    \n      0:2:55\n    \n    \n    \n      Overview of dataset\n\n    \n  \n  \n    \n      0:7:50\n    \n    dir\n    \n      Using dir function with its full.names argument to get file paths for all files in a folder\n\n    \n  \n  \n    \n      0:9:45\n    \n    \n    \n      Inspecting JSON-formatted data\n\n    \n  \n  \n    \n      0:10:40\n    \n    hoist\n    \n      Introducing hoist function as a way to deal with nested lists (typical for JSON data)\n\n    \n  \n  \n    \n      0:11:40\n    \n    hoist\n    \n      Continuing to use the hoist function\n\n    \n  \n  \n    \n      0:13:10\n    \n    pluck\n    \n      Brief explanation of pluck specification\n\n    \n  \n  \n    \n      0:16:35\n    \n    object.size\n    \n      Using object.size function to check size of json data\n\n    \n  \n  \n    \n      0:17:40\n    \n    map_chrstr_c\n    \n      Using map_chr and str_c functions together to combine paragraphs of text in a list into a single character string\n\n    \n  \n  \n    \n      0:20:00\n    \n    unnest_tokenstidytext\n    \n      Using unnest_tokens function from tidytext package to split full paragraphs into individual words\n\n    \n  \n  \n    \n      0:22:50\n    \n    \n    \n      Overview of scispaCy package for Python, which has named entity recognition features\n\n    \n  \n  \n    \n      0:24:40\n    \n    spacyr\n    \n      Introducting spacyr package, which is a R wrapper around the Python scispaCy package\n\n    \n  \n  \n    \n      0:28:50\n    \n    tidytext\n    \n      Showing how tidytext can use a custom tokenization function (David uses spacyr package's named entity recognition)\n\n    \n  \n  \n    \n      0:32:20\n    \n    tokenize_wordstokenizers\n    \n      Demonstrating the tokenize_words function from the tokenizers package\n\n    \n  \n  \n    \n      0:37:00\n    \n    unnest_tokenstidytext\n    \n      Actually using a custom tokenizer in unnest_tokens function\n\n    \n  \n  \n    \n      0:39:45\n    \n    sample_n\n    \n      Using sample_n function to get a random sample of n rows\n\n    \n  \n  \n    \n      0:43:25\n    \n    \n    \n      Asking, \"What are groups of words that tend to occur together?\"\n\n    \n  \n  \n    \n      0:44:30\n    \n    pairwise_corwidyr\n    \n      Using pairwise_cor from widyr package to find correlation between named entities\n\n    \n  \n  \n    \n      0:45:40\n    \n    ggraphigraph\n    \n      Using ggraph and igraph packages to create a network plot\n\n    \n  \n  \n    \n      0:52:05\n    \n    \n    \n      Starting to look at papers' references\n\n    \n  \n  \n    \n      0:53:30\n    \n    unnest_wider\n    \n      Using unnest_longer then unnest_wider function to convert lists into a tibble\n\n    \n  \n  \n    \n      0:59:30\n    \n    str_trunc\n    \n      Using str_trunc function to truncate long character strings to a certain number of characters\n\n    \n  \n  \n    \n      1:06:25\n    \n    glueglue\n    \n      Using glue function for easy combination of strings and R code\n\n    \n  \n  \n    \n      1:19:15\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/cran-package-code.html",
    "href": "content_pages/cran-package-code.html",
    "title": "CRAN Package Code",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2019-12-29\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/cran-package-code.html#screencast",
    "href": "content_pages/cran-package-code.html#screencast",
    "title": "CRAN Package Code",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/cran-package-code.html#timestamps",
    "href": "content_pages/cran-package-code.html#timestamps",
    "title": "CRAN Package Code",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:30\n    \n    \n    \n      Summarizing many things by language (e.g., lines of code, comment/code ratio)\n\n    \n  \n  \n    \n      0:9:35\n    \n    gather\n    \n      Using gather function to consolidate multiple metrics into one dimension, then visualizing by facetting by metric\n\n    \n  \n  \n    \n      0:11:20\n    \n    facet_wrap\n    \n      Setting ncol = 1 within facet_wrap function to get facetted graphs to stack vertically\n\n    \n  \n  \n    \n      0:11:30\n    \n    reorder_withintidytext\n    \n      Using reorder_within function (tidytext package) to properly reorder factors within each facet\n\n    \n  \n  \n    \n      0:16:00\n    \n    geom_text\n    \n      Using geom_text label to add language name as label to scatter points\n\n    \n  \n  \n    \n      0:20:00\n    \n    \n    \n      Completing preliminary overview and looking at distribution of R code in packages\n\n    \n  \n  \n    \n      0:26:15\n    \n    str_extract\n    \n      Using str_extract to extract only letters and names from character vector (using regex)\n\n    \n  \n  \n    \n      0:34:00\n    \n    guides\n    \n      Re-ordering the order of categorical variables in the legend using guides function\n\n    \n  \n  \n    \n      0:36:00\n    \n    \n    \n      Investigating comment/code ratio\n\n    \n  \n  \n    \n      0:43:05\n    \n    \n    \n      Importing additional package data (looking around for a bit, then starting to actually import ~46:00)\n\n    \n  \n  \n    \n      0:54:40\n    \n    \n    \n      Importing even more additional data (available packages)\n\n    \n  \n  \n    \n      0:57:50\n    \n    separate_rows\n    \n      Using separate_rows function to separate delimited values\n\n    \n  \n  \n    \n      0:58:45\n    \n    extract\n    \n      Using extract function and regex to pull out specific types of characters from a string\n\n    \n  \n  \n    \n      1:05:35\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/european-energy.html",
    "href": "content_pages/european-energy.html",
    "title": "European Energy",
    "section": "",
    "text": "Notable topics: Data manipulation, Country flags, Slope graph, Function creation\nRecorded on: 2020-08-03\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/european-energy.html#screencast",
    "href": "content_pages/european-energy.html#screencast",
    "title": "European Energy",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/european-energy.html#timestamps",
    "href": "content_pages/european-energy.html#timestamps",
    "title": "European Energy",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:01:50\n    \n    countdplyr\n    \n      Using count to get an overview of scategorical data\n\n    \n  \n  \n    \n      0:07:25\n    \n    pivot_longergathertidyr\n    \n      Using pivot_longer and gather to pivot date variables from wide to long\n\n    \n  \n  \n    \n      0:09:00\n    \n    as.integerbase\n    \n      Using as.integer to change year variable from character to integer class\n\n    \n  \n  \n    \n      0:10:10\n    \n    fct_reorderforcats\n    \n      Using fct_reorder to reorder stacked barplot\n\n    \n  \n  \n    \n      0:10:30\n    \n    scale_y_continuouscommaggplot2scales\n    \n      Using scale_y_continuous  with labels = comma from scales package to insert a comma every three digits on the y-axis\n\n    \n  \n  \n    \n      0:16:35\n    \n    replace_natidyr\n    \n      Using replace_na and list to replace NA values in country_name column with United Kingdom\n\n    \n  \n  \n    \n      0:18:05\n    \n    fct_lumpforcats\n    \n      Using fct_lump to lump factor levels together except for the 10 most frequent for each facet panel\n\n    \n  \n  \n    \n      0:20:10\n    \n    reorder_withinscale_y_reorderedtidytext\n    \n      Using reorder_within with fun = sum and scale_y_reordered to reorder the categories within each facet panel\n\n    \n  \n  \n    \n      0:24:30\n    \n    geom_flagggfalgs\n    \n      Using ggflags package to add country flags | Debugging strategies include 1) minimal reproducible example and 2) binary search\n\n    \n  \n  \n    \n      0:29:20\n    \n    fct_recodeforcats\n    \n      (Unsuccessfully) Using fct_recode to rename the ISO two-digit identifier for the United Kingdom from the UK to GB\n\n    \n  \n  \n    \n      0:33:20\n    \n    ifelsebase\n    \n      Using ifelse to replace the ISO two-digit identifier for the United Kingdom from UK to GB & from EL to GR fro Greece | Debugging included\n\n    \n  \n  \n    \n      0:40:45\n    \n    str_to_lowerstringr\n    \n      Using str_to_lower to convert observations in country column to lower case\n\n    \n  \n  \n    \n      0:45:00\n    \n    geom_pointgeom_linescale_y_log10geom_flagggplot2ggflags\n    \n      Creating a slope graph to show differences in Nuclear production (2106 versus 2018) | Using scale_y_log10 to increase distance between points | Using ggflags for country flags\n\n    \n  \n  \n    \n      0:47:00\n    \n    scale_x_continuousggplot2\n    \n      Using scale_x_continuous with breaks = c(2016, 2018) to show only 2016 and 2018 on x-axis\n\n    \n  \n  \n    \n      0:48:20\n    \n    scale_x_continuousgeom_textggplot2\n    \n      Extend x-axis limits using scale_x_continuous with limits = c(2015, 2019) and geom_text with an ifelse within hjust to alternate labels for the right and left side of slope graph\n\n    \n  \n  \n    \n      0:52:40\n    \n    functionbase\n    \n      Creating a slopegraph function\n\n    \n  \n  \n    \n      1:00:00\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/french-train-delays.html",
    "href": "content_pages/french-train-delays.html",
    "title": "French Train Delays",
    "section": "",
    "text": "Notable topics: Heat map\nRecorded on: 2019-02-25\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/french-train-delays.html#screencast",
    "href": "content_pages/french-train-delays.html#screencast",
    "title": "French Train Delays",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/french-train-delays.html#timestamps",
    "href": "content_pages/french-train-delays.html#timestamps",
    "title": "French Train Delays",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:10:20\n    \n    fct_lump\n    \n      Boxplots of departure stations using fct_lump function\n\n    \n  \n  \n    \n      0:14:25\n    \n    \n    \n      Creating heat map of departure and arrival delays, then cleaning up a sparse heat map\n\n    \n  \n  \n    \n      0:15:30\n    \n    fct_reorderlength\n    \n      Using fct_reorder function and length function to reorder stations based on how frequently they appear\n\n    \n  \n  \n    \n      0:16:30\n    \n    fct_infreq\n    \n      Using fct_infreq to reorder based on infrequently-appearing stations (same as above, but without a trick needed)\n\n    \n  \n  \n    \n      0:17:45\n    \n    fct_lump\n    \n      Using fct_lump function to lump based on proportion instead of number of top categories desired\n\n    \n  \n  \n    \n      0:18:45\n    \n    scale_fill_gradient2\n    \n      Using scale_fill_gradient2 function to specify diverging colour scale\n\n    \n  \n  \n    \n      0:26:00\n    \n    \n    \n      Checking another person's take on the data, which is a heatmap over time\n\n    \n  \n  \n    \n      0:28:40\n    \n    sprintf\n    \n      Converting year and month (as digits) into date-class variable using sprintf function and padding month number with extra zero when necessary\n\n    \n  \n  \n    \n      0:34:50\n    \n    summarise_at\n    \n      Using summarise_at function to quickly sum multiple columns\n\n    \n  \n  \n    \n      0:39:35\n    \n    geom_tile\n    \n      Creating heatmap using geom_tile function for percentage of late trains by station over time\n\n    \n  \n  \n    \n      0:45:05\n    \n    fill\n    \n      Using fill function to fill in missing NA values with data from previous observations\n\n    \n  \n  \n    \n      0:50:35\n    \n    paste0\n    \n      Grouping multiple variables into a single category using paste0 function\n\n    \n  \n  \n    \n      0:51:40\n    \n    \n    \n      Grouping heatmap into International / National chunks with a weird hack\n\n    \n  \n  \n    \n      0:52:20\n    \n    \n    \n      Further separating International / National visually\n\n    \n  \n  \n    \n      0:53:30\n    \n    \n    \n      Less hacky way of separating International / National (compared to previous two rows)"
  },
  {
    "objectID": "content_pages/gdpr-violations.html",
    "href": "content_pages/gdpr-violations.html",
    "title": "GDPR Violations",
    "section": "",
    "text": "Notable topics: Data manipulation, Interactive dashboard with shinymetrics and tidymetrics\nRecorded on: 2020-04-20\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/gdpr-violations.html#screencast",
    "href": "content_pages/gdpr-violations.html#screencast",
    "title": "GDPR Violations",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/gdpr-violations.html#timestamps",
    "href": "content_pages/gdpr-violations.html#timestamps",
    "title": "GDPR Violations",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:4:05\n    \n    mdylubridate\n    \n      Use the mdy function from the lubridate package to change  the date variable from character class to date class.\n\n    \n  \n  \n    \n      0:5:35\n    \n    renamedplyr\n    \n      Use the rename function from the dplyr package to rename variable in the dataset.\n\n    \n  \n  \n    \n      0:6:15\n    \n    fct_reorderforcats\n    \n      Use the fct_reorder function from the forcats package to  sort the geom_col in descending order.\n\n    \n  \n  \n    \n      0:6:30\n    \n    fct_lumpcountforcatsdplyr\n    \n      Use the fct_lump function from the forcats package within count  to  lump together  country names except for the 6 most frequent.\n\n    \n  \n  \n    \n      0:7:05\n    \n    scale_x_continuousggplot2scales\n    \n      Use the scale_x_continuous function from ggplot2  with the scales package to change the x-axis values to dollar format.\n\n    \n  \n  \n    \n      0:8:15\n    \n    monthlubridate\n    \n      Use the month and floor_date  function from the lubridate package to get the month component from the date variable to count the total fines per month.\n\n    \n  \n  \n    \n      0:8:55\n    \n    na_ifdplyr\n    \n      Use the na_if function from the dplyr package to convert specific date value to NA.\n\n    \n  \n  \n    \n      0:11:05\n    \n    fct_reorderforcatsdplyr\n    \n      Use the fct_reorder function from the forcats package to sort the stacked geom_col and legend labels in descending order.\n\n    \n  \n  \n    \n      0:15:15\n    \n    dollarscales\n    \n      Use the dollar function from the scales package to convert the price variable into dollar format.\n\n    \n  \n  \n    \n      0:15:40\n    \n    str_truncstringr\n    \n      Use the str_trunc to shorten the summary string values to 140 characters.\n\n    \n  \n  \n    \n      0:17:35\n    \n    separate_rowstidyr\n    \n      Use the separate_rows function from the tidyr package with a regular expression to separate the values in the article_violated variable with each matching group placed in its own row.\n\n    \n  \n  \n    \n      0:19:30\n    \n    extracttidyr\n    \n      Use the extract function from the tidyr package with a regular expression to turn each matching group into a new column.\n\n    \n  \n  \n    \n      0:27:30\n    \n    geom_jitterggplot2\n    \n      Use the geom_jitter function from the ggplot2 package to add points to the horizontal box plot.\n\n    \n  \n  \n    \n      0:31:55\n    \n    inner_joindplyr\n    \n      Use the inner_join function from the dplyr package to join together  article_titles and separated_articles tables.\n\n    \n  \n  \n    \n      0:32:55\n    \n    paste0base R\n    \n      Use the paste0 function from base R to concatenate article and article_title.\n\n    \n  \n  \n    \n      0:38:48\n    \n    str_detectstringr\n    \n      Use the str_detect function from the stringr package to detect the presence of a pattern in a string.\n\n    \n  \n  \n    \n      0:40:25\n    \n    group_bysummarizegeom_pointdplyrggplot2\n    \n      Use the group_by and summarize functions from the dplyr package to aggregate fines that were issued to the same country on the same day allowing for size to be used in geom_point plot.\n\n    \n  \n  \n    \n      0:41:14\n    \n    ggplot2ggplot2\n    \n      Use the scale_size_continuous function from the ggplot2 package to remove the size legend.\n\n    \n  \n  \n    \n      0:42:55\n    \n    preview_metrictidymetricsshinymetrics\n    \n      Create an interactive dashboard using the shinymetrics and tidymetrics which is a tidy approach to business intelligence.\n\n    \n  \n  \n    \n      0:47:25\n    \n    cross_by_dimensionscross_by_periodsuse_metrics_scaffoldcreate_metricstidyr\n    \n      Use the cross_by_dimensions and cross_by_periods functions from the tidyr package which stacks an extra copy of the table for each dimension specified as an argument (country, article_title, type), replaces the value of the column with the word All and periods, and groups by all the columns. It acts as an extended group_by that allows complete summaries across each individual dimension and possible combinations."
  },
  {
    "objectID": "content_pages/global-crop-yields.html",
    "href": "content_pages/global-crop-yields.html",
    "title": "Global Crop Yields",
    "section": "",
    "text": "Notable topics: Interactive Shiny Dashboard\nRecorded on: 2020-08-31\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/global-crop-yields.html#screencast",
    "href": "content_pages/global-crop-yields.html#screencast",
    "title": "Global Crop Yields",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/global-crop-yields.html#timestamps",
    "href": "content_pages/global-crop-yields.html#timestamps",
    "title": "Global Crop Yields",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:03:35\n    \n    renamedplyr\n    \n      Using rename to shorten column name\n\n    \n  \n  \n    \n      0:06:40\n    \n    rename_allstr_removedplyrstring\n    \n      Using rename_all with str_remove and regex to remove characters in column name\n\n    \n  \n  \n    \n      0:07:40\n    \n    pivot_longertidyr\n    \n      Using pivot_longer to change data from wide to long\n\n    \n  \n  \n    \n      0:08:25\n    \n    geom_linefacet_wrapggplot2\n    \n      Create a faceted geom_line chart\n\n    \n  \n  \n    \n      0:09:40\n    \n    fct_reorderforcats\n    \n      Using fct_reorder to reorder facet panels in ascending order\n\n    \n  \n  \n    \n      0:11:50\n    \n    write_rdsinputPanelrenderPlotshinydplyrggplot2forcatsstringrplotly\n    \n      Create an interactive Shiny dashboard\n\n    \n  \n  \n    \n      0:33:20\n    \n    geom_lineggplot2\n    \n      Create a faceted geom_line chart with add_count and filter(n = max(x)) to subset the data for crops that have observations in every year\n\n    \n  \n  \n    \n      0:36:50\n    \n    geom_pointgeom_ablineggplot2\n    \n      Create a faceted geom_point chart showing the crop yields at start and end over a 50 year period (1968 start date and 2018 end date)\n\n    \n  \n  \n    \n      0:45:00\n    \n    geom_boxplotggplot2\n    \n      Create a geom_boxplot to visualize the distribution of yield ratios for the different crops to see how efficiency has increased across countries\n\n    \n  \n  \n    \n      0:46:00\n    \n    geom_colggplot2\n    \n      Create a geom_col chart to visualize the median yield ratio for each crop\n\n    \n  \n  \n    \n      0:47:50\n    \n    geom_pointgeom_text_repelggplot2ggrepel\n    \n      Create a geom_point chart to visualize efficiency imporvement for each country for a specific crop (yield start / yield ratio)\n\n    \n  \n  \n    \n      0:50:25\n    \n    countrycodegeom_hlinegeom_text_repelcountrycodeggplot2ggrepel\n    \n      Using the countrycode package to color geom_point chart by continent names\n\n    \n  \n  \n    \n      0:56:50\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/government-spending-on-kids.html",
    "href": "content_pages/government-spending-on-kids.html",
    "title": "Government Spending on Kids",
    "section": "",
    "text": "Notable topics: Data Manipulation, Functions, Embracing, Reading in Many .csv Files, Pairwise Correlation\nRecorded on: 2020-09-14\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/government-spending-on-kids.html#screencast",
    "href": "content_pages/government-spending-on-kids.html#screencast",
    "title": "Government Spending on Kids",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/government-spending-on-kids.html#timestamps",
    "href": "content_pages/government-spending-on-kids.html#timestamps",
    "title": "Government Spending on Kids",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:6:15\n    \n    geom_linesummarizeuniquesamplefacet_wrapfct_reordertheme_tuftegeom_vlineggplotdplyrbaseggthemesforcats\n    \n      Using geom_line and summarize to visualize education spending over time. First for all states. Then individual states. Then small groups of states using %in%. Then in random groups of size n using %in% and sample with unique. fct_reorder is used to reorder state factor levels by sorting along the inf_adj variable.\ngeom_vline used to add reference to the 2009 financial crisis.\n\n    \n  \n  \n    \n      0:16:00\n    \n    geom_linesummarizeuniquesamplefacet_wrapfct_reordertheme_tuftegeom_vlinegeom_hlineggplotdplyrbaseggthemesforcats\n    \n      Take the previous chart setting the inf_adj_perchild for the first year  1997 to 100% in order to show a measure of increase from 100% as opposed to absolute value for change over time for each state relative to 1997.  geom_hline used to add reference for the 100% starting point. David ends up changing the starting point from 100% to 0%\nfct_reorder with max used to reorder the plots in descending order based on highest peak values.\nDavid briefly mentions the small multiples approach to analyzing data.\n\n    \n  \n  \n    \n      0:23:35\n    \n    function\n    \n      Create a function named plot_changed_faceted to make it easier to visualize the many other variables included in the dataset.\n\n    \n  \n  \n    \n      0:27:25\n    \n    function\n    \n      Create a function named plot_faceted with a {{ y_axis }} embracing argument. Adding this function creates two stages: one for data transformation and another for plotting.\n\n    \n  \n  \n    \n      0:37:05\n    \n    dirmap_dffunctionset_namespivot_longerseparateextractbasepurrtidyr\n    \n      Use the dir function with pattern and purrr package's map_df function to read in many different .csv files with GDP values for each state.\nTroubleshooting Can't combine <character> and <double> columns error using function and mutate with across and as.numeric.\nExtract state name from filename using extract from tidyr and regular expression.\n\n    \n  \n  \n    \n      0:50:50\n    \n    read_xlsxreadxl\n    \n      Unsuccessful attempt at importing state population data via a not user friendly dataset from census.gov by skipping the first 3 rows of the Excel file.\n\n    \n  \n  \n    \n      0:54:22\n    \n    geom_colfct_reorderscale_fill_discreteggplotforcats\n    \n      Use geom_col to see which states spend the most for each child for a single variable and multiple variables using %in%.\nUse scale_fill_discrete with guide_legend(reverse = TRUE) to change the ordering of the legend.\n\n    \n  \n  \n    \n      0:57:40\n    \n    pairwise_corrfct_reorderwidyr\n    \n      Use geom_col and 'pairwise_corrto visualize the correlation between variables across states in 2016 usingpairwise correlation`.\n\n    \n  \n  \n    \n      1:02:02\n    \n    pivot_widergeom_pointgeom_textggplottidyr\n    \n      Use  geom_point to plot inf_adjust_perchild_PK12ed versus inf_adj_perchild_highered. geom_text used to apply state names to each point.\n\n    \n  \n  \n    \n      1:05:00\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/great-american-beer-festival.html",
    "href": "content_pages/great-american-beer-festival.html",
    "title": "Great American Beer Festival",
    "section": "",
    "text": "Notable topics: Log odds ratio, Logistic regression, TIE Fighter plot\nRecorded on: 2020-10-19\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/great-american-beer-festival.html#screencast",
    "href": "content_pages/great-american-beer-festival.html#screencast",
    "title": "Great American Beer Festival",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/great-american-beer-festival.html#timestamps",
    "href": "content_pages/great-american-beer-festival.html#timestamps",
    "title": "Great American Beer Festival",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:8:20\n    \n    pivot_widertidyr\n    \n      Use pivot_wider with values_fill = list(value =0)) from the tidyr package along with mutate(value = 1) to pivot the medal variable from long to wide adding a 1 for the medal type awarded and 0 for the remaining medal types in the row.\n\n    \n  \n  \n    \n      0:11:25\n    \n    fct_lumpforcats\n    \n      Use fct_lump from the forcats package to lump together all the beers except for the N most frequent.\n\n    \n  \n  \n    \n      0:12:25\n    \n    str_to_upperstringr\n    \n      Use str_to_upper from the stringr package to convert the case of the state variable to uppercase.\n\n    \n  \n  \n    \n      0:12:25\n    \n    fct_relevelforcats\n    \n      Use fct_relevel from the the forcats package in order to reorder the medal factor levels.\n\n    \n  \n  \n    \n      0:13:25\n    \n    fct_reorderforcats\n    \n      Use fct_reorder from the forcats package to sort beer_name factor levels by sorting along n.\n\n    \n  \n  \n    \n      0:14:30\n    \n    glueglue\n    \n      Use glue from the glue package to concatenate beer_name and brewery on the y-axis.\n\n    \n  \n  \n    \n      0:15:00\n    \n    fct_lumpforcats\n    \n      Use ties.mthod = \"first\"  within fct_lump to show only the first brewery when a tie exists between them.\n\n    \n  \n  \n    \n      0:19:25\n    \n    state.abbsetdiffdatasets\n    \n      Use setdiff from the dplyr package and the state.abb built in vector from the datasets package  to check which states are missing from the dataset.\n\n    \n  \n  \n    \n      0:21:25\n    \n    summarizedplyr\n    \n      Use summarize from the dplyr package to calculate the number of medals with n_medals = n(), number of beers with n_distinct, number of gold medals with sum(), and weighted medal totals using sum(as.integer() because medal is an ordered factor, so 1 for each bronze, 2 for each silver, and 3 for each gold.\n\n    \n  \n  \n    \n      0:26:05\n    \n    read_csvreadr\n    \n      Import Craft Beers Dataset from Kaggle using read_csv from the readr package.\n\n    \n  \n  \n    \n      0:28:00\n    \n    inner_joindplyr\n    \n      Use inner_join from the dplyr package to join together the 2 datasets from kaggle.\n\n    \n  \n  \n    \n      0:29:40\n    \n    semi_joindplyr\n    \n      Use semi_join from the dplyr package to join together to see if the beer names match with the kaggle dataset.  Ends up at a dead end with not enough matches between the datasets.\n\n    \n  \n  \n    \n      0:33:05\n    \n    bind_log_oddstidylo\n    \n      Use bind_log_odds from the tidylo package to show the representation of each beer category for each state compared to the categories across the other states.\n\n    \n  \n  \n    \n      0:33:35\n    \n    completetidyr\n    \n      Use complete from the tidyr package in order to turn missing values into explicit missing values.\n\n    \n  \n  \n    \n      0:35:30\n    \n    reorder_withinscale_y_reorderedfacet_wraptidytext\n    \n      Use reorder_within from the tidytext package and scale_y_reordered from the tidytext package in order to reorder the bars within each facet panel.\n\n    \n  \n  \n    \n      0:36:40\n    \n    fct_reorderforcats\n    \n      Use fct_reorder from the forcats package to reorder the facet panels in descending order.\n\n    \n  \n  \n    \n      0:39:35\n    \n    fillggplot2\n    \n      For the previous plot, use fill = log_odds_weighted > 0 in the ggplot aes argument to highlight the positive and negative values.\n\n    \n  \n  \n    \n      0:41:45\n    \n    add_countmutatedplyr\n    \n      Use add_count from the dplyr package to add a year_total variable which shows the total awards for each year. Then use this to calculate the percent change in totals medals per state using mutate(pct_year = n / year)\n\n    \n  \n  \n    \n      0:44:40\n    \n    glmcbindstats\n    \n      Use glm from the stats package to create a logistic regression model to find out if their is a statistical trend in the probability of award success over time.\n\n    \n  \n  \n    \n      0:47:15\n    \n    group_bysummarizelistglmmutatemapbroompurrr\n    \n      Exapnd on the previous model by using the broom package to fit multiple logistic regressions across multiple states instead of doing it for an individual state at a time.\n\n    \n  \n  \n    \n      0:50:25\n    \n    conf.int\n    \n      Use conf.int = TRUE to add confidence bounds to the logistic regression output then use it to create a TIE Fighter plot to show which states become more or less frequent medal winners over time.\n\n    \n  \n  \n    \n      0:53:00\n    \n    state.namematchdatasets\n    \n      Use the state.name dataset with match from base r  to change state abbreviation to the state name.\n\n    \n  \n  \n    \n      0:55:00\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/hbcu-enrollment.html",
    "href": "content_pages/hbcu-enrollment.html",
    "title": "HBCU Enrollment",
    "section": "",
    "text": "Notable topics: Data Cleaning\nRecorded on: 2021-02-01\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/hbcu-enrollment.html#screencast",
    "href": "content_pages/hbcu-enrollment.html#screencast",
    "title": "HBCU Enrollment",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/hbcu-enrollment.html#timestamps",
    "href": "content_pages/hbcu-enrollment.html#timestamps",
    "title": "HBCU Enrollment",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:45\n    \n    str_detectstringr\n    \n      Detect the presence or absence of a pattern in a string.\n\n    \n  \n  \n    \n      0:3:30\n    \n    separatetidyr\n    \n      Separate a character column into multiple columns with a regular expression or numeric locations\n\n    \n  \n  \n    \n      0:3:30\n    \n    renamedplyr\n    \n      Rename column.\n\n    \n  \n  \n    \n      0:4:20\n    \n    distinctdplyr\n    \n      Select only unique/distinct rows from a data frame.\n\n    \n  \n  \n    \n      0:5:55\n    \n    expand_limitsggplot2\n    \n      Expand the y axis plot limits by starting at 0.\n\n    \n  \n  \n    \n      0:6:20\n    \n    full_joindplyr\n    \n      Combine two datasets while including all rows in x and y.\n\n    \n  \n  \n    \n      0:11:00\n    \n    percentscales\n    \n      Y axis labels as percentages (2.5%, 50%, etc).\n\n    \n  \n  \n    \n      0:12:30\n    \n    bind_rowsdplyr\n    \n      Bind multiple data frames by row and an explanation as to why it's not the best approach for joining given the other options.\n\n    \n  \n  \n    \n      0:14:55\n    \n    rbindrow_binddplyrbase\n    \n      Brief discussion on the differences between rbind and row_bind.\n\n    \n  \n  \n    \n      0:16:10\n    \n    str_removestringr\n    \n      Remove matched patterns in a string.\n\n    \n  \n  \n    \n      0:17:10\n    \n    clean_namesjanitor\n    \n      Turn variable names into 'snake case' (e.g. Standard Error, standard_error).\n\n    \n  \n  \n    \n      0:18:10\n    \n    mutate_ifis.characterparse_numberdplyrbasereadr\n    \n      Mutate multiple columns to change type from character to numeric while parsing out the numbers while getting rid of the other characters in the dataset.\n\n    \n  \n  \n    \n      0:18:50\n    \n    slicedplyr\n    \n      Subset rows using their positions.\n\n    \n  \n  \n    \n      0:20:15\n    \n    gathermutateifelsestr_removespreadtidyrdplyrstringrbase\n    \n      Reshape the data from wide to long such that there is one row for each year and race.\n\n    \n  \n  \n    \n      0:21:25\n    \n    absbase\n    \n      Compute the absolute value of x\n\n    \n  \n  \n    \n      0:24:55\n    \n    str_removestringr\n    \n      Remove matched patterns in a string (e.g. black1, black & white1, white).\n\n    \n  \n  \n    \n      0:25:35\n    \n    fct_reorderforcats\n    \n      Reorder factor levels in geom_line plot by sorting along another variable.\n\n    \n  \n  \n    \n      0:29:25\n    \n    bind_rowsdplyr\n    \n      Bind multiple data frames by row.\n\n    \n  \n  \n    \n      0:36:05\n    \n    fct_relevelforcats\n    \n      Reorder factor levels by hand.\n\n    \n  \n  \n    \n      0:37:45\n    \n    str_removestringr\n    \n      Detect and remove the presence of a pattern in a string to remove duplication from geom_line plot legend.\n\n    \n  \n  \n    \n      0:38:50\n    \n    fct_reorderforcats\n    \n      \"Reorder factor levels in geom_line plot by sorting along another variable with ordering based on the last value to make the data line up with how the values are displayed in the legend. 'fct_reorder(race_ethnicity, percent, last, .desc = TRUE)`\"\n\n    \n  \n  \n    \n      0:40:35\n    \n    read_excelreadxl\n    \n      Import external Excel data set from Data.World.\n\n    \n  \n  \n    \n      0:44:20\n    \n    starts_withtidyselect\n    \n      Select variables that match a pattern to remove.\n\n    \n  \n  \n    \n      0:49:00\n    \n    str_removegroup_byfirstifelsecumsumstringrdplyr\n    \n      Unpack data in one column (field_gender) into two separate columns (field, gender).\n\n    \n  \n  \n    \n      0:49:20\n    \n    \n    \n      Summary of screencast.\n\n    \n  \n  \n    \n      0:58:00\n    \n    \n    \n      NA"
  },
  {
    "objectID": "content_pages/himalayan-climbers.html",
    "href": "content_pages/himalayan-climbers.html",
    "title": "Himalayan Climbers",
    "section": "",
    "text": "Notable topics: Data Manipulation, Empirical Bayes, Logistic Regression Model\nRecorded on: 2020-09-21\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/himalayan-climbers.html#screencast",
    "href": "content_pages/himalayan-climbers.html#screencast",
    "title": "Himalayan Climbers",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/himalayan-climbers.html#timestamps",
    "href": "content_pages/himalayan-climbers.html#timestamps",
    "title": "Himalayan Climbers",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:3:00\n    \n    ggplotfct_reorderggplotforcats\n    \n      Create a geom_col chart to visualize the top 50 tallest mountains.\nUse fct_reorder to reorder the peak_name factor levels by sorting along the height_metres variable.\n\n    \n  \n  \n    \n      0:8:50\n    \n    summarizeacrossarrangemutateinner_joindplyr\n    \n      Use summarize with across to get the total number of climbs, climbers, deaths, and first year climbed.\nUse mutate to calculate the percent death rate for members and hired staff.\nUse inner_join and select to join with peaks dataset by peak_id.\n\n    \n  \n  \n    \n      0:11:20\n    \n    \n    \n      Touching on statistical noise and how it impacts the death rate for mountains with fewer number of climbs, and how to account for it using various statistical methods including Beta Binomial Regression & Empirical Bayes.\n\n    \n  \n  \n    \n      0:14:30\n    \n    \n    \n      Further description of Empirical Bayes and how to account for not overestimating death rate for mountains with fewer climbers.\nRecommended reading: Introduction to Empirical Bayes: Examples from Baseball Statistics by David Robinson\n\n    \n  \n  \n    \n      0:17:00\n    \n    add_ebb_estimategeom_pointgeom_ablineebbrggplot\n    \n      Use the ebbr package (Empirical Bayes for Binomial in R) to create an Empirical Bayes Estimate for each mountain by fitting prior distribution across data and adjusting the death rates down or up based on the prior distributions.\nUse a geom_point chart to visualize the difference between the raw death rate and new ebbr fitted death rate.\n\n    \n  \n  \n    \n      0:21:20\n    \n    ggplotfct_reordergeom_errorbarhggplotforcats\n    \n      Use geom_point to visualize how deadly each mountain is with geom_errorbarh representing the 95% credible interval between minimum and maximum values.\n\n    \n  \n  \n    \n      0:26:35\n    \n    geom_pointggplotforcats\n    \n      Use geom_point to visualize the relationship between death rate and height of mountain.\nThere is not a clear relationship, but David does briefly mention how one could use Beta Binomial Regression to further inspect for possible relationships / trends.\n\n    \n  \n  \n    \n      0:28:00\n    \n    mutatecase_whenstr_detectfct_lumpfct_reorderdplyrstringrforcats\n    \n      Use geom_histogram and geom_boxplot to visualize the distribution of time it took climbers to go from basecamp to the mountain’s high point for successful climbs only.\nUse mutate to calculate the number of days it took climbers to get from basecamp to the highpoint.\nAdd column to data using case_when and str_detect to identify strings in termination_reason that contain the word Success and rename them to Success & how to use a vector and %in% to change multiple values in termination_reason to NA and rest to Failed.\nUse fct_lump to show the top 10 mountains while lumping the other factor levels (mountains) into other.\n\n    \n  \n  \n    \n      0:35:30\n    \n    geom_histogramgeom_densityggplot\n    \n      For just Mount Everest, use geom_histogram and geom_density with fill = success to visualize the days from basecamp to highpoint for climbs that ended in success, failure or other.\n\n    \n  \n  \n    \n      0:38:40\n    \n    geom_histogramggplot\n    \n      For just Mount Everest, use geom_histogram to see the distribution of climbs per year.\n\n    \n  \n  \n    \n      0:39:55\n    \n    mutatepmaxgeom_linegeom_pointggplotbasedplyr\n    \n      For just Mount Everest, use ‘geom_lineandgeom_pointto visualizepct_death` over time by decade.\nUse mutate with pmax and integer division to create a decade variable that lumps together the data for 1970 and before.\n\n    \n  \n  \n    \n      0:41:30\n    \n    function\n    \n      Write a function for summary statistics such as n_climbs, pct_success, first_climb, pct_death, ‘pct_hired_staff_death`.\n\n    \n  \n  \n    \n      0:46:20\n    \n    mutatepmaxgeom_linegeom_pointggplotbasedplyr\n    \n      For just Mount Everest, use geom_line and geom_point to visualize pct_success over time by decade.\n\n    \n  \n  \n    \n      0:47:10\n    \n    mutatepmaxgeom_linegeom_pointggplotbasedplyr\n    \n      For just Mount Everest, use geom_line and geom_point to visualize pct_hired_staff_deaths  over time by decade.\nDavid decides to visualize the pct_hired_staff_deaths and pct_death charts together on the same plot.\n\n    \n  \n  \n    \n      0:50:45\n    \n    fct_lumpglmformat.pvalforcatsstatsbroombase\n    \n      For just Mount Everest, fit a logistic regression model to predict the probability of death with format.pval to calculate the p.value.\nUse fct_lump to lump together all expedition_role factors except for the n most frequent.\n\n    \n  \n  \n    \n      0:56:30\n    \n    group_bysummarizedplyr\n    \n      Use group_by with integer division and summarize to calculate n_climbers and pct_death for age bucketed into decades.\n\n    \n  \n  \n    \n      0:59:45\n    \n    geom_pointgeom_errorbarhconf.intggplotbroom\n    \n      Use geom_point and geom_errorbarh to visualize the logistic regression model with confident intervals.\n\n    \n  \n  \n    \n      1:03:30\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/historical-phones.html",
    "href": "content_pages/historical-phones.html",
    "title": "Historical Phones",
    "section": "",
    "text": "Notable topics: Joining tables, Animated world choropleth, Adding IQR to geom_line, World development indicators package\nRecorded on: 2020-11-09\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/historical-phones.html#screencast",
    "href": "content_pages/historical-phones.html#screencast",
    "title": "Historical Phones",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/historical-phones.html#timestamps",
    "href": "content_pages/historical-phones.html#timestamps",
    "title": "Historical Phones",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:2:15\n    \n    bind_rowsdplyr\n    \n      Use bind_rows from the dplyr package to combine the two data sets.\n\n    \n  \n  \n    \n      0:7:30\n    \n    ggplotgroupggplot2\n    \n      Use group = interaction(type, country) within ggplot aes() to set the interaction type with every single country on one plot.\n\n    \n  \n  \n    \n      0:9:30\n    \n    semi_jointop_ndplyr\n    \n      Use semi_join from the dplyr package to join rows from phones with a match in country_sizes.\n\n    \n  \n  \n    \n      0:14:00\n    \n    quantilegeom_ribbonstatsggplot2\n    \n      Use quantile from the stats package within summarize to show the 25th, and 75th quantiles (interquartile range) on the plot.\n\n    \n  \n  \n    \n      0:17:50\n    \n    WDIselectWDIdplyr\n    \n      Import the wdi package (World Development Indicators from the World Bank) with extra = TRUE in order to get the iso3c code and income level for each country.\n\n    \n  \n  \n    \n      0:19:45\n    \n    inner_joindplyr\n    \n      Use inner_join from the dplyr package to join the WDI data with the phones data.\n\n    \n  \n  \n    \n      0:20:35\n    \n    fct_relevelforcats\n    \n      Use fct_relevel from the forcats package to reorder income factor levels in ascending order.\n\n    \n  \n  \n    \n      0:21:05\n    \n    .\n    \n      Create an anonymous function using . (dot).\n\n    \n  \n  \n    \n      0:29:30\n    \n    inner_joingeom_ablinedplyrggplot2\n    \n      Use inner_join from the dplyr package to join the mobile data and landline data together with a geom_abline to see how different the total populations are between the two datasets.\n\n    \n  \n  \n    \n      0:31:00\n    \n    geom_hlineggplot2\n    \n      Use geom_hline to add a refrence line to the plot shwoing when each country crossed the 50 per 100 subscription mark.\n\n    \n  \n  \n    \n      0:35:20\n    \n    summarizemindplyr\n    \n      Use summarize from the dplyr package with min(year([Mobile >= 50])) to find the year in which each country crossed the 50 per 100 subscription mark.\n\n    \n  \n  \n    \n      0:35:20\n    \n    summarizemaxdplyr\n    \n      Use summarize from the dplyr package with max(Mobile) to find the peak number of mobile subscriptions per country.\n\n    \n  \n  \n    \n      0:35:20\n    \n    na_ifsummarizedplyr\n    \n      Use na_if from the dplyr package within summarize to change Inf to NA.\n\n    \n  \n  \n    \n      0:38:20\n    \n    WDIsearchWDI\n    \n      Using the WDIsearch function to search the WDI package for proper GDP per capita indicator. Ended up using the NY.GDP.PCAP.PP.KD indicator.\n\n    \n  \n  \n    \n      0:39:05\n    \n    WDIselectWDIdplyr\n    \n      Adding the GDP data from the WDI package to the country_incomes table.\n\n    \n  \n  \n    \n      0:39:52\n    \n    inner_joindplyr\n    \n      Using the inner_join function from the dplyr package to join the phones table with the country_incomes table pulling in the gdp_per_capita variable.\n\n    \n  \n  \n    \n      0:42:25\n    \n    WDIsearchWDI\n    \n      Using the WDIsearch function to search the WDI package for proper population indicator. Ended up using the SP.POP.TOTL indicator.\n\n    \n  \n  \n    \n      0:50:00\n    \n    map_dataiso3166regex_left_joinleft_jointransition_manualmapsggplot2fuzzyjoingganimate\n    \n      Create an animated choropleth world map with fill = subscriptions.\n\n    \n  \n  \n    \n      1:00:00\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/horror-movie-profits.html",
    "href": "content_pages/horror-movie-profits.html",
    "title": "Horror Movie Profits",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2018-10-22\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/horror-movie-profits.html#screencast",
    "href": "content_pages/horror-movie-profits.html#screencast",
    "title": "Horror Movie Profits",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/horror-movie-profits.html#timestamps",
    "href": "content_pages/horror-movie-profits.html#timestamps",
    "title": "Horror Movie Profits",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:2:50\n    \n    parse_datelubridate\n    \n      Using parse_date function from lubridate package to convert date formatted as character to date class (should have used mdy function though)\n\n    \n  \n  \n    \n      0:7:45\n    \n    fct_lump\n    \n      Using fct_lump function to aggregate distributors into top 6 (by number of movies) and and \"Other\" category\n\n    \n  \n  \n    \n      0:8:50\n    \n    \n    \n      Investigating strange numbers in the data and discovering duplication\n\n    \n  \n  \n    \n      0:12:40\n    \n    problems\n    \n      Using problems function to look at parsing errors when importing data\n\n    \n  \n  \n    \n      0:14:35\n    \n    arrangedistinct\n    \n      Using arrange and distinct function and its .keep_all argument to de-duplicate observations\n\n    \n  \n  \n    \n      0:16:10\n    \n    goem_boxplot\n    \n      Using geom_boxplot function to create a boxplot of budget by distributor\n\n    \n  \n  \n    \n      0:19:20\n    \n    floor\n    \n      Using floor function to bin release years into decades (e.g., \"1970\" and \"1973\" both become \"1970\")\n\n    \n  \n  \n    \n      0:21:30\n    \n    summarise_at\n    \n      Using summarise_at function to apply the same function to multiple variables at the same time\n\n    \n  \n  \n    \n      0:24:10\n    \n    geom_line\n    \n      Using geom_line to visualize multiple metrics at the same time\n\n    \n  \n  \n    \n      0:26:00\n    \n    facet_wrap\n    \n      Using facet_wrap function to graph small multiples of genre-budget boxplots by distributor\n\n    \n  \n  \n    \n      0:28:35\n    \n    \n    \n      Starting analysis of profit ratio of movies\n\n    \n  \n  \n    \n      0:32:50\n    \n    paste0\n    \n      Using paste0 function in a custom function to show labels of multiple (e.g., \"4X\" or \"6X\" to mean \"4 times\" or \"6 times\")\n\n    \n  \n  \n    \n      0:41:20\n    \n    \n    \n      Starting analysis of the most common genres over time\n\n    \n  \n  \n    \n      0:45:55\n    \n    \n    \n      Starting analysis of the most profitable individual horror movies\n\n    \n  \n  \n    \n      0:51:45\n    \n    paste0\n    \n      Using paste0 function to add release date of movie to labels in a bar graph\n\n    \n  \n  \n    \n      0:53:25\n    \n    geom_text\n    \n      Using geom_text function, along with its check_overlap argument, to add labels to some points on a scatterplot\n\n    \n  \n  \n    \n      0:58:10\n    \n    ggplotlyplotly\n    \n      Using ggplotly function from plotly package to create an interactive scatterplot\n\n    \n  \n  \n    \n      1:00:55\n    \n    \n    \n      Reviewing unexplored areas of investigation"
  },
  {
    "objectID": "content_pages/horror-movies.html",
    "href": "content_pages/horror-movies.html",
    "title": "Horror Movies",
    "section": "",
    "text": "Notable topics: ANOVA, Text mining (tidytext package), LASSO regression (glmnet package)\nRecorded on: 2019-10-21\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/horror-movies.html#screencast",
    "href": "content_pages/horror-movies.html#screencast",
    "title": "Horror Movies",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/horror-movies.html#timestamps",
    "href": "content_pages/horror-movies.html#timestamps",
    "title": "Horror Movies",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:15\n    \n    extract\n    \n      Extracting digits (release year) from character string using regex, along with good explanation of extract function\n\n    \n  \n  \n    \n      0:8:00\n    \n    parse_number\n    \n      Quick check on why parse_number is unable to parse some values -- is it because they are NA or some other reason?\n\n    \n  \n  \n    \n      0:9:45\n    \n    \n    \n      Visually investigating correlation between budget and rating\n\n    \n  \n  \n    \n      0:11:50\n    \n    \n    \n      Investigating correlation between MPAA rating (PG-13, R, etc.) and rating using boxplots\n\n    \n  \n  \n    \n      0:12:50\n    \n    pull\n    \n      Using pull function to quickly check levels of a factor\n\n    \n  \n  \n    \n      0:13:30\n    \n    \n    \n      Using ANOVA to check difference of variation within groups (MPAA rating) than between groups\n\n    \n  \n  \n    \n      0:15:40\n    \n    separate_rows\n    \n      Separating genre using separate_rows function (instead of str_split and unnest)\n\n    \n  \n  \n    \n      0:18:00\n    \n    separate\n    \n      Removing boilerplate \"Directed by...\" and \"With...\" part of plot variable and isolating plot, first using regex, then by using separate function with periods as separator\n\n    \n  \n  \n    \n      0:20:40\n    \n    \n    \n      Unnesting word tokens, removing stop words, and counting appearances\n\n    \n  \n  \n    \n      0:21:20\n    \n    \n    \n      Aggregating by word to find words that appear in high- or low-rated movies\n\n    \n  \n  \n    \n      0:23:00\n    \n    \n    \n      Discussing potential confounding factors for ratings associated with specific words\n\n    \n  \n  \n    \n      0:24:50\n    \n    \n    \n      Searching for duplicated movie titles\n\n    \n  \n  \n    \n      0:25:50\n    \n    \n    \n      De-duping using distinct function\n\n    \n  \n  \n    \n      0:26:55\n    \n    glmnet\n    \n      Loading in and explaining glmnet package\n\n    \n  \n  \n    \n      0:28:00\n    \n    \n    \n      Using movie titles to pull out ratings using rownmaes and match functions to create an index of which rating to pull out of the original dataset\n\n    \n  \n  \n    \n      0:29:10\n    \n    \n    \n      Actually using glmnet function to create lasso model\n\n    \n  \n  \n    \n      0:34:05\n    \n    \n    \n      Showing built-in plot of lasso lambda against mean-squared error\n\n    \n  \n  \n    \n      0:37:05\n    \n    \n    \n      Explaining when certain terms appeared in the lasso model as the lambda value dropped\n\n    \n  \n  \n    \n      0:41:10\n    \n    \n    \n      Gathering all variables except for title, so that the dataset is very tall\n\n    \n  \n  \n    \n      0:42:35\n    \n    unite\n    \n      Using unite function to combine two variables (better alternative to paste)\n\n    \n  \n  \n    \n      0:45:45\n    \n    \n    \n      Creating a new lasso with tons of new variables other than plot words"
  },
  {
    "objectID": "content_pages/ikea-furniture.html",
    "href": "content_pages/ikea-furniture.html",
    "title": "IKEA Furniture",
    "section": "",
    "text": "Notable topics: Linear model, Coefficient/TIE fighter plot, Boxplots, Log scale discussion, Calculating volume\nRecorded on: 2020-11-02\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/ikea-furniture.html#screencast",
    "href": "content_pages/ikea-furniture.html#screencast",
    "title": "IKEA Furniture",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/ikea-furniture.html#timestamps",
    "href": "content_pages/ikea-furniture.html#timestamps",
    "title": "IKEA Furniture",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:4:30\n    \n    fct_reorderforcats\n    \n      Use fct_reorder from the forcats package to reorder the factor levels for category sorted along n.\n\n    \n  \n  \n    \n      0:6:00\n    \n    scale_x_log_10geom_boxplotggplot2\n    \n      Brief explanation of why scale_x_log10 is needed given the distribution of category and price with geom_boxplot.\n\n    \n  \n  \n    \n      0:7:00\n    \n    geom_jittergeom_boxplotggplot2\n    \n      Using geom_jitter with geom_boxplot to show how many items are within each category.\n\n    \n  \n  \n    \n      0:8:00\n    \n    glueadd_countgluedplyr\n    \n      Use add_count from the dplyr package and  glue from the glue package to concatenate the category name with category_total on the geom_boxplot y-axis.\n\n    \n  \n  \n    \n      0:9:00\n    \n    mutatedplyr\n    \n      Convert from Saudi Riyals to United States Dollars.\n\n    \n  \n  \n    \n      0:11:05\n    \n    geom_density_ridgesggridges\n    \n      Create a ridgeplot - AKA joyplot - using ggridges package showing the distribution of price across category.\n\n    \n  \n  \n    \n      0:12:50\n    \n    \n    \n      Discussion on distributions and when to use a log scale.\n\n    \n  \n  \n    \n      0:19:20\n    \n    fct_lumpforcats\n    \n      Use fct_lump from the forcats package to lump together all the levels in category except for the n most frequent.\n\n    \n  \n  \n    \n      0:21:00\n    \n    scale_fill_discreteggplot2\n    \n      Use scale_fill_discrete from the ggplot2 package with guide = guide_legend(reverse = TRUE) to reverse the fill legend.\n\n    \n  \n  \n    \n      0:24:20\n    \n    str_trimstr_replace_allstringr\n    \n      Use str_trim  from the stringr package to remove whitespace from the short_description variable. David then decides to use str_replace_all instead with the following regular expression \"\\\\s+\", \" \" to replace all whitespace with a single space instead.\n\n    \n  \n  \n    \n      0:25:30\n    \n    separatetidyr\n    \n      Use separate from the tidyr package with extra = \"merge\" and fill = \"right\" to separate item description from item dimension.\n\n    \n  \n  \n    \n      0:26:45\n    \n    extracttidyr\n    \n      Use extract from the tidyr package with the regular expression \"[\\\\d\\\\-xX]+) cm\" to extract the numbers before cm.\n\n    \n  \n  \n    \n      0:29:50\n    \n    unitetidyr\n    \n      Use unite from the tidyr package to paste together the category and main_description columns into a new column named category_and_description.\n\n    \n  \n  \n    \n      0:32:45\n    \n    mutatedplyr\n    \n      Calculate the volume given the depth, height, and width of each item in dataset in liters using depth * height * width / 1000. At 36:15, David decides to change to cubic meters instead using depth * height * width / 1000000.\n\n    \n  \n  \n    \n      0:44:20\n    \n    str_squishstringr\n    \n      Use str_squish from the stringr package to remove whitespace from the start to the end of the short_description variable.\n\n    \n  \n  \n    \n      0:48:00\n    \n    lmstats\n    \n      Use lm from the stats package to create a linear model on a log, log scale to predict the price of an item based on volume + category. David then uses fct_relevel to reorder the factor levels for category such that tables & desks is first (starting point) since it's the most frequent item in the category variable and it's price distribution is in the middle.\n\n    \n  \n  \n    \n      0:53:00\n    \n    tidygeom_pointgeom_errorbarhgeom_vlinebroom\n    \n      Use the broom package to turn the model output into a coefficient / TIE fighter plot.\n\n    \n  \n  \n    \n      0:56:20\n    \n    str_removestringr\n    \n      Use str_remove from the stringr package to remove category from the start of the strings on the y-axis using the regular expression \"^category\"\n\n    \n  \n  \n    \n      0:57:50\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/kenya-census.html",
    "href": "content_pages/kenya-census.html",
    "title": "Kenya Census",
    "section": "",
    "text": "Notable topics: Heatmap, Joining Datasets, ShapeFile, Choropleth Map, rKenyaCensus\nRecorded on: 2021-01-18\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/kenya-census.html#screencast",
    "href": "content_pages/kenya-census.html#screencast",
    "title": "Kenya Census",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/kenya-census.html#timestamps",
    "href": "content_pages/kenya-census.html#timestamps",
    "title": "Kenya Census",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:15\n    \n    str_trimstringr\n    \n      Trim whitespace from a string.\n\n    \n  \n  \n    \n      0:4:35\n    \n    fct_reorderforcats\n    \n      Reorder factor levels by sorting along another variable by sum.\n\n    \n  \n  \n    \n      0:5:00\n    \n    commascale_x_continuousscalesggplot2\n    \n      Label x axis numbers in decimal format (e.g. 0.12, 1,234).\n\n    \n  \n  \n    \n      0:5:15\n    \n    gathertidyr\n    \n      Pivot data from wide to long.\n\n    \n  \n  \n    \n      0:5:25\n    \n    str_to_titlestringr\n    \n      Convert case of a string to title case.\n\n    \n  \n  \n    \n      0:6:50\n    \n    geom_textggplot2\n    \n      Add text labels to the geom_point plot.\n\n    \n  \n  \n    \n      0:7:35\n    \n    geom_hlineggplot2\n    \n      Add horizontal reference line to geom_point plot.\n\n    \n  \n  \n    \n      0:7:35\n    \n    commascale_y_continuousscalesggplot2\n    \n      Label y axis numbers in percent format.\n\n    \n  \n  \n    \n      0:9:00\n    \n    expand_limitsggplot2\n    \n      Expand the plot axis limits by having the y axis begin at 0.\n\n    \n  \n  \n    \n      0:9:10\n    \n    scale_x_log10ggplot2\n    \n      Position x axis data on a log10 scale.\n\n    \n  \n  \n    \n      0:10:20\n    \n    gathertidyr\n    \n      Pivot data from wide to long.\n\n    \n  \n  \n    \n      0:11:15\n    \n    str_to_titlestringr\n    \n      Convert case of a string to title case.\n\n    \n  \n  \n    \n      0:11:50\n    \n    fct_reorderforcats\n    \n      Reorder factor levels by sorting along another variable by sum.\n\n    \n  \n  \n    \n      0:14:45\n    \n    geom_tileggplot2\n    \n      Create a heatmap.\n\n    \n  \n  \n    \n      0:15:13\n    \n    completetidyr\n    \n      Complete a data frame with missing combinations of data.\n\n    \n  \n  \n    \n      0:15:30\n    \n    themeggplot2\n    \n      Rotate x axis labels 90 degrees.\n\n    \n  \n  \n    \n      0:16:55\n    \n    full_joindplyr\n    \n      Join two datasets while including all rows in x or y.\n\n    \n  \n  \n    \n      0:18:00\n    \n    str_replace_allstringr\n    \n      Replace matched patterns in a string using str_replace_all with the regular expression ([a-z])([A-Z]) and \"\\\\1 \\\\2\" to separate words that were joined together (e.g. TanaRiver, Tana River).\n\n    \n  \n  \n    \n      0:19:40\n    \n    anti_joindplyr\n    \n      Join two datasets while returning all rows from x without a match in y.\n\n    \n  \n  \n    \n      0:19:40\n    \n    right_joindplyr\n    \n      Join two datasets while including all rows in y.\n\n    \n  \n  \n    \n      0:19:40\n    \n    inner_joindplyr\n    \n      Join two datasets while including all rows in x and y.\n\n    \n  \n  \n    \n      0:27:35\n    \n    KenyaCounties_SHPrKenyaCensus\n    \n      Import and basic exploration of the rKenyaCensus package shapefiles.\n\n    \n  \n  \n    \n      0:28:15\n    \n    st_as_sfgeom_sfggplot2sf\n    \n      Create a map using the rKenyaCensus shapefile data.\n\n    \n  \n  \n    \n      0:35:00\n    \n    st_simplifysf\n    \n      Simplify the shapefile data to make for faster processesing.\n\n    \n  \n  \n    \n      0:36:20\n    \n    left_joindplyr\n    \n      Join two datasets while including all rows in x.\n\n    \n  \n  \n    \n      0:37:25\n    \n    ggplotgeom_sftheme_mapggplot2sf\n    \n      Create a choropleth map - TROUBLSHOOTING through 41:45.\n\n    \n  \n  \n    \n      0:43:20\n    \n    filtergathermutategroup_bysummarizespreadselectggplot2dplyrtidyr\n    \n      Create a flexible function that generates geom_col plots used for for exploring the many different datasets in the rKenyaCensus package.\n\n    \n  \n  \n    \n      0:51:55\n    \n    fct_lumpforcats\n    \n      Lump together factor levels into \"other\".\n\n    \n  \n  \n    \n      0:59:20\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/malaria-incidence.html",
    "href": "content_pages/malaria-incidence.html",
    "title": "Malaria Incidence",
    "section": "",
    "text": "Notable topics: Map visualization\nRecorded on: 2018-11-11\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/malaria-incidence.html#screencast",
    "href": "content_pages/malaria-incidence.html#screencast",
    "title": "Malaria Incidence",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/malaria-incidence.html#timestamps",
    "href": "content_pages/malaria-incidence.html#timestamps",
    "title": "Malaria Incidence",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:45\n    \n    malariaAtlas\n    \n      Importing data using the malariaAtlas package\n\n    \n  \n  \n    \n      0:14:10\n    \n    geom_line\n    \n      Using geom_line function to visualize malaria prevalence over time\n\n    \n  \n  \n    \n      0:15:10\n    \n    geom_point\n    \n      Quick map visualization using longitude and latitude coordinates and the geom_point function\n\n    \n  \n  \n    \n      0:18:40\n    \n    borders\n    \n      Using borders function to add Kenyan country borders to map\n\n    \n  \n  \n    \n      0:19:50\n    \n    scale_colour_gradient2\n    \n      Using scale_colour_gradient2 function to change the colour scale of points on the map\n\n    \n  \n  \n    \n      0:20:40\n    \n    arrange\n    \n      Using arrange function to ensure that certain points on a map appear in front of/behind other points\n\n    \n  \n  \n    \n      0:21:50\n    \n    %/%\n    \n      Aggregating data into decades using the truncated division operator %/%\n\n    \n  \n  \n    \n      0:24:45\n    \n    \n    \n      Starting to look at aggregated malaria data (instead of country-specific data)\n\n    \n  \n  \n    \n      0:26:50\n    \n    sampleunique\n    \n      Using sample and unique functions to randomly select a few countries, which are then graphed\n\n    \n  \n  \n    \n      0:28:30\n    \n    last\n    \n      Using last function to select the most recent observation from a set of arranged data\n\n    \n  \n  \n    \n      0:32:55\n    \n    \n    \n      Creating a Bland-Altman plot to explore relationship between current incidence and change in incidence in past 15 years\n\n    \n  \n  \n    \n      0:35:45\n    \n    anti_join\n    \n      Using anti_join function to find which countries are not in the malaria dataset\n\n    \n  \n  \n    \n      0:36:40\n    \n    maps\n    \n      Using the iso3166 dataset set in the maps package to match three-letter country code (i.e., the ISO 3166 code) with country names\n\n    \n  \n  \n    \n      0:38:30\n    \n    geom_polygontheme_voidcoord_map\n    \n      Creating a world map using geom_polygon function (and eventually theme_void and coord_map functions)\n\n    \n  \n  \n    \n      0:39:00\n    \n    \n    \n      Getting rid of Antarctica from world map\n\n    \n  \n  \n    \n      0:42:35\n    \n    facet_wrap\n    \n      Using facet_wrap function to create small multiples of world map for different time periods\n\n    \n  \n  \n    \n      0:47:30\n    \n    \n    \n      Starting to create an animated map of malaria deaths (actual code writing starts at 57:45)\n\n    \n  \n  \n    \n      0:51:25\n    \n    \n    \n      Starting with a single year after working through some bugs\n\n    \n  \n  \n    \n      0:52:10\n    \n    regex_inner_joinfuzzyjoin\n    \n      Using regex_inner_join function from the fuzzyjoin package to join map datasets because one of them has values in regular expressions\n\n    \n  \n  \n    \n      0:55:15\n    \n    str_remove\n    \n      As alternative to fuzzyjoin package in above step, using str_remove function to get rid of unwanted regex\n\n    \n  \n  \n    \n      0:57:45\n    \n    gganimate\n    \n      Starting to turn static map into an animation using gganimate package\n\n    \n  \n  \n    \n      1:02:00\n    \n    \n    \n      The actual animated map\n\n    \n  \n  \n    \n      1:02:35\n    \n    countrycodecountrycode\n    \n      Using countrycode package to filter down to countries in a specific continent (Africa, in this case)\n\n    \n  \n  \n    \n      1:03:55\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/maryland-bridges.html",
    "href": "content_pages/maryland-bridges.html",
    "title": "Maryland Bridges",
    "section": "",
    "text": "Notable topics: Data manipulation, Map visualization\nRecorded on: 2018-11-26\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/maryland-bridges.html#screencast",
    "href": "content_pages/maryland-bridges.html#screencast",
    "title": "Maryland Bridges",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/maryland-bridges.html#timestamps",
    "href": "content_pages/maryland-bridges.html#timestamps",
    "title": "Maryland Bridges",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:9:15\n    \n    geom_line\n    \n      Using geom_line to create an exploratory line graph\n\n    \n  \n  \n    \n      0:10:10\n    \n    %/%\n    \n      Using %/% operator (truncated division) to bin years into decades (e.g., 1980, 1984, and 1987 would all become \"1980\")\n\n    \n  \n  \n    \n      0:12:30\n    \n    \n    \n      Converting two-digit year to four-digit year (e.g., \"16\" becomes \"2016\") by adding 2000 to each one\n\n    \n  \n  \n    \n      0:15:40\n    \n    percent_formatscales\n    \n      Using percent_format function from scales package to get nice-looking axis labels\n\n    \n  \n  \n    \n      0:19:55\n    \n    geom_col\n    \n      Using geom_col to create an ordered nice bar/column graph\n\n    \n  \n  \n    \n      0:21:35\n    \n    replace_na\n    \n      Using replace_na to replace NA values with \"Other\"\n\n    \n  \n  \n    \n      0:27:15\n    \n    \n    \n      Starting exploration of average daily traffic\n\n    \n  \n  \n    \n      0:29:05\n    \n    comma_formatscales\n    \n      Using comma_format function from scales package to get more readable axis labels (e.g., \"1e+05\" becomes \"100,000\")\n\n    \n  \n  \n    \n      0:31:15\n    \n    cut\n    \n      Using cut function to bin continuous variable into customized breaks (also does a mutate within a group_by!)\n\n    \n  \n  \n    \n      0:34:30\n    \n    \n    \n      Starting to make a map\n\n    \n  \n  \n    \n      0:37:00\n    \n    scale_colour_gradient2\n    \n      Encoding a continuous variable to colour, then using scale_colour_gradient2 function to specify colours and midpoint\n\n    \n  \n  \n    \n      0:38:20\n    \n    scale_colour_gradient2\n    \n      Specifying the trans argument (transformation) of the scale_colour_gradient2 function to get a logarithmic scale\n\n    \n  \n  \n    \n      0:45:55\n    \n    str_to_title\n    \n      Using str_to_title function to get values to Title Case (first letter of each word capitalized)\n\n    \n  \n  \n    \n      0:48:35\n    \n    glm\n    \n      Predicting whether bridges are in \"Good\" condition using logistic regression (remember to specify the family argument! Dave fixes this at 52:54)\n\n    \n  \n  \n    \n      0:50:30\n    \n    \n    \n      Explanation of why we should NOT be using an OLS linear regression\n\n    \n  \n  \n    \n      0:51:10\n    \n    augmentbroom\n    \n      Using the augment function from the broom package to illustrate why a linear model is not a good fit\n\n    \n  \n  \n    \n      0:52:05\n    \n    augmentbroom\n    \n      Specifying the type.predict argument in the augment function so that we get the actual predicted probability\n\n    \n  \n  \n    \n      0:54:40\n    \n    \n    \n      Explanation of why the sigmoidal shape of logistic regression can be a drawback\n\n    \n  \n  \n    \n      0:55:05\n    \n    \n    \n      Using a cubic spline model (a type of GAM, Generalized Additive Model) as an alternative to logistic regression\n\n    \n  \n  \n    \n      0:56:00\n    \n    \n    \n      Explanation of the shape that a cubic spline model can take (which logistic regression cannot)\n\n    \n  \n  \n    \n      1:02:15\n    \n    \n    \n      Visualizing the model in a different way, using a coefficient plot\n\n    \n  \n  \n    \n      1:04:35\n    \n    geom_vline\n    \n      Using geom_vline function to add a red reference line to a graph\n\n    \n  \n  \n    \n      1:04:50\n    \n    tidygeom_errorbarh\n    \n      Adding confidence intervals to the coefficient plot by specifying conf.int argument of tidy function and graphing using the geom_errorbarh function\n\n    \n  \n  \n    \n      1:05:35\n    \n    \n    \n      Brief explanation of log-odds coefficients\n\n    \n  \n  \n    \n      1:09:10\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/media-franchise-revenue.html",
    "href": "content_pages/media-franchise-revenue.html",
    "title": "Media Franchise Revenue",
    "section": "",
    "text": "Notable topics: Data manipulation (especially re-ordering factors)\nRecorded on: 2019-06-21\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/media-franchise-revenue.html#screencast",
    "href": "content_pages/media-franchise-revenue.html#screencast",
    "title": "Media Franchise Revenue",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/media-franchise-revenue.html#timestamps",
    "href": "content_pages/media-franchise-revenue.html#timestamps",
    "title": "Media Franchise Revenue",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:9:15\n    \n    semi_join\n    \n      Explaining use of semi_join function to aggregate and filter groups\n\n    \n  \n  \n    \n      0:11:00\n    \n    \n    \n      Putting the largest categories on the bottom of a stacked bar chart\n\n    \n  \n  \n    \n      0:14:30\n    \n    glueglue\n    \n      Using glue function as alternative to paste for combining text, plus good explanation of it\n\n    \n  \n  \n    \n      0:19:30\n    \n    fct_reorder\n    \n      Multiple re-ordering using fct_reorder function of facetted graph (he works through several obstacles)\n\n    \n  \n  \n    \n      0:20:40\n    \n    \n    \n      Re-ordering the position of facetted graphs so that highest total revenue is at top left\n\n    \n  \n  \n    \n      0:26:00\n    \n    \n    \n      Investigating relationship between year created and revenue\n\n    \n  \n  \n    \n      0:26:40\n    \n    geom_text\n    \n      Creating scatter plot with points scaled by size and labelled points (geom_text function)\n\n    \n  \n  \n    \n      0:29:30\n    \n    \n    \n      Summary of screencast up to this point\n\n    \n  \n  \n    \n      0:29:50\n    \n    \n    \n      Starting analysis original media of franchise (e.g., novel, video game, animated film) and revenue type (e.g., box office, merchandise)\n\n    \n  \n  \n    \n      0:33:35\n    \n    fct_reorder\n    \n      Graphing original media and revenue category as facetted bar plot with lots of reordering (ends at around 38:40)\n\n    \n  \n  \n    \n      0:40:30\n    \n    geom_tile\n    \n      Alternative visualization of original media/revenue category using heat map\n\n    \n  \n  \n    \n      0:41:20\n    \n    scale_fill_gradient2\n    \n      Using scale_fill_gradient2 function to specify custom colour scale\n\n    \n  \n  \n    \n      0:42:05\n    \n    \n    \n      Getting rid of gridlines in graph using theme function's panel.grid argument\n\n    \n  \n  \n    \n      0:44:05\n    \n    fct_rev\n    \n      Using fct_rev function to reverse levels of factors\n\n    \n  \n  \n    \n      0:44:35\n    \n    \n    \n      Fixing overlapping axis text with tweaks to theme function's axis.text argument\n\n    \n  \n  \n    \n      0:46:05\n    \n    \n    \n      Reviewing visualization that inspired this dataset\n\n    \n  \n  \n    \n      0:47:25\n    \n    \n    \n      Adding text of total revenue to the end of each bar in a previous graph\n\n    \n  \n  \n    \n      0:50:20\n    \n    paste0\n    \n      Using paste0 function at add a \"B\" (for \"billions\") to the end of text labels on graph\n\n    \n  \n  \n    \n      0:51:35\n    \n    expand_limits\n    \n      Using expand_limits functions to give more space for text labels not to get cut off\n\n    \n  \n  \n    \n      0:53:45\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/medium-articles.html",
    "href": "content_pages/medium-articles.html",
    "title": "Medium Articles",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package)\nRecorded on: 2018-12-03\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/medium-articles.html#screencast",
    "href": "content_pages/medium-articles.html#screencast",
    "title": "Medium Articles",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/medium-articles.html#timestamps",
    "href": "content_pages/medium-articles.html#timestamps",
    "title": "Medium Articles",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:40\n    \n    summarise_atstarts_with\n    \n      Using summarise_at and starts_with functions to quickly sum up all variables starting with \"tag_\"\n\n    \n  \n  \n    \n      0:6:55\n    \n    gather\n    \n      Using gather function (now pivot_longer) to convert topic tag variables from wide to tall (tidy) format\n\n    \n  \n  \n    \n      0:8:10\n    \n    \n    \n      Explanation of how gathering step above will let us find the most/least common tags\n\n    \n  \n  \n    \n      0:9:00\n    \n    median\n    \n      Explanation of using median (instead of mean) as measure of central tendency for number of claps an article got\n\n    \n  \n  \n    \n      0:9:50\n    \n    \n    \n      Visualizing log-normal (ish) distribution of number of claps an article gets\n\n    \n  \n  \n    \n      0:12:05\n    \n    pmin\n    \n      Using pmin function to bin reading times of 10 minutes or more to cap out at 10 minutes\n\n    \n  \n  \n    \n      0:12:35\n    \n    scale_x_continuous\n    \n      Changing scale_x_continuous function's breaks argument to get custom labels and tick marks on a histogram\n\n    \n  \n  \n    \n      0:14:35\n    \n    \n    \n      Discussion of using mean vs. median as measure of central tendency for reading time (he decides on mean)\n\n    \n  \n  \n    \n      0:16:00\n    \n    \n    \n      Starting text mining analysis\n\n    \n  \n  \n    \n      0:16:40\n    \n    unnest_tokenstidytext\n    \n      Using unnest_tokens function from tidytext package to split character string into individual words\n\n    \n  \n  \n    \n      0:17:50\n    \n    anti_jointidytext\n    \n      Explanation of stop words and using anti_join function from tidytext package to get rid of them\n\n    \n  \n  \n    \n      0:20:20\n    \n    str_detect\n    \n      Using str_detect function to filter out \"words\" that are just numbers (e.g., \"2\", \"35\")\n\n    \n  \n  \n    \n      0:22:35\n    \n    \n    \n      Quick analysis of which individual words are associated with more/fewer claps (\"What are the hype words?\")\n\n    \n  \n  \n    \n      0:25:15\n    \n    \n    \n      Using geometric mean as alternative to median to get more distinction between words (note 27:33 where he makes a quick fix)\n\n    \n  \n  \n    \n      0:28:10\n    \n    \n    \n      Starting analysis of clusters of related words (e.g., \"neural\" is linked to \"network\")\n\n    \n  \n  \n    \n      0:30:30\n    \n    pairwise_corwidyr\n    \n      Finding correlations pairs of words using pairwise_cor function from widyr package\n\n    \n  \n  \n    \n      0:34:00\n    \n    ggraphigraph\n    \n      Using ggraph and igraph packages to make network plot of correlated pairs of words\n\n    \n  \n  \n    \n      0:35:00\n    \n    geom_node_text\n    \n      Using geom_node_text to add labels for points (vertices) in the network plot\n\n    \n  \n  \n    \n      0:38:40\n    \n    \n    \n      Filtering original data to only include words appear in the network plot (150 word pairs with most correlation)\n\n    \n  \n  \n    \n      0:40:10\n    \n    \n    \n      Adding colour as a dimension to the network plot, representing geometric mean of claps\n\n    \n  \n  \n    \n      0:40:50\n    \n    scale_colour_gradient2\n    \n      Changing default colour scale to one with Blue = Low and High = Red with scale_colour_gradient2 function\n\n    \n  \n  \n    \n      0:43:15\n    \n    \n    \n      Adding dark outlines to points on network plot with a hack\n\n    \n  \n  \n    \n      0:44:45\n    \n    \n    \n      Starting to predict number of claps based on title tag (Lasso regression)\n\n    \n  \n  \n    \n      0:45:50\n    \n    cast_sparse\n    \n      Explanation of data format needed to conduct Lasso regression (and using cast_sparse function to get sparse matrix)\n\n    \n  \n  \n    \n      0:47:45\n    \n    \n    \n      Bringing in number of claps to the sparse matrix (un-tidy methods)\n\n    \n  \n  \n    \n      0:49:00\n    \n    cv.glmnetglmnet\n    \n      Using cv.glmnet function (cv = cross validated) from glmnet package to run Lasso regression\n\n    \n  \n  \n    \n      0:49:55\n    \n    \n    \n      Finding and fixing mistake in defining Lasso model\n\n    \n  \n  \n    \n      0:51:05\n    \n    \n    \n      Explanation of Lasso model\n\n    \n  \n  \n    \n      0:52:35\n    \n    tidybroom\n    \n      Using tidy function from the broom package to tidy up the Lasso model\n\n    \n  \n  \n    \n      0:54:35\n    \n    \n    \n      Visualizing how specific words affect the prediction of claps as lambda (Lasso's penalty parameter) changes\n\n    \n  \n  \n    \n      1:00:20\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/ncaa-womens-basketball.html",
    "href": "content_pages/ncaa-womens-basketball.html",
    "title": "NCAA Women’s Basketball",
    "section": "",
    "text": "Notable topics: Heatmap, Correlation analysis\nRecorded on: 2020-10-05\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/ncaa-womens-basketball.html#screencast",
    "href": "content_pages/ncaa-womens-basketball.html#screencast",
    "title": "NCAA Women’s Basketball",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/ncaa-womens-basketball.html#timestamps",
    "href": "content_pages/ncaa-womens-basketball.html#timestamps",
    "title": "NCAA Women’s Basketball",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:15:00\n    \n    fct_relevelforcats\n    \n      Use fct_relevel from the forcats package to order the factor levels for the tourney_finish variable.\n\n    \n  \n  \n    \n      0:16:35\n    \n    geom_tilescale_fill_gradient2ggplot2\n    \n      Use geom_tile from the ggplot2 package to create a heatmap to show how far a particular seed ends up going in the tournament.\n\n    \n  \n  \n    \n      0:20:35\n    \n    scale_y_continuousggplot2\n    \n      Use scale_y_continuous from the ggplot2 package with breaks = seq(1, 16) in order to include all 16 seeds.\n\n    \n  \n  \n    \n      0:20:55\n    \n    geom_textscalesggplot2\n    \n      Use geom_text from the ggplot2 package with label = percent(pct) to apply the percentage to each tile in the heatmap.\n\n    \n  \n  \n    \n      0:21:40\n    \n    scale_x_discretescale_y_continuousggplot2\n    \n      Use scale_x_discrete and scale_y_continuous both with expand = c(0, 0) to remove the space between the x and y axis and the heatmap tiles. David calls this flattening.\n\n    \n  \n  \n    \n      0:32:15\n    \n    scale_y_reverseggplot2\n    \n      Use scale_y_reverse to flip the order of the y-axis from 1-16 to 16-1.\n\n    \n  \n  \n    \n      0:34:45\n    \n    corgeom_linestatsggplot2\n    \n      Use cor from the stats package to calculate the correlation between seed and tourney_finish. Then plotted to determine if there is a correlation over time.\n\n    \n  \n  \n    \n      0:39:50\n    \n    geom_smoothggplot2\n    \n      Use geom_smooth with method = \"loess\" to add a smoothing line with confidence bound to aid in seeing the trend between seed and reg_percent.\n\n    \n  \n  \n    \n      0:42:10\n    \n    fct_lumpforcats\n    \n      Use fct_lump from the forcats package to lump together all the conference except for the n most frequent.\n\n    \n  \n  \n    \n      0:42:55\n    \n    geom_jitterggplot2\n    \n      Use geom_jitter from the ggplot2 package instead of geom_boxplot to avoid overplotting which makes it easier to visualize the points that make up the distribution of the seed variable.\n\n    \n  \n  \n    \n      0:47:05\n    \n    geom_smoothggplot2\n    \n      Use geom_smooth with method = \"lm\" to aid in seeing the trend between reg_percent and tourney_w.\n\n    \n  \n  \n    \n      0:54:20\n    \n    .%>%\n    \n      Create a dot pipe function using . and %>% to avoid duplicating summary statistics with summarize.\n\n    \n  \n  \n    \n      0:56:35\n    \n    glueglue\n    \n      Use glue from the glue package to concatenate together school and n_entries on the geo_col y-axis.\n\n    \n  \n  \n    \n      0:59:50\n    \n    \n    \n      Summary of screencast."
  },
  {
    "objectID": "content_pages/ninja-warrior.html",
    "href": "content_pages/ninja-warrior.html",
    "title": "Ninja Warrior",
    "section": "",
    "text": "Notable topics: Log-odds with tidylo package, Graphing with ggplot2\nRecorded on: 2020-12-14\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/ninja-warrior.html#screencast",
    "href": "content_pages/ninja-warrior.html#screencast",
    "title": "Ninja Warrior",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/ninja-warrior.html#timestamps",
    "href": "content_pages/ninja-warrior.html#timestamps",
    "title": "Ninja Warrior",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:35\n    \n    \n    \n      Inspecting the dataset\n\n    \n  \n  \n    \n      0:6:40\n    \n    geom_histogram\n    \n      Using geom_histogram to look at distribution of obstacles in a stage\n\n    \n  \n  \n    \n      0:9:05\n    \n    str_remove\n    \n      Using str_remove function to clean stage names (remove \"(Regional/City)\")\n\n    \n  \n  \n    \n      0:10:40\n    \n    \n    \n      Asking, \"Are there obstacles that are more common in the Finals than Qualifying rounds?\"\n\n    \n  \n  \n    \n      0:10:50\n    \n    bind_log_oddstidylo\n    \n      Using bind_log_odds function from tidylo package to calculate log-odds of obstacles within a stage type\n\n    \n  \n  \n    \n      0:16:05\n    \n    unite\n    \n      Using unite function to combine two columns\n\n    \n  \n  \n    \n      0:18:20\n    \n    \n    \n      Graphing the average position of different obstacles with many, many tweaks to make it look nice\n\n    \n  \n  \n    \n      0:23:10\n    \n    \n    \n      Creating a stacked bar plot of which obstacles appear in which order\n\n    \n  \n  \n    \n      0:30:30\n    \n    \n    \n      Turning stacked bar plot visualization into a custom function\n\n    \n  \n  \n    \n      0:37:40\n    \n    \n    \n      Asking, \"Is there data on how difficult an obstacle is?\"\n\n    \n  \n  \n    \n      0:45:30\n    \n    geom_tile\n    \n      Visualizing which obstacles appear in different seasons with geom_tile and a lot of tweaking\n\n    \n  \n  \n    \n      0:50:22\n    \n    \n    \n      Reviewing the result of the previous step (obstacles in different seasons)\n\n    \n  \n  \n    \n      0:59:25\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/nobel-prize-winners.html",
    "href": "content_pages/nobel-prize-winners.html",
    "title": "Nobel Prize Winners",
    "section": "",
    "text": "Notable topics: Data manipulation, Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2019-05-23\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/nobel-prize-winners.html#screencast",
    "href": "content_pages/nobel-prize-winners.html#screencast",
    "title": "Nobel Prize Winners",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/nobel-prize-winners.html#timestamps",
    "href": "content_pages/nobel-prize-winners.html#timestamps",
    "title": "Nobel Prize Winners",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:00\n    \n    geom_col%/%\n    \n      Creating a stacked bar plot using geom_col and the aes function's fill argument (also bins years into decades with truncated division operator %/%)\n\n    \n  \n  \n    \n      0:3:30\n    \n    n_distinct\n    \n      Using n_distinct function to quickly count unique years in a group\n\n    \n  \n  \n    \n      0:9:00\n    \n    distinct\n    \n      Using distinct function and its .keep_all argument to de-duplicate data\n\n    \n  \n  \n    \n      0:10:50\n    \n    coalesce\n    \n      Using coalesce function to replace NAs in a variable (similar to SQL COALESCE verb)\n\n    \n  \n  \n    \n      0:16:10\n    \n    yearlubridate\n    \n      Using year function from lubridate package to calculate (approx.) age of laureates at time of award\n\n    \n  \n  \n    \n      0:16:50\n    \n    fct_reorder\n    \n      Using fct_reorder function to arrange boxplot graph by the median age of winners\n\n    \n  \n  \n    \n      0:22:50\n    \n    count\n    \n      Defining a new variable within the count function (like doing a mutate in the count function)\n\n    \n  \n  \n    \n      0:23:40\n    \n    geom_colfacet_wrap\n    \n      Creating a small multiples bar plot using geom_col and facet_wrap functions\n\n    \n  \n  \n    \n      0:26:15\n    \n    WDIsearchWDI\n    \n      Importing income data from WDI package to explore relationship between high/low income countries and winners\n\n    \n  \n  \n    \n      0:33:45\n    \n    fct_relevel\n    \n      Using fct_relevel to change the levels of a categorical income variable (e.g., \"Upper middle income\") so that the ordering makes sense\n\n    \n  \n  \n    \n      0:36:25\n    \n    \n    \n      Starting to explore new dataset of nobel laureate publications\n\n    \n  \n  \n    \n      0:44:25\n    \n    mean\n    \n      Taking the mean of a subset of data without needing to fully filter the data beforehand\n\n    \n  \n  \n    \n      0:49:15\n    \n    rank\n    \n      Using rank function and its ties.method argument to add the ordinal number of a laureate's publication (e.g., 1st paper, 2nd paper)\n\n    \n  \n  \n    \n      1:05:10\n    \n    geom_histogram\n    \n      Lots of playing around with exploratory histograms (geom_histogram)\n\n    \n  \n  \n    \n      1:06:45\n    \n    \n    \n      Discussion of right-censoring as an issue (people winning the Nobel prize but still having active careers)\n\n    \n  \n  \n    \n      1:10:20\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/nyc-restaurant-inspections.html",
    "href": "content_pages/nyc-restaurant-inspections.html",
    "title": "NYC Restaurant Inspections",
    "section": "",
    "text": "Notable topics: Multiple t-test models (broom package), Principal Component Analysis (PCA)\nRecorded on: 2018-12-10\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/nyc-restaurant-inspections.html#screencast",
    "href": "content_pages/nyc-restaurant-inspections.html#screencast",
    "title": "NYC Restaurant Inspections",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/nyc-restaurant-inspections.html#timestamps",
    "href": "content_pages/nyc-restaurant-inspections.html#timestamps",
    "title": "NYC Restaurant Inspections",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:18:45\n    \n    separate\n    \n      Separating column using separate function\n\n    \n  \n  \n    \n      0:21:15\n    \n    distinct\n    \n      Taking distinct observation, but keeping the remaining variables using distinct function with .keep_all argument\n\n    \n  \n  \n    \n      0:25:00\n    \n    nestt.testbroom\n    \n      Using broom package and nest function to perform multiple t-tests at the same time\n\n    \n  \n  \n    \n      0:26:20\n    \n    broom\n    \n      Tidying nested t-test models using broom package\n\n    \n  \n  \n    \n      0:27:00\n    \n    \n    \n      Creating TIE fighter plot of estimates of means and their confidence intervals\n\n    \n  \n  \n    \n      0:28:45\n    \n    \n    \n      Recode long description using regex to remove everything after a parenthesis\n\n    \n  \n  \n    \n      0:33:45\n    \n    cut\n    \n      Using cut function to manually bin data along user-specified intervals\n\n    \n  \n  \n    \n      0:42:00\n    \n    \n    \n      Asking, \"What type of violations tend to occur more in some cuisines than others?\"\n\n    \n  \n  \n    \n      0:42:45\n    \n    semi_join\n    \n      Using semi_join function to get the most recent inspection of all the restaurants\n\n    \n  \n  \n    \n      0:52:00\n    \n    \n    \n      Asking, \"What violations tend to occur together?\"\n\n    \n  \n  \n    \n      0:53:00\n    \n    pairwise_corwidyr\n    \n      Using widyr package function pairwise_cor (pairwise correlation) to find co-occurrence of violation types\n\n    \n  \n  \n    \n      0:55:30\n    \n    widely_svd\n    \n      Beginning of PCA (Principal Component Analysis) using widely_svd function\n\n    \n  \n  \n    \n      0:58:00\n    \n    widely_svd\n    \n      Actually typing in the widely_svd function\n\n    \n  \n  \n    \n      0:58:15\n    \n    widely_svd\n    \n      Reviewing and explaining output of widely_svd function\n\n    \n  \n  \n    \n      1:01:30\n    \n    \n    \n      Creating graph of opposing elements of a PCA dimension\n\n    \n  \n  \n    \n      1:02:00\n    \n    str_sub\n    \n      Shortening string using str_sub function\n\n    \n  \n  \n    \n      1:04:00\n    \n    \n    \n      Reference to Julia Silge's PCA walkthrough using StackOverflow data: https://juliasilge.com/blog/stack-overflow-pca/"
  },
  {
    "objectID": "content_pages/nyc-squirrel-census.html",
    "href": "content_pages/nyc-squirrel-census.html",
    "title": "NYC Squirrel Census",
    "section": "",
    "text": "Notable topics: Map visualization (ggmap package)\nRecorded on: 2019-10-31\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/nyc-squirrel-census.html#screencast",
    "href": "content_pages/nyc-squirrel-census.html#screencast",
    "title": "NYC Squirrel Census",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/nyc-squirrel-census.html#timestamps",
    "href": "content_pages/nyc-squirrel-census.html#timestamps",
    "title": "NYC Squirrel Census",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:45\n    \n    geom_point\n    \n      Starter EDA of latitude and longitude using geom_point\n\n    \n  \n  \n    \n      0:6:45\n    \n    \n    \n      Aggregating squirrel counts by hectare to get a \"binned\" map\n\n    \n  \n  \n    \n      0:9:00\n    \n    \n    \n      Investigating colour notes\n\n    \n  \n  \n    \n      0:10:30\n    \n    \n    \n      Asking question, \"Are there areas of the parks where we see certain-coloured squirrels\n\n    \n  \n  \n    \n      0:12:45\n    \n    \n    \n      Plotting latitude and percentage of gray squirrels to answer, \"Do we get a lower proportion of gray squirrels as we go farther north?\"\n\n    \n  \n  \n    \n      0:13:30\n    \n    \n    \n      Using logistic regression to test gray squirrel (proportion as we go farther north)\n\n    \n  \n  \n    \n      0:16:30\n    \n    \n    \n      Noting that he could have used original data sets as input for logistic regression function\n\n    \n  \n  \n    \n      0:19:30\n    \n    \n    \n      \"Does a squirrel run away?\" based on location in the park (latitude), using logistic regression\n\n    \n  \n  \n    \n      0:20:45\n    \n    summarise_at\n    \n      Using summarise_at function to apply same function to multiple variables\n\n    \n  \n  \n    \n      0:25:25\n    \n    ggmap\n    \n      Loading ggmap package\n\n    \n  \n  \n    \n      0:27:00\n    \n    get_mapggmap\n    \n      Start using ggmap, with the get_map function\n\n    \n  \n  \n    \n      0:28:20\n    \n    \n    \n      Decision to not set up Google API key to use ggmap properly\n\n    \n  \n  \n    \n      0:30:15\n    \n    sf\n    \n      Using the sf package to read in a shapefile of Central Park\n\n    \n  \n  \n    \n      0:30:40\n    \n    read_sfsf\n    \n      Using read_sf function from sf package to import a shapefile into R\n\n    \n  \n  \n    \n      0:31:30\n    \n    geom_sfsf\n    \n      Using geom_sf function from sf package to visualise the imported shapefile\n\n    \n  \n  \n    \n      0:32:45\n    \n    \n    \n      Combining shapefile \"background\" with relevant squirrel data in one plot\n\n    \n  \n  \n    \n      0:34:40\n    \n    \n    \n      Visualising pathways (footpaths, bicycle paths) in the shapefile\n\n    \n  \n  \n    \n      0:37:55\n    \n    \n    \n      Finishing visualisation and moving on to analysing activity types\n\n    \n  \n  \n    \n      0:38:45\n    \n    ends_withgather\n    \n      Selecting fields based on whether they end with \"ing\", then gathering those fields into tidy format\n\n    \n  \n  \n    \n      0:39:50\n    \n    shiny\n    \n      Decision to create a Shiny visualisation\n\n    \n  \n  \n    \n      0:41:30\n    \n    shiny\n    \n      Setting Shiny app settings (e.g., slider for minimum number of squirrels)\n\n    \n  \n  \n    \n      0:42:15\n    \n    shiny\n    \n      Setting up Shiny app options / variables\n\n    \n  \n  \n    \n      0:43:50\n    \n    shiny\n    \n      Explanation of why setting up options in Shiny app the way he did\n\n    \n  \n  \n    \n      0:46:00\n    \n    shiny\n    \n      Solving error \"Discrete value supplied to continuous scale\"\n\n    \n  \n  \n    \n      0:46:50\n    \n    shiny\n    \n      First draft of Shiny app\n\n    \n  \n  \n    \n      0:48:35\n    \n    shiny\n    \n      Creating a dynamic midpoint for the two-gradient scale in the app\n\n    \n  \n  \n    \n      0:51:30\n    \n    shiny\n    \n      Adding additional variables of more behaviours to Shiny app (kuks, moans, runs from, etc.)\n\n    \n  \n  \n    \n      0:53:10\n    \n    \n    \n      \"What are the distributions of some of these [behaviours]?\"\n\n    \n  \n  \n    \n      0:56:50\n    \n    shiny\n    \n      Adding ground location (above ground, ground plane) to Shiny app\n\n    \n  \n  \n    \n      0:58:20\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/palmer-penguins.html",
    "href": "content_pages/palmer-penguins.html",
    "title": "Palmer Penguins",
    "section": "",
    "text": "Notable topics: Modeling (logistic regression, k-nearest neighbors, decision tree, multiclass logistic regression) with cross validated accuracy\nRecorded on: 2020-07-27\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/palmer-penguins.html#screencast",
    "href": "content_pages/palmer-penguins.html#screencast",
    "title": "Palmer Penguins",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/palmer-penguins.html#timestamps",
    "href": "content_pages/palmer-penguins.html#timestamps",
    "title": "Palmer Penguins",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:11:17\n    \n    pivot_longergeom_histogramfacet_wraptidyrggplot2\n    \n      Create a pivoted histogram plot to visualize the distribution of penguin metrics using pivot_longer, geom_histogram, and facet_wrap\n\n    \n  \n  \n    \n      0:14:40\n    \n    geom_densityfacet_wrapggplot2\n    \n      Create a pivoted density plot  to visualize the distribution of penguin metrics using geom_density and facet_wrap\n\n    \n  \n  \n    \n      0:15:21\n    \n    geom_boxplotfacet_wrapggplot2\n    \n      Create a pivoted boxplot plot to visualize the distribution of penguin metrics using geom_boxplot and facet_wrap\n\n    \n  \n  \n    \n      0:17:50\n    \n    geom_barggplot2\n    \n      Create a bar plot to show penguin species changed over time\n\n    \n  \n  \n    \n      0:18:25\n    \n    geom_barggplot2\n    \n      Create a bar plot to show specie counts per island\n\n    \n  \n  \n    \n      0:20:00\n    \n    initital_splittraininglogistic_regset_enginefitfct_lumppredictmetricsvfold_cvfit_resamplescollect_metricstidymodelsrsampleparsnip\nyardstick\n    \n      Create a logistic regression model to predict if a penguin is Adelie or not using bill length with cross validaiton of metrics\n\n    \n  \n  \n    \n      0:39:35\n    \n    initital_splittraininglogistic_regset_enginefitfct_lumppredictmetricsvfold_cvfit_resamplescollect_metricstidymodelsrsampleparsnip\nyardstick\n    \n      Create second logistic regression model using 4 predictive metrics (bill length, bill depth, flipper length, body mass) and then compare the accuracy of both models\n\n    \n  \n  \n    \n      0:43:25\n    \n    nearest_neighborinitital_splittraininglogistic_regset_enginefitfct_lumppredictmetricsvfold_cvfit_resamplescollect_metricstidymodelsrsampleparsnip\nyardstick\n    \n      Create a k-nearest neighbor model and then compare accuracy against logistic regression models to see which has the highest cross validated accuracy\n\n    \n  \n  \n    \n      0:53:05\n    \n    testingpredictmetricsrsamplestatsyardstick\n    \n      What is the accuracy of the testing holdout data on the k-nearest neighbor model?\n\n    \n  \n  \n    \n      1:05:40\n    \n    decision_treeset_engineparsnip\n    \n      Create a decision tree and then compare accuracy against the previous models to see which has the highest cross validated accuracy + how to extract a decision tree\n\n    \n  \n  \n    \n      1:10:45\n    \n    multinom_regset_enginefit_resamplesparsniptune\n    \n      Perform multi class regression using multinom_reg\n\n    \n  \n  \n    \n      1:19:40\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/pizza-ratings.html",
    "href": "content_pages/pizza-ratings.html",
    "title": "Pizza Ratings",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-09-30\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/pizza-ratings.html#screencast",
    "href": "content_pages/pizza-ratings.html#screencast",
    "title": "Pizza Ratings",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/pizza-ratings.html#timestamps",
    "href": "content_pages/pizza-ratings.html#timestamps",
    "title": "Pizza Ratings",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:45\n    \n    \n    \n      Transforming time into something more readable (from time value of seconds since Unix epoch [1970-01-01] ), then converting it into a date\n\n    \n  \n  \n    \n      0:9:05\n    \n    fct_relevel\n    \n      Formatting x-axis text so that it is rotated and readable, then re-ordering using fct_relevel function so that it is in its proper ordinal order\n\n    \n  \n  \n    \n      0:11:00\n    \n    \n    \n      Converting string answers to integer counterparts to get an overall numeric value for how good each place is\n\n    \n  \n  \n    \n      0:12:30\n    \n    \n    \n      Commentary on speed of mutate calculation within or without a group (non-grouped is slightly faster)\n\n    \n  \n  \n    \n      0:15:30\n    \n    fct_reorder\n    \n      Re-ordering groups by total votes using fct_reorder function, while still maintaining the groups themselves\n\n    \n  \n  \n    \n      0:19:15\n    \n    glueglue\n    \n      Using glue package to combine place name and total respondents\n\n    \n  \n  \n    \n      0:20:30\n    \n    \n    \n      Using statistical test to give confidence intervals on average score\n\n    \n  \n  \n    \n      0:22:15\n    \n    t.test\n    \n      Actually using the t.test function with toy example\n\n    \n  \n  \n    \n      0:23:15\n    \n    \n    \n      Using weighted linear model instead (which doesn't end up working)\n\n    \n  \n  \n    \n      0:26:00\n    \n    rep\n    \n      Using custom function with rep function to get vector of repeated scores (sneaky way of weighting) so that we can perform a proper t-test\n\n    \n  \n  \n    \n      0:27:30\n    \n    \n    \n      Summarizing t-test function into a list (alternative to nesting)\n\n    \n  \n  \n    \n      0:31:20\n    \n    geom_errorbarh\n    \n      Adding error bars using geom_errorbarh to make a TIE fighter plot that shows confidence intervals\n\n    \n  \n  \n    \n      0:36:30\n    \n    \n    \n      Bringing in additional data from Barstool ratings (to supplement survey of Open R meetup NY)\n\n    \n  \n  \n    \n      0:39:45\n    \n    \n    \n      Getting survey data to the place level so that we can add an additional dataset\n\n    \n  \n  \n    \n      0:41:15\n    \n    \n    \n      Checking for duplicates in the joined data\n\n    \n  \n  \n    \n      0:42:15\n    \n    \n    \n      Calling off the planned analysis due to low sample sizes (too much noise, not enough overlap between datasets)\n\n    \n  \n  \n    \n      0:45:15\n    \n    \n    \n      Looking at Barstool data on its own\n\n    \n  \n  \n    \n      0:55:15\n    \n    \n    \n      Renaming all variables with a certain string pattern in them\n\n    \n  \n  \n    \n      0:58:00\n    \n    \n    \n      Comparing Dave's reviews with all other critics\n\n    \n  \n  \n    \n      0:59:15\n    \n    geom_abline\n    \n      Adding geom_abline showing x = y as comparison for geom_smooth linear model line\n\n    \n  \n  \n    \n      1:02:30\n    \n    \n    \n      Changing the location of the aes() to change what the legend icons look like for size aesthetic"
  },
  {
    "objectID": "content_pages/plants-in-danger.html",
    "href": "content_pages/plants-in-danger.html",
    "title": "Plants in Danger",
    "section": "",
    "text": "Notable topics: Data manipulation, Web scraping (rvest package) and SelectorGadget\nRecorded on: 2020-08-17\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/plants-in-danger.html#screencast",
    "href": "content_pages/plants-in-danger.html#screencast",
    "title": "Plants in Danger",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/plants-in-danger.html#timestamps",
    "href": "content_pages/plants-in-danger.html#timestamps",
    "title": "Plants in Danger",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:00\n    \n    countfct_lumpfct_reorderdplyrforcats\n    \n      Using count, fct_lump, and fct_reorder to get an overview of categorical data\n\n    \n  \n  \n    \n      0:5:00\n    \n    fct_relevelforcats\n    \n      Using fct_relevel to reorder the \"Before 1900\" level to the first location leaving the other levels in their existing order\n\n    \n  \n  \n    \n      0:8:05\n    \n    fct_reorderforcats\n    \n      Using n and sum in fct_reorder to reorder factor levels when there are multiple categories in count\n\n    \n  \n  \n    \n      0:12:00\n    \n    reorder_withinscale_y_reorderedtidytext\n    \n      Using reorder_within and scale_y_reordered such that the values are ordered within each facet\n\n    \n  \n  \n    \n      0:14:55\n    \n    axis.text.xggplot2\n    \n      Using `axis.text.x\" to rotate overlapping labels\n\n    \n  \n  \n    \n      0:19:05\n    \n    filterfct_lumpdplyrforcats\n    \n      Using filter and fct_lump to lump all levels except for the 8 most frequest facet panels\n\n    \n  \n  \n    \n      0:26:55\n    \n    separatetidyr\n    \n      Using separate to separate the character column binomial_name into multiple columns (genus and species)\n\n    \n  \n  \n    \n      0:28:20\n    \n    fct_lumpforcats\n    \n      Using fct_lump within count to lump all levels except for the 8 most frequent genus\n\n    \n  \n  \n    \n      0:45:30\n    \n    read_htmlhtml_nodeshtml_textrvest\n    \n      Using rvest and SelectorGadget to web scrape list of species\n\n    \n  \n  \n    \n      0:49:35\n    \n    str_trimstringr\n    \n      Using str_trim to remove whitespace from character string\n\n    \n  \n  \n    \n      0:50:00\n    \n    separatetidyr\n    \n      Using separate to separate character string into genus, species, and rest/citation columns and using extra = \"merge\" to merge extra pieces into the rest/citation column\n\n    \n  \n  \n    \n      0:51:00\n    \n    read_htmlhtml_nodeshtml_text \nhtml_attrinner_joinpaste0maprvestdplyr\npurrr\n    \n      Using rvest and SelectorGadget to web scrape image links\n\n    \n  \n  \n    \n      0:57:50\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/plastic-waste.html",
    "href": "content_pages/plastic-waste.html",
    "title": "Plastic Waste",
    "section": "",
    "text": "Notable topics: Map visualization (especially choropleth)\nRecorded on: 2019-05-26\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/plastic-waste.html#screencast",
    "href": "content_pages/plastic-waste.html#screencast",
    "title": "Plastic Waste",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/plastic-waste.html#timestamps",
    "href": "content_pages/plastic-waste.html#timestamps",
    "title": "Plastic Waste",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:45\n    \n    summarise_all\n    \n      Using summarise_all to get proportion of NA values across many variables\n\n    \n  \n  \n    \n      0:16:50\n    \n    geom_text\n    \n      Adding text labels to scatter plot for some points using check_overlap argument\n\n    \n  \n  \n    \n      0:21:45\n    \n    pmin\n    \n      Using pmin function to get the lower of two possible numbers for a percentage variable that was showing > 100%\n\n    \n  \n  \n    \n      0:29:00\n    \n    \n    \n      Starting to make a choropleth map\n\n    \n  \n  \n    \n      0:29:30\n    \n    \n    \n      Connecting ISO country names (used in mapping code) to country names given in the dataset\n\n    \n  \n  \n    \n      0:32:00\n    \n    \n    \n      Actual code to create the map using given longitude and latitude\n\n    \n  \n  \n    \n      0:33:45\n    \n    regex_left_joinfuzzyjoin\n    \n      Using fuzzyjoin package to link variables that use regular expression instead of character (using regex_right_join / regex_left_join function)\n\n    \n  \n  \n    \n      0:36:15\n    \n    coord_fixed\n    \n      Using coord_fixed function as a hack to get proper ratios for maps\n\n    \n  \n  \n    \n      0:39:30\n    \n    \n    \n      Bringing in additional data using WDI package\n\n    \n  \n  \n    \n      0:47:30\n    \n    patchwork\n    \n      Using patchwork package to show multiple graphs in the same plot\n\n    \n  \n  \n    \n      0:53:00\n    \n    \n    \n      Importing and rename multiple indicators from the WDI package at the same time"
  },
  {
    "objectID": "content_pages/r-downloads.html",
    "href": "content_pages/r-downloads.html",
    "title": "R Downloads",
    "section": "",
    "text": "Notable topics: Data manipulation (especially time series)\nRecorded on: 2018-10-29\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/r-downloads.html#screencast",
    "href": "content_pages/r-downloads.html#screencast",
    "title": "R Downloads",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/r-downloads.html#timestamps",
    "href": "content_pages/r-downloads.html#timestamps",
    "title": "R Downloads",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:20\n    \n    geom_line\n    \n      Using geom_line function to visualize changes over time\n\n    \n  \n  \n    \n      0:7:35\n    \n    lubridate\n    \n      Starting to decompose time series data into day-of-week trend and overall trend (lots of lubridate package functions)\n\n    \n  \n  \n    \n      0:9:50\n    \n    \n    \n      Using floor_date function from lubridate package to round dates down to the week level\n\n    \n  \n  \n    \n      0:10:05\n    \n    \n    \n      Using min function to drop incomplete/partial week at the start of the dataset\n\n    \n  \n  \n    \n      0:12:20\n    \n    countrycodecountrycode\n    \n      Using countrycode function from countrycode package to replace two-letter country codes with full names (e.g., \"CA\" becomes \"Canada\")\n\n    \n  \n  \n    \n      0:17:20\n    \n    fct_lump\n    \n      Using fct_lump function to get top N categories within a categorical variable and classify the rest as \"Other\"\n\n    \n  \n  \n    \n      0:20:30\n    \n    hourlubridate\n    \n      Using hour function from lubridate package to pull out integer hour value from a datetime variable\n\n    \n  \n  \n    \n      0:22:20\n    \n    facet_wrap\n    \n      Using facet_wrap function to graph small multiples of downloads by country, then changing scales argument to allow different scales on y-axis\n\n    \n  \n  \n    \n      0:31:00\n    \n    \n    \n      Starting analysis of downloads by IP address\n\n    \n  \n  \n    \n      0:35:20\n    \n    as.POSIXlt\n    \n      Using as.POSIXlt to combine separate date and time variables to get a single datetime variable\n\n    \n  \n  \n    \n      0:36:35\n    \n    lag\n    \n      Using lag function to calculate time between downloads (time between events) per IP address (comparable to SQL window function)\n\n    \n  \n  \n    \n      0:38:05\n    \n    as.numeric\n    \n      Using as.numeric function to convert variable from a time interval object to a numeric variable (number in seconds)\n\n    \n  \n  \n    \n      0:38:40\n    \n    \n    \n      Explanation of a bimodal log-normal distribution\n\n    \n  \n  \n    \n      0:39:05\n    \n    scale_x_log10\n    \n      Handy trick for setting easy-to-interpret intervals for time data on scale_x_log10 function's breaks argument\n\n    \n  \n  \n    \n      0:47:40\n    \n    \n    \n      Starting to explore package downloads\n\n    \n  \n  \n    \n      0:52:15\n    \n    \n    \n      Adding 1 to the numerator and denominator when calculating a ratio to get around dividing by zero\n\n    \n  \n  \n    \n      0:57:55\n    \n    cran_downloadscranlogs\n    \n      Showing how to look at package download data over time using cran_downloads function from the cranlogs package"
  },
  {
    "objectID": "content_pages/r-trick-creating-pascals-triangle-with-accumulate.html",
    "href": "content_pages/r-trick-creating-pascals-triangle-with-accumulate.html",
    "title": "R trick: Creating Pascal’s Triangle with accumulate()",
    "section": "",
    "text": "Notable topics: accumulate() function for recursive formulas\nRecorded on: 2020-03-28\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/r-trick-creating-pascals-triangle-with-accumulate.html#screencast",
    "href": "content_pages/r-trick-creating-pascals-triangle-with-accumulate.html#screencast",
    "title": "R trick: Creating Pascal’s Triangle with accumulate()",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/r-trick-creating-pascals-triangle-with-accumulate.html#timestamps",
    "href": "content_pages/r-trick-creating-pascals-triangle-with-accumulate.html#timestamps",
    "title": "R trick: Creating Pascal’s Triangle with accumulate()",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:10\n    \n    accumulate\n    \n      Simple explanation of accumulate function\n\n    \n  \n  \n    \n      0:1:30\n    \n    \n    \n      Example using letters\n\n    \n  \n  \n    \n      0:2:55\n    \n    ~\n    \n      Using tilde ~ to create an anonymous function\n\n    \n  \n  \n    \n      0:4:35\n    \n    \n    \n      Introducing Pascal's Triangle\n\n    \n  \n  \n    \n      0:6:25\n    \n    \n    \n      Starting to create Pascal's triangle in R\n\n    \n  \n  \n    \n      0:8:05\n    \n    accumulate\n    \n      Concerting the conceptual solution into an accumulate function"
  },
  {
    "objectID": "content_pages/ramen-reviews.html",
    "href": "content_pages/ramen-reviews.html",
    "title": "Ramen Reviews",
    "section": "",
    "text": "Notable topics: Web scraping (rvest package)\nRecorded on: 2019-06-03\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/ramen-reviews.html#screencast",
    "href": "content_pages/ramen-reviews.html#screencast",
    "title": "Ramen Reviews",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/ramen-reviews.html#timestamps",
    "href": "content_pages/ramen-reviews.html#timestamps",
    "title": "Ramen Reviews",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:1:45\n    \n    \n    \n      Looking at the website the data came from\n\n    \n  \n  \n    \n      0:2:55\n    \n    gather\n    \n      Using gather function (now pivot_longer) to convert wide data to long (tidy) format\n\n    \n  \n  \n    \n      0:4:15\n    \n    \n    \n      Graphing counts of all categorical variables at once, then exploring them\n\n    \n  \n  \n    \n      0:5:35\n    \n    fct_lump\n    \n      Using fct_lump function to lump three categorical variables to the top N categories and \"Other\"\n\n    \n  \n  \n    \n      0:7:45\n    \n    reorder_within\n    \n      Using reorder_within function to re-order factors that have the same name across multiple facets\n\n    \n  \n  \n    \n      0:9:10\n    \n    lm\n    \n      Using lm function (linear model) to predict star rating\n\n    \n  \n  \n    \n      0:9:50\n    \n    \n    \n      Visualising effects (and 95% CI) of indendent variables in linear model with a coefficient plot (TIE fighter plot)\n\n    \n  \n  \n    \n      0:11:30\n    \n    fct_relevel\n    \n      Using fct_relevel function to get \"Other\" as the base reference level for categorical independent variables in a linear model\n\n    \n  \n  \n    \n      0:13:05\n    \n    extract\n    \n      Using extract function and regex to split a camelCase variable into two separate variables\n\n    \n  \n  \n    \n      0:14:45\n    \n    facet_wrap\n    \n      Using facet_wrap function to split coefficient / TIE fighter plot into three separate plots, based on type of coefficient\n\n    \n  \n  \n    \n      0:15:40\n    \n    geom_vline\n    \n      Using geom_vline function to add reference line to graph\n\n    \n  \n  \n    \n      0:17:20\n    \n    unnest_tokenstidytext\n    \n      Using unnest_tokens function from tidytext package to explore the relationship between variety (a sparse categorical variable) and star rating\n\n    \n  \n  \n    \n      0:18:55\n    \n    \n    \n      Explanation of how he would approach variety variable with Lasso regression\n\n    \n  \n  \n    \n      0:19:35\n    \n    rvest\n    \n      Web scraping the using rvest package and SelectorGadget (Chrome Extension CSS selector)\n\n    \n  \n  \n    \n      0:21:20\n    \n    read_htmlhtml_nodehtml_tablervest\n    \n      Actually writing code for web scraping, using read_html, html_node, and html_table functions\n\n    \n  \n  \n    \n      0:22:25\n    \n    clean_namesjanitor\n    \n      Using clean_names function from janitor package to clean up names of variables\n\n    \n  \n  \n    \n      0:23:05\n    \n    \n    \n      Explanation of web scraping task: get full review text using the links from the review summary table scraped above\n\n    \n  \n  \n    \n      0:25:40\n    \n    parse_number\n    \n      Using parse_number function as alternative to as.integer function to cleverly drop extra weird text in review number\n\n    \n  \n  \n    \n      0:26:45\n    \n    \n    \n      Using SelectorGadget (Chrome Extension CSS selector) to identify part of page that contains review text\n\n    \n  \n  \n    \n      0:27:35\n    \n    html_nodeshtml_textstr_subsetrvest\n    \n      Using html_nodes, html_text, and str_subset functions to write custom function to scrape review text identified in step above\n\n    \n  \n  \n    \n      0:29:15\n    \n    message\n    \n      Adding message function to custom scraping function to display URLs as they are being scraped\n\n    \n  \n  \n    \n      0:30:15\n    \n    unnest_tokensanti_join\n    \n      Using unnest_tokens and anti_join functions to split review text into individual words and remove stop words (e.g., \"the\", \"or\", \"and\")\n\n    \n  \n  \n    \n      0:31:05\n    \n    \n    \n      Catching a mistake in the custom function causing it to read the same URL every time\n\n    \n  \n  \n    \n      0:31:55\n    \n    str_detect\n    \n      Using str_detect function to filter out review paragraphs without a keyword in it\n\n    \n  \n  \n    \n      0:32:40\n    \n    str_remove\n    \n      Using str_remove function and regex to get rid of string that follows a specific pattern\n\n    \n  \n  \n    \n      0:34:10\n    \n    possiblysafelypurrr\n    \n      Explanation of possibly and safely functions in purrr package\n\n    \n  \n  \n    \n      0:37:45\n    \n    \n    \n      Reviewing output of the URL that failed to scrape, including using character(0) as a default null value\n\n    \n  \n  \n    \n      0:48:00\n    \n    pairwise_corwidyr\n    \n      Using pairwise_cor function from widyr package to see which words tend to appear in reviews together\n\n    \n  \n  \n    \n      0:51:05\n    \n    igraphggraph\n    \n      Using igraph and ggraph packages to make network plot of word correlations\n\n    \n  \n  \n    \n      0:51:55\n    \n    geom_node_textigraphggraph\n    \n      Using geom_node_text function to add labels to network plot\n\n    \n  \n  \n    \n      0:52:35\n    \n    igraphggraph\n    \n      Including all words (not just those connected to others) as vertices in the network plot\n\n    \n  \n  \n    \n      0:54:40\n    \n    \n    \n      Tweaking and refining network plot aesthetics (vertex size and colour)\n\n    \n  \n  \n    \n      0:56:00\n    \n    \n    \n      Weird hack for getting a dark outline on hard-to-see vertex points\n\n    \n  \n  \n    \n      0:59:15\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/riddler-monte-carlo-simulation.html",
    "href": "content_pages/riddler-monte-carlo-simulation.html",
    "title": "Riddler: Monte Carlo Simulation",
    "section": "",
    "text": "Notable topics: Simulation\nRecorded on: 2018-12-03\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-monte-carlo-simulation.html#screencast",
    "href": "content_pages/riddler-monte-carlo-simulation.html#screencast",
    "title": "Riddler: Monte Carlo Simulation",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-monte-carlo-simulation.html#timestamps",
    "href": "content_pages/riddler-monte-carlo-simulation.html#timestamps",
    "title": "Riddler: Monte Carlo Simulation",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:3:10\n    \n    crossing\n    \n      Using crossing function to set up structure of simulation (1,000 trials, each with 12 chess games)\n\n    \n  \n  \n    \n      0:4:00\n    \n    \n    \n      Adding result to the tidy simulation dataset\n\n    \n  \n  \n    \n      0:6:45\n    \n    sample\n    \n      Using sample function to simulate win/loss/draw for each game (good explanation of individual arguments within sample)\n\n    \n  \n  \n    \n      0:7:05\n    \n    group_bysummarise\n    \n      Using group_by and summarise to get total points for each trial\n\n    \n  \n  \n    \n      0:8:10\n    \n    \n    \n      Adding red vertical reference line to histogram to know when a player wins a matchup\n\n    \n  \n  \n    \n      0:10:00\n    \n    \n    \n      Answering second piece of riddle (how many games would need to be played for better player to win 90% or 99% of the time?)\n\n    \n  \n  \n    \n      0:10:50\n    \n    unnestseq_len\n    \n      Using unnest and seq_len function to create groups of number of games (20, 40, …, 100), each with one game per row\n\n    \n  \n  \n    \n      0:12:15\n    \n    \n    \n      Creating a win field based on the simulated data, then summarising win percentage for each group of number of games (20, 40, …, 100)\n\n    \n  \n  \n    \n      0:13:55\n    \n    seq\n    \n      Using seq function to create groups of number of games programmatically\n\n    \n  \n  \n    \n      0:15:05\n    \n    \n    \n      Explanation of using logarithmic scale for this riddle\n\n    \n  \n  \n    \n      0:15:45\n    \n    \n    \n      Changing spacing of number of games from even spacing (20, 40, …, 100) to exponential (doubles every time, 12, 24, 48, …, 1536)\n\n    \n  \n  \n    \n      0:18:00\n    \n    \n    \n      Changing spacing of number of games to be finer\n\n    \n  \n  \n    \n      0:19:00\n    \n    \n    \n      Introduction of interpolation as the last step we will do\n\n    \n  \n  \n    \n      0:19:30\n    \n    approx\n    \n      Introducing approx function as method to linearly interpolate data\n\n    \n  \n  \n    \n      0:22:35\n    \n    \n    \n      Break point for the next riddle\n\n    \n  \n  \n    \n      0:24:30\n    \n    \n    \n      Starting recursive approach to this riddle\n\n    \n  \n  \n    \n      0:25:35\n    \n    matrix\n    \n      Setting up a N x N matrix (N = 4 to start)\n\n    \n  \n  \n    \n      0:25:55\n    \n    \n    \n      Explanation of approach (random ball goes into random cup, represented by matrix)\n\n    \n  \n  \n    \n      0:26:25\n    \n    sample\n    \n      Using sample function to pick a random element of the matrix\n\n    \n  \n  \n    \n      0:27:15\n    \n    \n    \n      Using for loop to iterate random selection 100 times\n\n    \n  \n  \n    \n      0:28:25\n    \n    \n    \n      Converting for loop to while loop, using colSums to keep track of number of balls in cups\n\n    \n  \n  \n    \n      0:30:05\n    \n    \n    \n      Starting to code the pruning phase\n\n    \n  \n  \n    \n      0:30:15\n    \n    \n    \n      Using diag function to pick matching matrix elements (e.g., the 4th row of the 4th column)\n\n    \n  \n  \n    \n      0:31:50\n    \n    \n    \n      Turning code up to this point into a custom simulate_round function\n\n    \n  \n  \n    \n      0:32:25\n    \n    \n    \n      Using custom simulate_round function to simulate 100 rounds\n\n    \n  \n  \n    \n      0:33:30\n    \n    all\n    \n      Using all function to perform logic check on whether all cups in a round are not empty\n\n    \n  \n  \n    \n      0:34:05\n    \n    \n    \n      Converting loop approach to tidy approach\n\n    \n  \n  \n    \n      0:35:10\n    \n    rerunmap_lgl\n    \n      Using rerun and map_lgl functions from purrr package to simulate a round for each for in a dataframe\n\n    \n  \n  \n    \n      0:36:20\n    \n    \n    \n      Explanation of the tidy approach\n\n    \n  \n  \n    \n      0:37:05\n    \n    cumsumlag\n    \n      Using cumsum and lag functions to keep track of the number of rounds until you win a \"game\"\n\n    \n  \n  \n    \n      0:39:45\n    \n    \n    \n      Creating histogram of number of rounds until winning a game\n\n    \n  \n  \n    \n      0:40:10\n    \n    \n    \n      Setting boundary argument of geom_histogram function to include count of zeros\n\n    \n  \n  \n    \n      0:40:30\n    \n    \n    \n      Brief explanation of geometric distribution\n\n    \n  \n  \n    \n      0:41:25\n    \n    \n    \n      Extending custom simulate_round function to include number of balls thrown to win (in addition to whether we won a round)\n\n    \n  \n  \n    \n      0:46:10\n    \n    \n    \n      Extending to two values of N (N = 3 or N = 4)\n\n    \n  \n  \n    \n      0:49:50\n    \n    \n    \n      Reviewing results of N = 3 and N = 4\n\n    \n  \n  \n    \n      0:52:20\n    \n    \n    \n      Extending to N = 5\n\n    \n  \n  \n    \n      0:53:55\n    \n    \n    \n      Checking results of chess riddle with Riddler solution\n\n    \n  \n  \n    \n      0:55:10\n    \n    \n    \n      Checking results of ball-cup riddle with Riddler solution (Dave slightly misinterpreted what the riddle was asking)\n\n    \n  \n  \n    \n      0:56:35\n    \n    \n    \n      Changing simulation code to correct the misinterpretation\n\n    \n  \n  \n    \n      1:01:40\n    \n    \n    \n      Reviewing results of corrected simulation\n\n    \n  \n  \n    \n      1:03:30\n    \n    \n    \n      Checking results of ball-cup riddle with corrected simulation with Riddler solutions\n\n    \n  \n  \n    \n      1:06:00\n    \n    \n    \n      Visualizing number of balls thrown and rounds played"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-branching-process.html",
    "href": "content_pages/riddler-simulating-a-branching-process.html",
    "title": "Riddler: Simulating a Branching Process",
    "section": "",
    "text": "Notable topics: Simulation, Probability distributions (Exponential and Geometric)\nRecorded on: 2020-04-12\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-branching-process.html#screencast",
    "href": "content_pages/riddler-simulating-a-branching-process.html#screencast",
    "title": "Riddler: Simulating a Branching Process",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-branching-process.html#timestamps",
    "href": "content_pages/riddler-simulating-a-branching-process.html#timestamps",
    "title": "Riddler: Simulating a Branching Process",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:0:35\n    \n    \n    \n      Explanation of a Poisson process\n\n    \n  \n  \n    \n      0:2:40\n    \n    \n    \n      Asking \"How long do you have to wait for X to happen?\", which the Exponential distribution can answer\n\n    \n  \n  \n    \n      0:4:20\n    \n    rexp\n    \n      Using rexp function to generate numbers from the Exponential distribution\n\n    \n  \n  \n    \n      0:5:25\n    \n    rexp\n    \n      Using a vector of rates inside the rexp function (to explore consecutive waiting times)\n\n    \n  \n  \n    \n      0:7:05\n    \n    \n    \n      Using cumsum function to calculate total waiting time until hitting a specific number in the Poisson process\n\n    \n  \n  \n    \n      0:7:35\n    \n    which\n    \n      Using which function to determine the first instance > 3 in a vector\n\n    \n  \n  \n    \n      0:9:20\n    \n    replicate\n    \n      Using replicate function to do a quick simulation of the function just written\n\n    \n  \n  \n    \n      0:10:55\n    \n    \n    \n      Discussing methods of making the simulation function faster\n\n    \n  \n  \n    \n      0:12:00\n    \n    crossing\n    \n      Using crossing function to set up \"tidy\" simulation (gives you all possible combinations of values you provide it)\n\n    \n  \n  \n    \n      0:13:15\n    \n    \n    \n      Noting how the consecutive waiting times seems to follow the Harmonic series\n\n    \n  \n  \n    \n      0:17:10\n    \n    \n    \n      Noticing that we are missing trials with 0 comments and fixing\n\n    \n  \n  \n    \n      0:20:25\n    \n    nls\n    \n      Using nls function (non-linear least squares) to test how well the data fits with an exponential curve\n\n    \n  \n  \n    \n      0:23:05\n    \n    \n    \n      Visualizing fit between data and the exponential curve calculated with nls in previous step\n\n    \n  \n  \n    \n      0:23:50\n    \n    augment\n    \n      Using augment function to added fitted values of the nls function\n\n    \n  \n  \n    \n      0:26:00\n    \n    \n    \n      Exploring whether the data actually follows a Geometric distribution\n\n    \n  \n  \n    \n      0:30:55\n    \n    \n    \n      Explanation of the Geometric distribution as it applies to this question\n\n    \n  \n  \n    \n      0:34:05\n    \n    \n    \n      Generalizing the question to ask how long it takes to get to multiple comments (not just 3)\n\n    \n  \n  \n    \n      0:38:45\n    \n    \n    \n      Explanation of why we subtract 1 when fitting an exponential curve\n\n    \n  \n  \n    \n      0:46:00\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-circular-random-walk.html",
    "href": "content_pages/riddler-simulating-a-circular-random-walk.html",
    "title": "Riddler: Simulating a Circular Random Walk",
    "section": "",
    "text": "Notable topics: Simulation\nRecorded on: 2020-11-22\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-circular-random-walk.html#screencast",
    "href": "content_pages/riddler-simulating-a-circular-random-walk.html#screencast",
    "title": "Riddler: Simulating a Circular Random Walk",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-circular-random-walk.html#timestamps",
    "href": "content_pages/riddler-simulating-a-circular-random-walk.html#timestamps",
    "title": "Riddler: Simulating a Circular Random Walk",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:1:25\n    \n    samplecumsum\n    \n      Using sample() and cumsum() to simulate a random walk\n\n    \n  \n  \n    \n      0:2:30\n    \n    %%\n    \n      Using %% (modulo operator) to \"close\" the circle (set the number of people in the circle)\n\n    \n  \n  \n    \n      0:3:40\n    \n    crossing\n    \n      Using crossing function to set up \"tidy\" simulation (gives you all possible combinations of values you provide it)\n\n    \n  \n  \n    \n      0:5:10\n    \n    distinct\n    \n      Using distinct function and its .keep_all argument to get only the first unique set of the variables you give it\n\n    \n  \n  \n    \n      0:8:15\n    \n    \n    \n      Visualizing the number of steps it takes for the sauce to reach people at differents seats\n\n    \n  \n  \n    \n      0:13:40\n    \n    \n    \n      Visualizing the distribution of number of steps it takes to reach each seat\n\n    \n  \n  \n    \n      0:26:30\n    \n    \n    \n      Investigating the parabolic shape of average number of steps to reach a given seat\n\n    \n  \n  \n    \n      0:28:40\n    \n    lmI\n    \n      Using lm and I functions to calculate formula of the parabola describing average number of steps\n\n    \n  \n  \n    \n      0:30:15\n    \n    \n    \n      Starting to vary the size of the table\n\n    \n  \n  \n    \n      0:38:45\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-non-increasing-sequence.html",
    "href": "content_pages/riddler-simulating-a-non-increasing-sequence.html",
    "title": "Riddler: Simulating a Non-increasing Sequence",
    "section": "",
    "text": "Notable topics: Simulation\nRecorded on: 2020-04-05\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-non-increasing-sequence.html#screencast",
    "href": "content_pages/riddler-simulating-a-non-increasing-sequence.html#screencast",
    "title": "Riddler: Simulating a Non-increasing Sequence",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-non-increasing-sequence.html#timestamps",
    "href": "content_pages/riddler-simulating-a-non-increasing-sequence.html#timestamps",
    "title": "Riddler: Simulating a Non-increasing Sequence",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:20\n    \n    accumulate\n    \n      Introducing accumulate functon as a possible solution (but not used here)\n\n    \n  \n  \n    \n      0:3:20\n    \n    sample\n    \n      Using sample function to simulate 1000 rolls of a 10-sided die\n\n    \n  \n  \n    \n      0:3:40\n    \n    \n    \n      Explanation of dividing sample rolls into streaks (instead of using logic similar to a while loop)\n\n    \n  \n  \n    \n      0:4:55\n    \n    cumsum\n    \n      Using cumsum function to separate 1000 rolls into individual sequences (which end when a 0 is rolled)\n\n    \n  \n  \n    \n      0:5:50\n    \n    lag\n    \n      Using lag function to \"shift\" sequence numbering down by one row\n\n    \n  \n  \n    \n      0:7:35\n    \n    cummaxlag\n    \n      Using cummax and lag functions to check whether a roll is less than the highest value rolled previously in the sequence\n\n    \n  \n  \n    \n      0:9:30\n    \n    cummin\n    \n      Fixing previous step with cummin function (instead of cummax) and dropping the lag function\n\n    \n  \n  \n    \n      0:13:05\n    \n    \n    \n      Finished simulation code and starting to calculate scores\n\n    \n  \n  \n    \n      0:13:10\n    \n    row_number\n    \n      Using -row_number function (note the minus sign!) to calculate decimal position of number in the score\n\n    \n  \n  \n    \n      0:15:30\n    \n    \n    \n      Investigating the distribution of scores\n\n    \n  \n  \n    \n      0:16:25\n    \n    seqscale_x_continuous\n    \n      Using seq function in the breaks argument of scale_x_continuous to set custom, evenly-spaced axis ticks and labels"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-week-of-rain.html",
    "href": "content_pages/riddler-simulating-a-week-of-rain.html",
    "title": "Riddler: Simulating a Week of Rain",
    "section": "",
    "text": "Notable topics: Simulation\nRecorded on: 2018-12-11\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-week-of-rain.html#screencast",
    "href": "content_pages/riddler-simulating-a-week-of-rain.html#screencast",
    "title": "Riddler: Simulating a Week of Rain",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-a-week-of-rain.html#timestamps",
    "href": "content_pages/riddler-simulating-a-week-of-rain.html#timestamps",
    "title": "Riddler: Simulating a Week of Rain",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:20\n    \n    crossing\n    \n      Using crossing function to get all combinations of specified variables (100 trials of 5 days)\n\n    \n  \n  \n    \n      0:2:35\n    \n    rbinom\n    \n      Using rbinom function to simulate whether it rains or not\n\n    \n  \n  \n    \n      0:3:15\n    \n    ifelse\n    \n      Using ifelse function to set starting number of umbrellas at beginning of week\n\n    \n  \n  \n    \n      0:4:20\n    \n    \n    \n      Explanation of structure of simulation and approach to determining number of umbrellas in each location\n\n    \n  \n  \n    \n      0:5:30\n    \n    \n    \n      Changing structure so that we have a row for each day's morning or evening\n\n    \n  \n  \n    \n      0:7:10\n    \n    group_byifelserow_number\n    \n      Using group_by, ifelse, and row_number functions to set starting number of umbrellas for each trial\n\n    \n  \n  \n    \n      0:8:45\n    \n    case_when\n    \n      Using case_when function to returns different values for multiple logical checks (allows for more outputs than ifelse)\n\n    \n  \n  \n    \n      0:10:20\n    \n    cumsum\n    \n      Using cumsum function to create a running tally of number of umbrellas in each location\n\n    \n  \n  \n    \n      0:11:25\n    \n    \n    \n      Explanation of output of simulated data\n\n    \n  \n  \n    \n      0:12:30\n    \n    \n    \n      Using any function to check if any day had a negative \"umbrella count\" (indicating there wasn't an umbrella available when raining)\n\n    \n  \n  \n    \n      0:15:40\n    \n    \n    \n      Asking, \"When was the first time Louie got wet?\"\n\n    \n  \n  \n    \n      0:17:10\n    \n    \n    \n      Creating a custom vector to convert an integer to a weekday (e.g., 2 = Tue)"
  },
  {
    "objectID": "content_pages/riddler-simulating-and-optimizing-coin-flipping.html",
    "href": "content_pages/riddler-simulating-and-optimizing-coin-flipping.html",
    "title": "Riddler: Simulating and Optimizing Coin Flipping",
    "section": "",
    "text": "Notable topics: Simulation\nRecorded on: 2020-05-02\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-simulating-and-optimizing-coin-flipping.html#screencast",
    "href": "content_pages/riddler-simulating-and-optimizing-coin-flipping.html#screencast",
    "title": "Riddler: Simulating and Optimizing Coin Flipping",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-and-optimizing-coin-flipping.html#timestamps",
    "href": "content_pages/riddler-simulating-and-optimizing-coin-flipping.html#timestamps",
    "title": "Riddler: Simulating and Optimizing Coin Flipping",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:15\n    \n    crossing\n    \n      Using crossing function to set up \"tidy\" simulation (gives you all possible combinations of values you provide it)\n\n    \n  \n  \n    \n      0:3:00\n    \n    rbinom\n    \n      Using rbinom function to simulate the number of prisoners who choose to flip, then using rbinom again to simulate number of tails\n\n    \n  \n  \n    \n      0:7:20\n    \n    dbinom\n    \n      Using dbinom function (probability mass function) to see probabilities of any given number of prisoners choosing to flip\n\n    \n  \n  \n    \n      0:10:15\n    \n    map_dbl\n    \n      Using map_dbl function to iterate a function, making sure to return a dbl-class object\n\n    \n  \n  \n    \n      0:11:25\n    \n    seq_len\n    \n      Using seq_len(n) instead of 1:n to be slightly more efficient\n\n    \n  \n  \n    \n      0:12:20\n    \n    optimise\n    \n      Using optimise function to conduct single-dimension optimisation (for analytical solution to this question)\n\n    \n  \n  \n    \n      0:14:15\n    \n    \n    \n      Using backticks (like this) for inline R functions in RMarkdown\n\n    \n  \n  \n    \n      0:15:15\n    \n    \n    \n      Starting the Extra Credit portion of the problem (N prisoners instead of 4)\n\n    \n  \n  \n    \n      0:16:30\n    \n    map2_dbl\n    \n      Using map2_dbl function to iterate a function that requires two inputs (and make sure it returns a dbl-class object)\n\n    \n  \n  \n    \n      0:20:05\n    \n    \n    \n      Reviewing visualisation of probabilties with a varying numbers of prisoners\n\n    \n  \n  \n    \n      0:21:30\n    \n    \n    \n      Tweaking graph to look nicer\n\n    \n  \n  \n    \n      0:22:00\n    \n    \n    \n      Get the exact optimal probability value for each number of prisoners\n\n    \n  \n  \n    \n      0:22:45\n    \n    optimise\n    \n      Troubleshooting optimise function to work when iterated over different numbers of prisoners\n\n    \n  \n  \n    \n      0:23:45\n    \n    unnest_wider\n    \n      Using unnest_wider function to disaggregate a list, but put different elements on separate columns (not separate rows, which unnest does\n\n    \n  \n  \n    \n      0:25:30\n    \n    \n    \n      Explanation of what happens to probabilities as number of prisoners increases\n\n    \n  \n  \n    \n      0:29:45\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-replacing-die-sides.html",
    "href": "content_pages/riddler-simulating-replacing-die-sides.html",
    "title": "Riddler: Simulating Replacing Die Sides",
    "section": "",
    "text": "Notable topics: accumulate() function for simulation\nRecorded on: 2020-03-29\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-simulating-replacing-die-sides.html#screencast",
    "href": "content_pages/riddler-simulating-replacing-die-sides.html#screencast",
    "title": "Riddler: Simulating Replacing Die Sides",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-simulating-replacing-die-sides.html#timestamps",
    "href": "content_pages/riddler-simulating-replacing-die-sides.html#timestamps",
    "title": "Riddler: Simulating Replacing Die Sides",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:0:45\n    \n    \n    \n      Explaining why the recursive nature of this problem is well-suited to simulation\n\n    \n  \n  \n    \n      0:2:05\n    \n    accumulate\n    \n      Introducing the accumulate function as a tool for simulation\n\n    \n  \n  \n    \n      0:3:50\n    \n    \n    \n      Creating a condition to call the done function\n\n    \n  \n  \n    \n      0:7:00\n    \n    replicate\n    \n      After creating a function to simulate one round of the problem, using replicate function to run simulation many times\n\n    \n  \n  \n    \n      0:7:15\n    \n    qplot\n    \n      Using qplot function to quickly create a histogram of simulations\n\n    \n  \n  \n    \n      0:7:40\n    \n    \n    \n      Making observations on the distribution of simulations (looks kind of like a gamma distribution)\n\n    \n  \n  \n    \n      0:10:05\n    \n    \n    \n      Observing that the distribution is kind of log-normal (but that doesn't really apply because we're using integers)\n\n    \n  \n  \n    \n      0:10:35\n    \n    tablesort\n    \n      Using table and sort functions to find the most common number of rolls\n\n    \n  \n  \n    \n      0:11:20\n    \n    \n    \n      Starting the Extra Credit portion of the problem (N-sided die)\n\n    \n  \n  \n    \n      0:11:40\n    \n    crossing\n    \n      Using the crossing function to set up a tibble to run simulations\n\n    \n  \n  \n    \n      0:12:35\n    \n    map_dbl\n    \n      Using map_dbl function to apply a set of simulations to each possibility of N sides\n\n    \n  \n  \n    \n      0:13:30\n    \n    \n    \n      Spotting an error in the formula for simulating one round (6-sided die was hard-coded)\n\n    \n  \n  \n    \n      0:16:40\n    \n    lm\n    \n      Using simple linear regression with the lm function to find the relationship between number of sides and average number of rolls\n\n    \n  \n  \n    \n      0:17:20\n    \n    \n    \n      Reviewing distributions for different N-sided dice\n\n    \n  \n  \n    \n      0:18:00\n    \n    \n    \n      Calculating variance, standard deviation, and coefficient of variation to get hints on the distribution (and ruling out Poisson)"
  },
  {
    "objectID": "content_pages/riddler-spelling-bee-honeycomb.html",
    "href": "content_pages/riddler-spelling-bee-honeycomb.html",
    "title": "Riddler: Spelling Bee Honeycomb",
    "section": "",
    "text": "Notable topics: Simulation with matrixes\nRecorded on: 2020-01-05\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/riddler-spelling-bee-honeycomb.html#screencast",
    "href": "content_pages/riddler-spelling-bee-honeycomb.html#screencast",
    "title": "Riddler: Spelling Bee Honeycomb",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/riddler-spelling-bee-honeycomb.html#timestamps",
    "href": "content_pages/riddler-spelling-bee-honeycomb.html#timestamps",
    "title": "Riddler: Spelling Bee Honeycomb",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:00\n    \n    read_lines\n    \n      Using read_lines function to import a plain text file (.txt)\n\n    \n  \n  \n    \n      0:2:35\n    \n    str_detect\n    \n      Using str_detect function to filter out words that do not contain the letter \"g\"\n\n    \n  \n  \n    \n      0:3:25\n    \n    str_split\n    \n      Using str_split function to get a list of a word's individual letters\n\n    \n  \n  \n    \n      0:3:55\n    \n    setdiff\n    \n      Using setdiff function to find words with invalid letters (letters that are not in the puzzle honeycomb) -- also needs map function (at 4:35)\n\n    \n  \n  \n    \n      0:10:45\n    \n    \n    \n      Changing existing code to make a function that will calculate scores for letter combinations\n\n    \n  \n  \n    \n      0:14:10\n    \n    n_distinct\n    \n      Noticing the rule about bonus points for pangrams and using n_distinct function to determine if a word gets those points\n\n    \n  \n  \n    \n      0:17:25\n    \n    mappurrr\n    \n      Using map function to eliminate duplicate letters from each word's list of component letters\n\n    \n  \n  \n    \n      0:25:55\n    \n    acastreshape2\n    \n      Using acast function from reshape2 package to create a matrix of words by letters\n\n    \n  \n  \n    \n      0:27:50\n    \n    \n    \n      Using a the words/letters matrix to find valid words for a given letter combination\n\n    \n  \n  \n    \n      0:29:55\n    \n    %*%\n    \n      Using the matrix multiplication operator %*% to find the number of \"forbidden\" letters for each word\n\n    \n  \n  \n    \n      0:42:05\n    \n    microbenchmarkmicrobenchmark\n    \n      Using microbenchmark function from microbenchmark package to test how long it takes to run a function\n\n    \n  \n  \n    \n      0:43:35\n    \n    combn\n    \n      Using combn function to get the actual combinations of 6 letters (not just the count)\n\n    \n  \n  \n    \n      0:45:15\n    \n    map\n    \n      Using map function to get scores for different combinations of letters created above\n\n    \n  \n  \n    \n      0:47:30\n    \n    which.max\n    \n      Using which.max function to find the position of the max value in a vector\n\n    \n  \n  \n    \n      1:05:10\n    \n    t\n    \n      Using t function to transpose a matrix\n\n    \n  \n  \n    \n      1:19:15\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/seattle-bike-counts.html",
    "href": "content_pages/seattle-bike-counts.html",
    "title": "Seattle Bike Counts",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-04-04\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/seattle-bike-counts.html#screencast",
    "href": "content_pages/seattle-bike-counts.html#screencast",
    "title": "Seattle Bike Counts",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/seattle-bike-counts.html#timestamps",
    "href": "content_pages/seattle-bike-counts.html#timestamps",
    "title": "Seattle Bike Counts",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:6:15\n    \n    summarise_allsummarise_at\n    \n      Using summarise_all / summarise_at function to aggregate multiple variables at the same time\n\n    \n  \n  \n    \n      0:8:15\n    \n    \n    \n      Using magnitude instead of absolute numbers to see trends in time of day\n\n    \n  \n  \n    \n      0:12:00\n    \n    between\n    \n      Dividing time into categories (four categories for times of day, e.g., morning commute, night) using between function\n\n    \n  \n  \n    \n      0:15:00\n    \n    \n    \n      Looking for systematically missing data (which would bias the results of the analysis)\n\n    \n  \n  \n    \n      0:19:45\n    \n    \n    \n      Summarising using a filter in the arguments based on whether the time window is during a commute time\n\n    \n  \n  \n    \n      0:22:45\n    \n    as.difftimelubridate\n    \n      Combining day of week and hour using functions in the lubridate package and as.difftime function (but then he uses facetting as an easier method)\n\n    \n  \n  \n    \n      0:26:30\n    \n    \n    \n      Normalizing day of week data to percent of weekly traffic\n\n    \n  \n  \n    \n      0:42:00\n    \n    \n    \n      Starting analysis of directions of travel by time of day (commute vs. reverse-commute)\n\n    \n  \n  \n    \n      0:43:45\n    \n    wdaylubridate\n    \n      Filtering out weekend days using wday function from lubridate package\n\n    \n  \n  \n    \n      0:45:30\n    \n    spread\n    \n      Using spread function to create new variable of ratio of bike counts at different commute times\n\n    \n  \n  \n    \n      0:47:30\n    \n    \n    \n      Visualizing ratio of bike counts by time of day\n\n    \n  \n  \n    \n      0:50:15\n    \n    \n    \n      Visualizing ratio by hour instead of time of day\n\n    \n  \n  \n    \n      0:52:50\n    \n    \n    \n      Ordering crossing in graph by when the average trip happened using mean of hour weighted by bike count\n\n    \n  \n  \n    \n      0:54:50\n    \n    mutate\n    \n      Quick and dirty filter when creating a new variable within a mutate function"
  },
  {
    "objectID": "content_pages/seattle-pet-names.html",
    "href": "content_pages/seattle-pet-names.html",
    "title": "Seattle Pet Names",
    "section": "",
    "text": "Notable topics: Hypergeometric hypothesis testing, Adjusting for multiple hypothesis testing\nRecorded on: 2019-03-15\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/seattle-pet-names.html#screencast",
    "href": "content_pages/seattle-pet-names.html#screencast",
    "title": "Seattle Pet Names",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/seattle-pet-names.html#timestamps",
    "href": "content_pages/seattle-pet-names.html#timestamps",
    "title": "Seattle Pet Names",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:2:40\n    \n    mdylubridate\n    \n      Using mdy function from lubridate package to convert character-formatted date to date-class\n\n    \n  \n  \n    \n      0:4:20\n    \n    geom_col\n    \n      Exploratory bar graph showing top species of cats, using geom_col function\n\n    \n  \n  \n    \n      0:6:30\n    \n    facet_wrap\n    \n      Specifying facet_wrap function's ncol argument to get graphs stacked vertically (instead of side-by-side)\n\n    \n  \n  \n    \n      0:9:55\n    \n    \n    \n      Asking, \"Are some animal names associated with particular dog breeds?\"\n\n    \n  \n  \n    \n      0:11:15\n    \n    add_count\n    \n      Explanation of add_count function\n\n    \n  \n  \n    \n      0:12:35\n    \n    \n    \n      Adding up various metrics (e.g., number of names overall, number of breeds overall), but note a mistake that gets fixed at 17:05\n\n    \n  \n  \n    \n      0:16:10\n    \n    \n    \n      Calculating a ratio for names that appear over-represented within a breed, then explaining how small samples can be misleading\n\n    \n  \n  \n    \n      0:17:05\n    \n    \n    \n      Spotting and fixing an aggregation mistake\n\n    \n  \n  \n    \n      0:17:55\n    \n    \n    \n      Explanation of how to investigate which names might be over-represented within a breed\n\n    \n  \n  \n    \n      0:18:55\n    \n    \n    \n      Explanation of how to use hypergeometric distribution to test for name over-representation\n\n    \n  \n  \n    \n      0:20:40\n    \n    phyper\n    \n      Using phyper function to calculate p-values for a one-sided hypergeometric test\n\n    \n  \n  \n    \n      0:23:30\n    \n    \n    \n      Additional explanation of hypergeometric distribution\n\n    \n  \n  \n    \n      0:24:00\n    \n    \n    \n      First investigation of why and how to interpret a p-value histogram (second at 29:45, third at 37:45, and answer at 39:30)\n\n    \n  \n  \n    \n      0:25:15\n    \n    \n    \n      Noticing that we are missing zeros (i.e., having a breed/name combination with 0 dogs), which is important for the hypergeometric test\n\n    \n  \n  \n    \n      0:27:10\n    \n    complete\n    \n      Using complete function to turn implicit zeros (for breed/name combination) into explicit zeros\n\n    \n  \n  \n    \n      0:29:45\n    \n    \n    \n      Second investigation of p-value histogram (after adding in implicit zeros)\n\n    \n  \n  \n    \n      0:31:55\n    \n    p.adjust\n    \n      Explanation of multiple hypothesis testing and correction methods (e.g., Bonferroni, Holm), and applying using p.adjust function\n\n    \n  \n  \n    \n      0:34:25\n    \n    p.adjust\n    \n      Explanation of False Discovery Rate (FDR) control as a method for correcting for multiple hypothesis testing, and applying using p.adjust function\n\n    \n  \n  \n    \n      0:37:45\n    \n    \n    \n      Third investigation of p-value histogram, to hunt for under-represented names\n\n    \n  \n  \n    \n      0:39:30\n    \n    \n    \n      Answer to why the p-value distribution is not well-behaved\n\n    \n  \n  \n    \n      0:42:40\n    \n    crossing\n    \n      Using crossing function to created a simulated dataset to explore how different values affect the p-value\n\n    \n  \n  \n    \n      0:44:55\n    \n    \n    \n      Explanation of how total number of names and total number of breeds affects p-value\n\n    \n  \n  \n    \n      0:46:00\n    \n    \n    \n      More general explanation of what different shapes of p-value histogram might indicate\n\n    \n  \n  \n    \n      0:47:30\n    \n    transmute\n    \n      Renaming variables within a transmute function, using backticks to get names with spaces in them\n\n    \n  \n  \n    \n      0:49:20\n    \n    kableknitr\n    \n      Using kable function from the knitr package to create a nice-looking table\n\n    \n  \n  \n    \n      0:50:00\n    \n    \n    \n      Explanation of one-side p-value (as opposed to two-sided p-value)\n\n    \n  \n  \n    \n      0:53:55\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/simpsons-guest-stars.html",
    "href": "content_pages/simpsons-guest-stars.html",
    "title": "Simpsons Guest Stars",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package)\nRecorded on: 2019-08-29\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/simpsons-guest-stars.html#screencast",
    "href": "content_pages/simpsons-guest-stars.html#screencast",
    "title": "Simpsons Guest Stars",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/simpsons-guest-stars.html#timestamps",
    "href": "content_pages/simpsons-guest-stars.html#timestamps",
    "title": "Simpsons Guest Stars",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:15\n    \n    str_detect\n    \n      Using str_detect function to find guests that played themselves\n\n    \n  \n  \n    \n      0:7:55\n    \n    separate_rows\n    \n      Using separate_rows function and regex to get delimited values onto different rows (e.g., \"Edna Krabappel; Ms. Melon\" gets split into two rows)\n\n    \n  \n  \n    \n      0:9:55\n    \n    parse_number\n    \n      Using parse_number function to convert a numeric variable coded as character to a proper numeric variable\n\n    \n  \n  \n    \n      0:14:45\n    \n    \n    \n      Downloading and importing supplementary dataset of dialogue\n\n    \n  \n  \n    \n      0:16:10\n    \n    semi_join\n    \n      Using semi_join function to filter dataframe based on values that appear in another dataframe\n\n    \n  \n  \n    \n      0:18:05\n    \n    anti_join\n    \n      Using anti_join function to check which values in a dataframe do not appear in another dataframe\n\n    \n  \n  \n    \n      0:20:50\n    \n    ifelse\n    \n      Using ifelse function to recode a single value with another (i.e., \"Edna Krapabbel\" becomes \"Edna Krabappel-Flanders\")\n\n    \n  \n  \n    \n      0:26:20\n    \n    \n    \n      Explaining the goal of all the data cleaning steps\n\n    \n  \n  \n    \n      0:31:25\n    \n    sample\n    \n      Using sample function to get an example line for each character\n\n    \n  \n  \n    \n      0:33:20\n    \n    geom_histogram\n    \n      Setting geom_histogram function's binwidth and center arguments to get specific bin sizes\n\n    \n  \n  \n    \n      0:37:25\n    \n    unnest_tokensanti_jointidytext\n    \n      Using unnest_tokens and anti_join functions from tidytext package to split dialogue into individual words and remove stop words (e.g., \"the\", \"or\", \"and\")\n\n    \n  \n  \n    \n      0:38:55\n    \n    bind_tf_idftidytext\n    \n      Using bind_tf_idf function from tidytext package to get the TF-IDF (term frequency-inverse document frequency) of individual words\n\n    \n  \n  \n    \n      0:42:50\n    \n    top_n\n    \n      Using top_n function to get the top 1 TF-IDF value for each role\n\n    \n  \n  \n    \n      0:44:05\n    \n    paste0\n    \n      Using paste0 function to combine two character variables (e.g., \"Groundskeeper Willie\" and \"ach\" (separate variables) become \"Groundskeeper Willie: ach\")\n\n    \n  \n  \n    \n      0:48:10\n    \n    \n    \n      Explanation of what TF-IDF (text frequency-inverse document frequency) tells us and how it is a \"catchphrase detector\"\n\n    \n  \n  \n    \n      0:56:40\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/space-launches.html",
    "href": "content_pages/space-launches.html",
    "title": "Space Launches",
    "section": "",
    "text": "Notable topics: Graphing for EDA (Exploratory Data Analysis)\nRecorded on: 2019-01-14\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/space-launches.html#screencast",
    "href": "content_pages/space-launches.html#screencast",
    "title": "Space Launches",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/space-launches.html#timestamps",
    "href": "content_pages/space-launches.html#timestamps",
    "title": "Space Launches",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:40\n    \n    str_detect\n    \n      Using str_detect function to find missions with \"Apollo\" in their name\n\n    \n  \n  \n    \n      0:6:20\n    \n    \n    \n      Starting EDA (exploratory data analysis)\n\n    \n  \n  \n    \n      0:15:10\n    \n    fct_collapse\n    \n      Using fct_collapse function to recode factors (similar to case_when function)\n\n    \n  \n  \n    \n      0:16:45\n    \n    countrycodecountrycode\n    \n      Using countrycode function from countrycode package to get full country names from country codes (e.g. \"RU\" becomes \"Russia\")\n\n    \n  \n  \n    \n      0:18:15\n    \n    \n    \n      Using replace_na function to convert NA (missing) observations to \"Other\"\n\n    \n  \n  \n    \n      0:19:10\n    \n    geom_line\n    \n      Creating a line graph using geom_line function with different colours for different categories\n\n    \n  \n  \n    \n      0:21:05\n    \n    fct_reorder\n    \n      Using fct_reorder function to reorder factors in line graph above, in order to make legend more readable\n\n    \n  \n  \n    \n      0:32:00\n    \n    geom_col\n    \n      Creating a bar graph, using geom_col function, of most active (by number of launches) private or startup agencies\n\n    \n  \n  \n    \n      0:35:05\n    \n    %/%\n    \n      Using truncated division operator %/% to bin data into decades\n\n    \n  \n  \n    \n      0:35:35\n    \n    complete\n    \n      Using complete function to turn implicit zeros into explicit zeros (makes for a cleaner line graph)\n\n    \n  \n  \n    \n      0:37:15\n    \n    facet_wrap\n    \n      Using facet_wrap function to create small multiples of a line graph, then proceeding to tweak the graph\n\n    \n  \n  \n    \n      0:42:50\n    \n    semi_join\n    \n      Using semi_join function as a filtering step\n\n    \n  \n  \n    \n      0:43:15\n    \n    geom_point\n    \n      Using geom_point to create a timeline of launches by vehicle type\n\n    \n  \n  \n    \n      0:47:20\n    \n    \n    \n      Explanation of why boxplots over time might not be a good visualization choice\n\n    \n  \n  \n    \n      0:48:00\n    \n    geom_jitter\n    \n      Using geom_jitter function to tweak the timeline graph to be more readable\n\n    \n  \n  \n    \n      0:51:30\n    \n    \n    \n      Creating a second timeline graph for US vehicles and launches\n\n    \n  \n  \n    \n      0:56:35\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/student-teacher-ratios.html",
    "href": "content_pages/student-teacher-ratios.html",
    "title": "Student-Teacher Ratios",
    "section": "",
    "text": "Notable topics: WDI package (World Development Indicators)\nRecorded on: 2019-05-09\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/student-teacher-ratios.html#screencast",
    "href": "content_pages/student-teacher-ratios.html#screencast",
    "title": "Student-Teacher Ratios",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/student-teacher-ratios.html#timestamps",
    "href": "content_pages/student-teacher-ratios.html#timestamps",
    "title": "Student-Teacher Ratios",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:7:30\n    \n    slice\n    \n      Using slice function to select 10 highest and 10 lowest student-teacher ratios (like a filter using row numbers)\n\n    \n  \n  \n    \n      0:12:35\n    \n    WDI\n    \n      Adding GDP per capita to a dataset using WDI package\n\n    \n  \n  \n    \n      0:17:40\n    \n    geom_text\n    \n      Using geom_text to add labels to points on a scatterplot\n\n    \n  \n  \n    \n      0:19:00\n    \n    WDIsearchWDI\n    \n      Using WDIsearch function from WDI package to search for country population data\n\n    \n  \n  \n    \n      0:23:20\n    \n    \n    \n      Explanation of trick with geom_text function's check_overlap argument to get label for US to appear by rearranging row order\n\n    \n  \n  \n    \n      0:25:45\n    \n    comma_formatscales\n    \n      Using comma_format function from scales format to get more readable numeric legend (e.g., \"500,000,000\" instead of \"5e+08\")\n\n    \n  \n  \n    \n      0:27:55\n    \n    WDI\n    \n      Exploring different education-related indicators in the WDI package\n\n    \n  \n  \n    \n      0:31:55\n    \n    spreadpivot_wider\n    \n      Using spread function (now pivot_wider) to turn data from tidy to wide format\n\n    \n  \n  \n    \n      0:32:15\n    \n    to_snake_casesnakecase\n    \n      Using to_snake_case function from snakecase package to conver field names to snake_case\n\n    \n  \n  \n    \n      0:48:30\n    \n    \n    \n      Exploring female/male school secondary school enrollment\n\n    \n  \n  \n    \n      0:51:50\n    \n    \n    \n      Note of caution on keeping confounders in mind when interpreting scatterplots\n\n    \n  \n  \n    \n      0:52:30\n    \n    lm\n    \n      Creating a linear regression of secondary school enrollment to explore confounders\n\n    \n  \n  \n    \n      0:54:30\n    \n    \n    \n      Discussing the actual confounder (GDP per capita) in the linear regression above\n\n    \n  \n  \n    \n      0:57:20\n    \n    \n    \n      Adding world region as another potential confounder\n\n    \n  \n  \n    \n      0:58:00\n    \n    aov\n    \n      Using aov function (ANOVA) to explore confounders further\n\n    \n  \n  \n    \n      1:06:50\n    \n    \n    \n      Reviewing and interpreting the final linear regression model\n\n    \n  \n  \n    \n      1:08:00\n    \n    cor\n    \n      Using cor function (correlation) to get correlation matrix for three variables (and brief explanation of multi-collinearity)\n\n    \n  \n  \n    \n      1:10:10\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/tennis-tournaments.html",
    "href": "content_pages/tennis-tournaments.html",
    "title": "Tennis Tournaments",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-04-08\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/tennis-tournaments.html#screencast",
    "href": "content_pages/tennis-tournaments.html#screencast",
    "title": "Tennis Tournaments",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/tennis-tournaments.html#timestamps",
    "href": "content_pages/tennis-tournaments.html#timestamps",
    "title": "Tennis Tournaments",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:00\n    \n    \n    \n      Identifying duplicated rows ands fixing them\n\n    \n  \n  \n    \n      0:11:15\n    \n    add_countfct_reorder\n    \n      Using add_count and fct_reorder functions to order categories that are broken down into sub-categories for graphing\n\n    \n  \n  \n    \n      0:13:00\n    \n    str_to_titlestr_replace\n    \n      Tidying graph titles (e.g., replacing underscores with spaces) using str_to_title and str_replace functions\n\n    \n  \n  \n    \n      0:15:00\n    \n    inner_join\n    \n      Using inner_join function to merge datasets\n\n    \n  \n  \n    \n      0:15:30\n    \n    difftimeas.numeric\n    \n      Calculating age from date of birth using difftime and as.numeric functions\n\n    \n  \n  \n    \n      0:16:35\n    \n    \n    \n      Adding simple calculations like mean and median into the text portion of markdown document\n\n    \n  \n  \n    \n      0:17:45\n    \n    \n    \n      Looking at distribution of wins by sex using overlapping histograms\n\n    \n  \n  \n    \n      0:18:55\n    \n    %/%\n    \n      Binning years into decades using truncated division %/%\n\n    \n  \n  \n    \n      0:20:15\n    \n    interaction\n    \n      Splitting up boxplots so that they are separated into pairs (M/F) across a different group (decade) using interaction function\n\n    \n  \n  \n    \n      0:20:30\n    \n    \n    \n      Analyzing distribution of ages across decades, looking specifically at the effect of Serena Williams (one individual having a disproportionate affect on the data, making it look like there's a trend)\n\n    \n  \n  \n    \n      0:24:30\n    \n    \n    \n      Avoiding double-counting of individuals by counting their average age instead of their age at each win\n\n    \n  \n  \n    \n      0:30:20\n    \n    \n    \n      Starting analysis to predict winner of Grand Slam tournaments\n\n    \n  \n  \n    \n      0:35:00\n    \n    row_number\n    \n      Creating rolling count using row_number function to make a count of previous tournament experience\n\n    \n  \n  \n    \n      0:39:45\n    \n    cumsum\n    \n      Creating rolling win count using cumsum function\n\n    \n  \n  \n    \n      0:41:00\n    \n    lag\n    \n      Lagging rolling win count using lag function (otherwise we get information about a win before a player has actually won, for prediction purposes)\n\n    \n  \n  \n    \n      0:43:30\n    \n    \n    \n      Asking, \"When someone is a finalist, what is their probability of winning as a function of previous tournaments won?\"\n\n    \n  \n  \n    \n      0:48:00\n    \n    \n    \n      Asking, \"How does the number of wins a finalist has affect their chance of winning?\"\n\n    \n  \n  \n    \n      0:49:00\n    \n    \n    \n      Backtesting simple classifier where person with more tournament wins is predicted to win the given tournament\n\n    \n  \n  \n    \n      0:51:45\n    \n    \n    \n      Creating classifier that gives points based on how far a player got in previous tournaments\n\n    \n  \n  \n    \n      0:52:55\n    \n    match\n    \n      Using match function to turn name of round reached (1st round, 2nd round, …) into a number score (1, 2, …)\n\n    \n  \n  \n    \n      0:54:20\n    \n    cummean\n    \n      Using cummean function to get score of average past performance (instead of cumsum function)\n\n    \n  \n  \n    \n      1:04:10\n    \n    \n    \n      Pulling names of rounds (1st round, 2nd round, … ) based on the rounded numeric score of previous performance"
  },
  {
    "objectID": "content_pages/thanksgiving-dinner.html",
    "href": "content_pages/thanksgiving-dinner.html",
    "title": "Thanksgiving Dinner",
    "section": "",
    "text": "Notable topics: Survey data, Network graphing\nRecorded on: 2018-11-20\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/thanksgiving-dinner.html#screencast",
    "href": "content_pages/thanksgiving-dinner.html#screencast",
    "title": "Thanksgiving Dinner",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/thanksgiving-dinner.html#timestamps",
    "href": "content_pages/thanksgiving-dinner.html#timestamps",
    "title": "Thanksgiving Dinner",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:10\n    \n    \n    \n      Exploratory bar chart of age distribution (and gender) of survey respondents\n\n    \n  \n  \n    \n      0:7:40\n    \n    \n    \n      Using count function on multiple columns to get detailed counts\n\n    \n  \n  \n    \n      0:11:25\n    \n    \n    \n      Parsing numbers from text using parse_number function, then using those numbers to re-level an ordinal factor (income bands)\n\n    \n  \n  \n    \n      0:13:05\n    \n    \n    \n      Exploring relationship between income and using homemade (vs. canned) cranberry sauce\n\n    \n  \n  \n    \n      0:14:00\n    \n    \n    \n      Adding group = 1 argument to the aes function to properly display a line chart\n\n    \n  \n  \n    \n      0:14:30\n    \n    \n    \n      Rotating text for axis labels that overlap\n\n    \n  \n  \n    \n      0:16:50\n    \n    qbeta\n    \n      Getting confidence intervals for proportions using Jeffreys interval (using beta distribution with an uniformative prior)\n\n    \n  \n  \n    \n      0:17:55\n    \n    \n    \n      Explanation of Clopper-Pearson approach as alternative to Jeffreys interval\n\n    \n  \n  \n    \n      0:18:30\n    \n    geom_ribbon\n    \n      Using geom_ribbon function add shaded region to line chart that shows confidence intervals\n\n    \n  \n  \n    \n      0:21:55\n    \n    starts_with\n    \n      Using starts_with function to select fields with names that start with a certain string (e.g., using \"pie\" selects \"pie1\" and \"pie2\")\n\n    \n  \n  \n    \n      0:22:55\n    \n    gather\n    \n      Using gather function to get wide-format data to tidy (tall) format\n\n    \n  \n  \n    \n      0:23:45\n    \n    str_remove\n    \n      Using str_remove and regex to remove digits from field values (e.g., \"dessert1\" and \"dessert2\" get turned into \"dessert\")\n\n    \n  \n  \n    \n      0:27:00\n    \n    \n    \n      \"What are people eating?\" Graphing pies, sides, and desserts\n\n    \n  \n  \n    \n      0:28:00\n    \n    fct_reorder\n    \n      Using fct_reorder function to reorder foods based on how popular they are\n\n    \n  \n  \n    \n      0:28:45\n    \n    n_distinct\n    \n      Using n_distinct function count the number of unique respondents\n\n    \n  \n  \n    \n      0:30:25\n    \n    \n    \n      Using facet_wrap function to facet food types into their own graphs\n\n    \n  \n  \n    \n      0:32:50\n    \n    parse_number\n    \n      Using parse_number function to convert age ranges as character string into a numeric field\n\n    \n  \n  \n    \n      0:35:35\n    \n    \n    \n      Exploring relationship between US region and food types\n\n    \n  \n  \n    \n      0:36:15\n    \n    \n    \n      Using group_by, then mutate, then count to calculate a complicated summary\n\n    \n  \n  \n    \n      0:40:35\n    \n    \n    \n      Exploring relationship between praying at Thanksgiving (yes/no) and food types\n\n    \n  \n  \n    \n      0:42:30\n    \n    add_ebb_estimateebbr\n    \n      Empirical Bayes binomial estimation for calculating binomial confidence intervals (see Dave's book on Empirical Bayes)\n\n    \n  \n  \n    \n      0:45:30\n    \n    \n    \n      Asking, \"What sides/desserts/pies are eaten together?\"\n\n    \n  \n  \n    \n      0:46:20\n    \n    pairwise_corwidyr\n    \n      Calculating pairwise correlation of food types\n\n    \n  \n  \n    \n      0:49:05\n    \n    ggraphigraph\n    \n      Network graph of pairwise correlation\n\n    \n  \n  \n    \n      0:51:40\n    \n    geom_node_textggraphigraph\n    \n      Adding text labels to nodes using geom_node_text function\n\n    \n  \n  \n    \n      0:53:00\n    \n    theme_void\n    \n      Getting rid of unnecessary graph elements (e.g., axes, gridlines) with theme_void function\n\n    \n  \n  \n    \n      0:53:25\n    \n    \n    \n      Explanation of network graph relationships\n\n    \n  \n  \n    \n      0:55:05\n    \n    \n    \n      Adding dimension to network graph (node colour) to represent the type of food\n\n    \n  \n  \n    \n      0:57:45\n    \n    geom_node_textggraphigraph\n    \n      Fixing overlapping text labels using the geom_node_text function's repel argument\n\n    \n  \n  \n    \n      0:58:55\n    \n    scales\n    \n      Tweaking display of percentage legend to be in more readable format (e.g., \"40%\" instead of \"0.4\")\n\n    \n  \n  \n    \n      1:00:05\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/the-office.html",
    "href": "content_pages/the-office.html",
    "title": "The Office",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package), LASSO regression (glmnet package)\nRecorded on: 2020-03-15\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/the-office.html#screencast",
    "href": "content_pages/the-office.html#screencast",
    "title": "The Office",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/the-office.html#timestamps",
    "href": "content_pages/the-office.html#timestamps",
    "title": "The Office",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:1:45\n    \n    \n    \n      Overview of transcripts data\n\n    \n  \n  \n    \n      0:2:25\n    \n    \n    \n      Overview of ratintgs data\n\n    \n  \n  \n    \n      0:4:10\n    \n    fct_inorder\n    \n      Using fct_inorder function to create a factor with levels based on when they appear in the dataframe\n\n    \n  \n  \n    \n      0:4:50\n    \n    themeelement_text\n    \n      Using theme and element_text to turn axis labels 90 degrees\n\n    \n  \n  \n    \n      0:5:55\n    \n    geom_linegeom_point\n    \n      Creating a line graph with points at each observation (using geom_line and geom_point)\n\n    \n  \n  \n    \n      0:7:10\n    \n    \n    \n      Adding text labels to very high and very low-rated episodes\n\n    \n  \n  \n    \n      0:8:50\n    \n    themeelement_blank\n    \n      Using theme function's panel.grid.major argument to get rid of some extraneous gridlines, using element_blank function\n\n    \n  \n  \n    \n      0:10:15\n    \n    geom_text_repelggrepel\n    \n      Using geom_text_repel from ggrepel package to experiment with different labelling (before abandoning this approach)\n\n    \n  \n  \n    \n      0:12:45\n    \n    row_number\n    \n      Using row_number function to add episode_number field to make graphing easier\n\n    \n  \n  \n    \n      0:14:05\n    \n    \n    \n      Explanation of why number of ratings (votes) is relevant to interpreting the graph\n\n    \n  \n  \n    \n      0:19:10\n    \n    unnest_tokenstidytext\n    \n      Using unnest_tokens function from tidytext package to split full-sentence text field to individual words\n\n    \n  \n  \n    \n      0:20:10\n    \n    anti_join\n    \n      Using anti_join function to filter out stop words (e.g., and, or, the)\n\n    \n  \n  \n    \n      0:22:25\n    \n    str_remove_all\n    \n      Using str_remove_all function to get rid of quotation marks from character names (quirks that might pop up when parsing)\n\n    \n  \n  \n    \n      0:25:40\n    \n    bind_tf_idftidytext\n    \n      Asking, \"Are there words that are specific to certain characters?\" (using bind_tf_idf function)\n\n    \n  \n  \n    \n      0:32:25\n    \n    reorder_withinscale_x_reordered\n    \n      Using reorder_within function to re-order factors within a grouping (when a term appears in multiple groups) and scale_x_reordered function to graph\n\n    \n  \n  \n    \n      0:37:05\n    \n    \n    \n      Asking, \"What effects the popularity of an episode?\"\n\n    \n  \n  \n    \n      0:37:55\n    \n    \n    \n      Dealing with inconsistent episode names between datasets\n\n    \n  \n  \n    \n      0:41:25\n    \n    str_remove\n    \n      Using str_remove function and some regex to remove \"(Parts 1&2)\" from some episode names\n\n    \n  \n  \n    \n      0:42:45\n    \n    str_to_lower\n    \n      Using str_to_lower function to further align episode names (addresses inconsistent capitalization)\n\n    \n  \n  \n    \n      0:52:20\n    \n    \n    \n      Setting up dataframe of features for a LASSO regression, with director and writer each being a feature with its own line\n\n    \n  \n  \n    \n      0:52:55\n    \n    separate_rows\n    \n      Using separate_rows function to separate episodes with multiple writers so that each has their own row\n\n    \n  \n  \n    \n      0:58:25\n    \n    log2\n    \n      Using log2 function to transform number of lines fields to something more useable (since it is log-normally distributed)\n\n    \n  \n  \n    \n      1:00:20\n    \n    cast_sparsetidytext\n    \n      Using cast_sparse function from tidytext package to create a sparse matrix of features by episode\n\n    \n  \n  \n    \n      1:01:55\n    \n    semi_join\n    \n      Using semi_join function as a \"filtering join\"\n\n    \n  \n  \n    \n      1:02:30\n    \n    \n    \n      Setting up dataframes (after we have our features) to run LASSO regression\n\n    \n  \n  \n    \n      1:03:50\n    \n    cv.glmnetglmnet\n    \n      Using cv.glmnet function from glmnet package to run a cross-validated LASSO regression\n\n    \n  \n  \n    \n      1:05:35\n    \n    \n    \n      Explanation of how to pick a lambda penalty parameter\n\n    \n  \n  \n    \n      1:05:55\n    \n    \n    \n      Explanation of output of LASSO model\n\n    \n  \n  \n    \n      1:09:25\n    \n    \n    \n      Outline of why David likes regularized linear models (which is what LASSO is)\n\n    \n  \n  \n    \n      1:10:55\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/tidy-tuesday-tweets.html",
    "href": "content_pages/tidy-tuesday-tweets.html",
    "title": "TidyTuesday Tweets",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package)\nRecorded on: 2019-01-06\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/tidy-tuesday-tweets.html#screencast",
    "href": "content_pages/tidy-tuesday-tweets.html#screencast",
    "title": "TidyTuesday Tweets",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/tidy-tuesday-tweets.html#timestamps",
    "href": "content_pages/tidy-tuesday-tweets.html#timestamps",
    "title": "TidyTuesday Tweets",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:1:20\n    \n    \n    \n      Importing an rds file using read_rds function\n\n    \n  \n  \n    \n      0:2:55\n    \n    floor_datelubridate\n    \n      Using floor_date function from lubridate package to round dates down (that's what the floor part does) to the month level\n\n    \n  \n  \n    \n      0:5:25\n    \n    \n    \n      Asking, \"Which tweets get the most re-tweets?\"\n\n    \n  \n  \n    \n      0:5:50\n    \n    contains\n    \n      Using contains function to select only columns that contain a certain string (\"retweet\" in this case)\n\n    \n  \n  \n    \n      0:8:05\n    \n    \n    \n      Exploring likes/re-tweets ratio, including dealing with one or the other being 0 (which would cause divide by zero error)\n\n    \n  \n  \n    \n      0:11:00\n    \n    \n    \n      Starting exploration of actual text of tweets\n\n    \n  \n  \n    \n      0:11:35\n    \n    unnest_tokenstidytext\n    \n      Using unnest_tokens function from tidytext package to break tweets into individual words (using token argument specifically for tweet-style text)\n\n    \n  \n  \n    \n      0:12:55\n    \n    anti_join\n    \n      Using anti_join function to filter out stop words (e.g., \"and\", \"or\", \"the\") from tokenized data frame\n\n    \n  \n  \n    \n      0:14:45\n    \n    \n    \n      Calculating summary statistics per word (average retweets and likes), then looking at distributions\n\n    \n  \n  \n    \n      0:16:00\n    \n    \n    \n      Explanation of Poisson log normal distribution (number of retweets fits this distribution)\n\n    \n  \n  \n    \n      0:17:45\n    \n    \n    \n      Additional example of Poisson log normal distribution (number of likes)\n\n    \n  \n  \n    \n      0:18:20\n    \n    \n    \n      Explanation of geometric mean as better summary statistic than median or arithmetic mean\n\n    \n  \n  \n    \n      0:25:20\n    \n    floor_datelubridate\n    \n      Using floor_date function from lubridate package to floor dates to the week level and tweaking so that a week starts on Monday (default is Sunday)\n\n    \n  \n  \n    \n      0:30:20\n    \n    \n    \n      Asking, \"What topic is each week about?\" using just the tweet text\n\n    \n  \n  \n    \n      0:31:30\n    \n    bind_tf_idftidytext\n    \n      Calculating TF-IDF of tweets, with week as the \"document\"\n\n    \n  \n  \n    \n      0:33:45\n    \n    top_n\n    \n      Using top_n and group_by functions to select the top tf-idf score for each week\n\n    \n  \n  \n    \n      0:37:55\n    \n    str_detect\n    \n      Using str_detect function to filter out \"words\" that are just numbers (e.g., 16, 36)\n\n    \n  \n  \n    \n      0:41:00\n    \n    distinct\n    \n      Using distinct function with .keep_all argument to ensure only top 1 result, as alternative to top_n function (which includes ties)\n\n    \n  \n  \n    \n      0:42:30\n    \n    \n    \n      Making Jenny Bryan disappointed\n\n    \n  \n  \n    \n      0:42:55\n    \n    geom_text\n    \n      Using geom_text function to add text labels to graph to show to word associated with each week\n\n    \n  \n  \n    \n      0:44:10\n    \n    geom_text_repelggrepel\n    \n      Using geom_text_repel function from ggrepel package as an alternative to geom_text function for adding text labels to graph\n\n    \n  \n  \n    \n      0:46:30\n    \n    rvest\n    \n      Using rvest package to scrape web data from a table in Tidy Tuesday README\n\n    \n  \n  \n    \n      0:51:00\n    \n    \n    \n      Starting to look at #rstats tweets\n\n    \n  \n  \n    \n      0:56:35\n    \n    \n    \n      Spotting signs of fake accounts with purchased followers (lots of hashtags)\n\n    \n  \n  \n    \n      0:59:15\n    \n    \n    \n      Explanation of spotting fake accounts\n\n    \n  \n  \n    \n      1:00:45\n    \n    str_detect\n    \n      Using str_detect to filter out web URLs\n\n    \n  \n  \n    \n      1:03:55\n    \n    str_count\n    \n      Using str_count function and some regex to count how many hashtags a tweet has\n\n    \n  \n  \n    \n      1:07:25\n    \n    \n    \n      Creating a Bland-Altman plot (total on x-axis, variable of interest on y-axis)\n\n    \n  \n  \n    \n      1:08:45\n    \n    geom_text\n    \n      Using geom_text function with check_overlap argument to add labels to scatterplot\n\n    \n  \n  \n    \n      1:12:20\n    \n    \n    \n      Asking, \"Who are the most active #rstats tweeters?\"\n\n    \n  \n  \n    \n      1:15:00\n    \n    \n    \n      Summary of screncast"
  },
  {
    "objectID": "content_pages/tour-de-france.html",
    "href": "content_pages/tour-de-france.html",
    "title": "Tour de France",
    "section": "",
    "text": "Notable topics: Survival analysis, Animated bar graph (gganimate package)\nRecorded on: 2020-04-06\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/tour-de-france.html#screencast",
    "href": "content_pages/tour-de-france.html#screencast",
    "title": "Tour de France",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/tour-de-france.html#timestamps",
    "href": "content_pages/tour-de-france.html#timestamps",
    "title": "Tour de France",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:3:55\n    \n    \n    \n      Getting an overview of the data\n\n    \n  \n  \n    \n      0:8:55\n    \n    %/%\n    \n      Aggregating data into decades using the truncated division operator %/%\n\n    \n  \n  \n    \n      0:21:50\n    \n    \n    \n      Noting that death data is right-censored (i.e., some winners are still alive)\n\n    \n  \n  \n    \n      0:24:05\n    \n    transmute\n    \n      Using transmute function, which combines functionality of mutate (to create new variables) and select (to choose variables to keep)\n\n    \n  \n  \n    \n      0:25:30\n    \n    survfitsurvival\n    \n      Using survfit function from survival package to conduct survival analysis\n\n    \n  \n  \n    \n      0:27:30\n    \n    glancebroom\n    \n      Using glance function from broom package to get a one-row model summary of the survival model\n\n    \n  \n  \n    \n      0:31:00\n    \n    extract\n    \n      Using extract function to pull out a string matching a regular expression from a variable (stage number in this case)\n\n    \n  \n  \n    \n      0:34:30\n    \n    \n    \n      Theorizing that there is a parsing issue with the original data's time field\n\n    \n  \n  \n    \n      0:41:15\n    \n    group_by\n    \n      Using group_by function's built-in \"peeling\" feature, where a summarise call will \"peel away\" one group but left other groupings intact\n\n    \n  \n  \n    \n      0:42:05\n    \n    rankpercent_rank\n    \n      Using rank function, then upgrading to percent_rank function to give percentile rankings (between 0 and 1)\n\n    \n  \n  \n    \n      0:47:50\n    \n    geom_smooth\n    \n      Using geom_smooth function with method argument as \"lm\" to plot a linear regression\n\n    \n  \n  \n    \n      0:48:10\n    \n    cut\n    \n      Using cut function to bin numbers (percentiles in this case) into categories\n\n    \n  \n  \n    \n      0:50:25\n    \n    \n    \n      Reviewing boxplots exploring relationship between first-stage performance and overall Tour performance\n\n    \n  \n  \n    \n      0:51:30\n    \n    gganimate\n    \n      Starting to create an animation using gganimate package\n\n    \n  \n  \n    \n      0:56:00\n    \n    \n    \n      Actually writing the code to create the animation\n\n    \n  \n  \n    \n      0:58:20\n    \n    reorder_withintidytext\n    \n      Using reorder_within function from tidytext package to re-order factors that have the same name across multiple groups\n\n    \n  \n  \n    \n      1:02:40\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/transit-costs.html",
    "href": "content_pages/transit-costs.html",
    "title": "Transit Costs",
    "section": "",
    "text": "Notable topics: EDA (Exploratory Data Analysis) with boxplots, interactive Shiny dashboard\nRecorded on: 2021-01-04\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/transit-costs.html#screencast",
    "href": "content_pages/transit-costs.html#screencast",
    "title": "Transit Costs",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/transit-costs.html#timestamps",
    "href": "content_pages/transit-costs.html#timestamps",
    "title": "Transit Costs",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:45\n    \n    countrycodecountrycode\n    \n      Using countrycode function from countrycode package to convert two-letter country codes to country names\n\n    \n  \n  \n    \n      0:6:20\n    \n    geom_errorbarh\n    \n      Using geom_errorbarh function to visualize start and end times of transit projects\n\n    \n  \n  \n    \n      0:7:15\n    \n    fct_reorder\n    \n      Using fct_reorder function to reorder lines by project midpoint year\n\n    \n  \n  \n    \n      0:9:10\n    \n    as.numeric\n    \n      Using as.numeric to convert character field (real_cost) to proper numeric field\n\n    \n  \n  \n    \n      0:10:20\n    \n    mutate_at\n    \n      Using mutate_at function to apply the same function (as.numeric) to multiple fields in one line of code\n\n    \n  \n  \n    \n      0:13:40\n    \n    geom_boxplotfct_lump\n    \n      Using geom_boxplot and fct_lump to visualize cost per kilometre by country as boxplots\n\n    \n  \n  \n    \n      0:15:35\n    \n    glueglue\n    \n      Using glue function from glue package to combine fields to make easy-to-read labels on a graph\n\n    \n  \n  \n    \n      0:19:15\n    \n    factor\n    \n      Splitting boxplots into whether they are railroads (rr) or not, using factor function and fill argument\n\n    \n  \n  \n    \n      0:24:15\n    \n    \n    \n      Investigating sources of missing data for Shanghai\n\n    \n  \n  \n    \n      0:31:35\n    \n    geom_jitter\n    \n      Using geom_jitter with geom_boxplot to show distribution of items within each group\n\n    \n  \n  \n    \n      0:33:00\n    \n    geom_boxplot\n    \n      Setting geom_boxplot argument outlier.size = -1 as a hack to get rid of boxplot-generated outlier points\n\n    \n  \n  \n    \n      0:40:40\n    \n    shiny\n    \n      Starting to build a shiny app\n\n    \n  \n  \n    \n      0:48:55\n    \n    shiny\n    \n      Review of preliminary shiny app\n\n    \n  \n  \n    \n      0:58:00\n    \n    \n    \n      Screencast summary\n\n    \n  \n  \n    \n      1:00:25\n    \n    \n    \n      Showing how to upload code to GitHub in RStudio"
  },
  {
    "objectID": "content_pages/tv-golden-age.html",
    "href": "content_pages/tv-golden-age.html",
    "title": "TV Golden Age",
    "section": "",
    "text": "Notable topics: Data manipulation, Logistic regression\nRecorded on: 2019-01-08\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/tv-golden-age.html#screencast",
    "href": "content_pages/tv-golden-age.html#screencast",
    "title": "TV Golden Age",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/tv-golden-age.html#timestamps",
    "href": "content_pages/tv-golden-age.html#timestamps",
    "title": "TV Golden Age",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:25\n    \n    \n    \n      Quick tip on how to start exploring a new dataset\n\n    \n  \n  \n    \n      0:7:30\n    \n    \n    \n      Investigating inconsistency of shows having a count of seasons that is different from the number of seasons given in the data\n\n    \n  \n  \n    \n      0:10:10\n    \n    %in%all\n    \n      Using %in% operator and all function to only get shows that have a first season and don't have skipped seasons in the data\n\n    \n  \n  \n    \n      0:15:30\n    \n    \n    \n      Asking, \"Which seasons have the most variation in ratings?\"\n\n    \n  \n  \n    \n      0:20:25\n    \n    facet_wrap\n    \n      Using facet_wrap function to separate different shows on a line graph into multiple small graphs\n\n    \n  \n  \n    \n      0:20:50\n    \n    \n    \n      Writing custom embedded function to get width of breaks on the x-axis to always be even (e.g., season 2, 4, 6, etc.)\n\n    \n  \n  \n    \n      0:23:50\n    \n    \n    \n      Committing, finding, and explaining a common error of using the same variable name when summarizing multiple things\n\n    \n  \n  \n    \n      0:28:20\n    \n    %/%\n    \n      Using truncated division operator %/% to bin data into two-year bins instead of annual (e.g., 1990 and 1991 get binned to 1990)\n\n    \n  \n  \n    \n      0:31:30\n    \n    mutate\n    \n      Using subsetting (with square brackets) within the mutate function to calculate mean on only a subset of data (without needing to filter)\n\n    \n  \n  \n    \n      0:33:50\n    \n    gather\n    \n      Using gather function (now pivot_longer) to get metrics as columns into tidy format, in order to graph them all at once with a facet_wrap\n\n    \n  \n  \n    \n      0:36:30\n    \n    pmin\n    \n      Using pmin function to lump all seasons after 4 into one row (it still shows \"4\", but it represents \"4+\")\n\n    \n  \n  \n    \n      0:39:00\n    \n    \n    \n      Asking, \"If season 1 is good, do you get a second season?\" (show survival)\n\n    \n  \n  \n    \n      0:40:35\n    \n    paste0spread\n    \n      Using paste0 and spread functions to get season 1-3 ratings into three columns, one for each season\n\n    \n  \n  \n    \n      0:42:05\n    \n    distinct\n    \n      Using distinct function with .keep_all argument remove duplicates by only keeping the first one that appears\n\n    \n  \n  \n    \n      0:45:50\n    \n    glm\n    \n      Using logistic regression to answer, \"Does season 1 rating affect the probability of getting a second season?\" (note he forgets to specify the family argument, fixed at 57:25)\n\n    \n  \n  \n    \n      0:48:35\n    \n    ntilecut\n    \n      Using ntile function to divide data into N bins (5 in this case), then eventually using cut function instead\n\n    \n  \n  \n    \n      0:57:00\n    \n    \n    \n      Adding year as an independent variable to the logistic regression model\n\n    \n  \n  \n    \n      0:58:50\n    \n    \n    \n      Adding an interaction term (season 1 interacting with year) to the logistic regression model\n\n    \n  \n  \n    \n      0:59:55\n    \n    augment\n    \n      Using augment function as a method of visualizing and interpreting coefficients of regression model\n\n    \n  \n  \n    \n      1:00:30\n    \n    crossing\n    \n      Using crossing function to create new data to test the logistic regression model on and interpret model coefficients\n\n    \n  \n  \n    \n      1:03:40\n    \n    splines\n    \n      Fitting natural splines using the splines package, which would capture a non-linear relationship\n\n    \n  \n  \n    \n      1:06:15\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/us-dairy-consumption.html",
    "href": "content_pages/us-dairy-consumption.html",
    "title": "US Dairy Consumption",
    "section": "",
    "text": "Notable topics: Time series analysis, Forecasting (sweep package)\nRecorded on: 2019-01-28\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/us-dairy-consumption.html#screencast",
    "href": "content_pages/us-dairy-consumption.html#screencast",
    "title": "US Dairy Consumption",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/us-dairy-consumption.html#timestamps",
    "href": "content_pages/us-dairy-consumption.html#timestamps",
    "title": "US Dairy Consumption",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:2:50\n    \n    \n    \n      Identifying the need for a gather step\n\n    \n  \n  \n    \n      0:4:40\n    \n    str_to_titlestr_replace_all\n    \n      Changing snake case to title case using str_to_title and str_replace_all functions\n\n    \n  \n  \n    \n      0:6:20\n    \n    \n    \n      Identifying need for separating categories into major and minor categories (e.g., \"Cheese Other\" can be divided into \"Cheese\" and \"Other\")\n\n    \n  \n  \n    \n      0:7:10\n    \n    separate\n    \n      Using separate function to split categories into major and minor categories (good explanation of \"extra\" argument, which merges additional separations into one field)\n\n    \n  \n  \n    \n      0:8:20\n    \n    coalesce\n    \n      Using coalesce function to deal with NAs resulting from above step\n\n    \n  \n  \n    \n      0:10:30\n    \n    \n    \n      Dealing with graph of minor category that is linked to multiple major categories (\"Other\" linked to \"Cheese\" and \"Frozen\")\n\n    \n  \n  \n    \n      0:13:10\n    \n    fct_lump\n    \n      Introducing fct_lump function as an approach to work with many categories\n\n    \n  \n  \n    \n      0:14:50\n    \n    facet_wrap\n    \n      Introducing facetting (facet_wrap function) as second alternative to working with many categories\n\n    \n  \n  \n    \n      0:15:50\n    \n    \n    \n      Dealing with \"Other\" category having two parts to it by using ifelse function in the cleaning step (e.g., go from \"Other\" to \"Other Cheese\")\n\n    \n  \n  \n    \n      0:19:45\n    \n    sweep\n    \n      Looking at page for the sweep package\n\n    \n  \n  \n    \n      0:21:20\n    \n    tk_tssweep\n    \n      Using tk_ts function to coerce a tibble to a timeseries\n\n    \n  \n  \n    \n      0:22:10\n    \n    \n    \n      Turning year column (numeric) into a date by adding number of years to Jan 1, 0001\n\n    \n  \n  \n    \n      0:26:00\n    \n    \n    \n      Nesting time series object into each combination of category and product\n\n    \n  \n  \n    \n      0:27:50\n    \n    \n    \n      Applying ETS (Error, Trend, Seasonal) model to each time series\n\n    \n  \n  \n    \n      0:28:10\n    \n    sw_glancesweep\n    \n      Using sw_glance function (sweep package's version of glance function) to pull out model parameters from model field created in above step\n\n    \n  \n  \n    \n      0:29:45\n    \n    sw_augment\n    \n      Using sw_augment function to append fitted values and residuals from the model to the original data\n\n    \n  \n  \n    \n      0:30:50\n    \n    \n    \n      Visualising actual and fitted values on the same graph to get a look at the ETS model\n\n    \n  \n  \n    \n      0:32:10\n    \n    Arima\n    \n      Using Arima function (note the capital A) as alternative to ETS (not sure what difference is between arima and Arima)\n\n    \n  \n  \n    \n      0:35:00\n    \n    sw_sweepforecastsweep\n    \n      Forecasting into the future using an ETS model using various functions: unnest, sw_sweep, forecast\n\n    \n  \n  \n    \n      0:37:45\n    \n    geom_ribbon\n    \n      Using geom_ribbon function to add confidence bounds to forecast\n\n    \n  \n  \n    \n      0:40:20\n    \n    \n    \n      Forecasting using auto-ARIMA (instead of ETS)\n\n    \n  \n  \n    \n      0:40:55\n    \n    crossing\n    \n      Applying two forecasting methods at the same time (auto-ARIMA and ETS) using the crossing function\n\n    \n  \n  \n    \n      0:41:55\n    \n    invoke\n    \n      Quick test of how invoke function works (used to call a function easily, e.g., when it is a character string instead of called directly)\n\n    \n  \n  \n    \n      0:47:35\n    \n    scale_linetype_discrete\n    \n      Removing only one part of legend (line type of solid or dashed) using scale_linetype_discrete function\n\n    \n  \n  \n    \n      0:51:25\n    \n    gather\n    \n      Using gather function to clean up new dataset\n\n    \n  \n  \n    \n      0:52:05\n    \n    fct_recode\n    \n      Using fct_recode to fix a typo in a categorical variable\n\n    \n  \n  \n    \n      0:56:00\n    \n    \n    \n      Copy-pasting previous forecasting code to cheese and reviewing any changes needed\n\n    \n  \n  \n    \n      0:57:20\n    \n    shiny\n    \n      Discussing alternative approach: creating interactive visualisation using shiny package to do direct comparisons"
  },
  {
    "objectID": "content_pages/us-incarceration.html",
    "href": "content_pages/us-incarceration.html",
    "title": "US Incarceration",
    "section": "",
    "text": "Notable topics: Animated map (gganimate package), Dealing with missing data\nRecorded on: 2019-01-24\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/us-incarceration.html#screencast",
    "href": "content_pages/us-incarceration.html#screencast",
    "title": "US Incarceration",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/us-incarceration.html#timestamps",
    "href": "content_pages/us-incarceration.html#timestamps",
    "title": "US Incarceration",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:4:30\n    \n    facet_wrap\n    \n      Creating a facetted (small multiples) line graph of incarceration rate by urbanicity and race over time\n\n    \n  \n  \n    \n      0:7:45\n    \n    \n    \n      Discussion of statistical testing of incarceration rates by urbanicity (e.g., rural, suburban)\n\n    \n  \n  \n    \n      0:11:25\n    \n    \n    \n      Exploring the extent of missing data on prison population\n\n    \n  \n  \n    \n      0:14:15\n    \n    any\n    \n      Using any function to filter down to states that have at least one (hence the any function) row of non-missing data\n\n    \n  \n  \n    \n      0:18:40\n    \n    cut\n    \n      Using cut function to manually bin data along user-specified intervals\n\n    \n  \n  \n    \n      0:24:15\n    \n    \n    \n      Starting to create a choropleth map of incarceration rate by state\n\n    \n  \n  \n    \n      0:26:20\n    \n    match\n    \n      Using match function to match two-letter state abbreviation to full state name, in order to get data needed to create a map\n\n    \n  \n  \n    \n      0:28:00\n    \n    \n    \n      Actually typing the code (now that we have the necessary data) to create a choropleth map\n\n    \n  \n  \n    \n      0:33:05\n    \n    str_remove\n    \n      Using str_remove function and regex to chop off the end of county names (e.g., \"Allen Parish\" becomes \"Allen\")\n\n    \n  \n  \n    \n      0:33:30\n    \n    \n    \n      Making choropleth more specific by drilling down to county-level data\n\n    \n  \n  \n    \n      0:41:10\n    \n    gganimate\n    \n      Starting to make an animated choropleth map using gganimate\n\n    \n  \n  \n    \n      0:42:20\n    \n    %%\n    \n      Using modulo operator %% to choose every 5th year\n\n    \n  \n  \n    \n      0:43:45\n    \n    scale_fill_gradient2\n    \n      Using scale_fill_gradient2 function's limits argument to exclude unusally high values that were blowing out the scale\n\n    \n  \n  \n    \n      0:48:15\n    \n    summarise_at\n    \n      Using summarise_at function to apply the same function to multiple fields at the same time\n\n    \n  \n  \n    \n      0:50:10\n    \n    \n    \n      Starting to investigate missing data (how much is missing, where is it missing, etc.)\n\n    \n  \n  \n    \n      0:54:50\n    \n    \n    \n      Creating a line graph that excludes counties with missing data\n\n    \n  \n  \n    \n      0:57:05\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/us-ph-ds.html",
    "href": "content_pages/us-ph-ds.html",
    "title": "US PhDs",
    "section": "",
    "text": "Notable topics: Data cleaning (getting messy data into tidy format)\nRecorded on: 2019-02-21\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/us-ph-ds.html#screencast",
    "href": "content_pages/us-ph-ds.html#screencast",
    "title": "US PhDs",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/us-ph-ds.html#timestamps",
    "href": "content_pages/us-ph-ds.html#timestamps",
    "title": "US PhDs",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:3:15\n    \n    read_xlsx\n    \n      Using read_xlsx function to read in Excel spreadsheet, including skipping first few rows that don't have data\n\n    \n  \n  \n    \n      0:7:25\n    \n    \n    \n      Overview of starting very messy data\n\n    \n  \n  \n    \n      0:8:20\n    \n    gather\n    \n      Using gather function to clean up wide dataset\n\n    \n  \n  \n    \n      0:9:20\n    \n    fill\n    \n      Using fill function to fill in NA values with a entries in a previous observation\n\n    \n  \n  \n    \n      0:10:10\n    \n    fillifelse\n    \n      Cleaning variable that has number and percent in it, on top of one another using a combination of ifelse and fill functions\n\n    \n  \n  \n    \n      0:12:00\n    \n    spread\n    \n      Using spread function on cleaned data to separate number and percent by year\n\n    \n  \n  \n    \n      0:13:50\n    \n    str_detect\n    \n      Spotted a mistake where he had the wrong string on str_detect function\n\n    \n  \n  \n    \n      0:16:50\n    \n    sample\n    \n      Using sample function to get 6 random fields of study to graph\n\n    \n  \n  \n    \n      0:18:50\n    \n    \n    \n      Cleaning another dataset, which is much easier to clean\n\n    \n  \n  \n    \n      0:19:05\n    \n    \n    \n      Renaming the first field, even without knowing the exact name\n\n    \n  \n  \n    \n      0:21:55\n    \n    \n    \n      Cleaning another dataset\n\n    \n  \n  \n    \n      0:23:10\n    \n    \n    \n      Discussing challenge of when indentation is used in original dataset (for group / sub-group distinction)\n\n    \n  \n  \n    \n      0:25:20\n    \n    \n    \n      Starting to separate out data that is appended to one another in the original dataset (all, male, female)\n\n    \n  \n  \n    \n      0:27:30\n    \n    contains\n    \n      Removing field with long name using contains function\n\n    \n  \n  \n    \n      0:28:10\n    \n    fct_recode\n    \n      Using fct_recode function to rename an oddly-named category in a categorical variable (ifelse function is probably a better alternative)\n\n    \n  \n  \n    \n      0:35:30\n    \n    \n    \n      Discussing solution to broad major field description and fine major field description (meaningfully indented in original data)\n\n    \n  \n  \n    \n      0:39:40\n    \n    setdiff\n    \n      Using setdiff function to separate broad and fine major fields"
  },
  {
    "objectID": "content_pages/us-wind-turbines.html",
    "href": "content_pages/us-wind-turbines.html",
    "title": "US Wind Turbines",
    "section": "",
    "text": "Notable topics: Animated map (gganimate package)\nRecorded on: 2018-11-05\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/us-wind-turbines.html#screencast",
    "href": "content_pages/us-wind-turbines.html#screencast",
    "title": "US Wind Turbines",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/us-wind-turbines.html#timestamps",
    "href": "content_pages/us-wind-turbines.html#timestamps",
    "title": "US Wind Turbines",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:3:50\n    \n    count\n    \n      Using count function to explore categorical variables\n\n    \n  \n  \n    \n      0:5:00\n    \n    geom_point\n    \n      Creating a quick-and-dirty map using geom_point function and latitude and longitude data\n\n    \n  \n  \n    \n      0:6:10\n    \n    coord_mapmapproj\n    \n      Explaining need for mapproj package when plotting maps in ggplot2\n\n    \n  \n  \n    \n      0:7:35\n    \n    borders\n    \n      Using borders function to add US state borders to map\n\n    \n  \n  \n    \n      0:10:45\n    \n    fct_lump\n    \n      Using fct_lump to get the top 6 project categories and put the rest in a lumped \"Other\" category\n\n    \n  \n  \n    \n      0:11:30\n    \n    \n    \n      Changing data so that certain categories' points appear in front of other categories' points on the map\n\n    \n  \n  \n    \n      0:14:15\n    \n    \n    \n      Taking the centroid (average longitude and latitude) of points across a geographic area as a way to aggregate categories to one point\n\n    \n  \n  \n    \n      0:19:40\n    \n    ifelse\n    \n      Using ifelse function to clean missing data that is coded as \"-9999\"\n\n    \n  \n  \n    \n      0:26:00\n    \n    \n    \n      Asking, \"How has turbine capacity changed over time?\"\n\n    \n  \n  \n    \n      0:33:15\n    \n    \n    \n      Exploring different models of wind turbines\n\n    \n  \n  \n    \n      0:38:00\n    \n    mutate_if\n    \n      Using mutate_if function to find NA values (coded as -9999) in multiple columns and replace them with an actual NA\n\n    \n  \n  \n    \n      0:45:40\n    \n    gganimate\n    \n      Reviewing documentation for gganimate package\n\n    \n  \n  \n    \n      0:47:00\n    \n    gganimate\n    \n      Attempting to set up gganimate map\n\n    \n  \n  \n    \n      0:48:55\n    \n    gganimate\n    \n      Understanding gganimate package using a \"Hello World\" / toy example, then trying to debug turbine animation\n\n    \n  \n  \n    \n      0:56:45\n    \n    is.infinite\n    \n      Using is.infinite function to get rid of troublesome Inf values\n\n    \n  \n  \n    \n      0:57:55\n    \n    crossing\n    \n      Quick hack for getting cumulative data from a table using crossing function (though it does end up with some duplication)\n\n    \n  \n  \n    \n      1:01:45\n    \n    gganimate\n    \n      Diagnosis of gganimate issue (points between integer years are being interpolated)\n\n    \n  \n  \n    \n      1:04:35\n    \n    gganimate\n    \n      Pseudo-successful gganimate map (cumulative points show up, but some points are missing)\n\n    \n  \n  \n    \n      1:05:40\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/volcano-eruptions.html",
    "href": "content_pages/volcano-eruptions.html",
    "title": "Volcano Eruptions",
    "section": "",
    "text": "Notable topics: Static map with ggplot2, Interactive map with leaflet, Animated map with gganimate\nRecorded on: 2020-05-11\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/volcano-eruptions.html#screencast",
    "href": "content_pages/volcano-eruptions.html#screencast",
    "title": "Volcano Eruptions",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/volcano-eruptions.html#timestamps",
    "href": "content_pages/volcano-eruptions.html#timestamps",
    "title": "Volcano Eruptions",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:7:00\n    \n    mutatedplyr\n    \n      Change the last_eruption_year into years_ago by using mutate from the dplyr package with years_ago = 2020 - as.numeric(last_eruption_year)). In the plot David includes +1 to account for 0 values in the years_ago variable.\n\n    \n  \n  \n    \n      0:9:50\n    \n    str_detectstringr\n    \n      Use str_detect from the stringr package to search the volcano_name variable for Vesuvius when not sure if spelling is correct.\n\n    \n  \n  \n    \n      0:12:50\n    \n    grom_pointtheme_mapbordersggplot2ggthemes\n    \n      Use the longitude and latitude to create a world map showing where the volcanoes are located.\n\n    \n  \n  \n    \n      0:15:30\n    \n    fct_lumpforcats\n    \n      Use fct_lump from theforcats package to lump together all primary_volcano_type factor levels except for the n most frequent.\n\n    \n  \n  \n    \n      0:16:25\n    \n    str_removestringr\n    \n      Use str_remove from the stringr package with the regular expression \"\\\\(.\\\\)\" to remove the parentheses.\n\n    \n  \n  \n    \n      0:18:30\n    \n    leafletaddTilesaddCircleMarkersleaflet\n    \n      Use the leaflet package to create an interactive map with popup information about each volcano.\n\n    \n  \n  \n    \n      0:24:10\n    \n    glueglue\n    \n      Use glue from the glue package to create an HTML string by concatenating volcano_name and primary_volcano_type between HTML <p></p> tags.\n\n    \n  \n  \n    \n      0:27:15\n    \n    gathernestmapDT\n    \n      Use the DT package to turn the leaflet popup information into a datatable.\n\n    \n  \n  \n    \n      0:31:40\n    \n    str_replacestr_to_titlestringr\n    \n      Use str_replace_all fromt he stringr package to replace all the underscores _ in volcano_name with space. Then use str_to_title from the stringr package to convert the volcano_name variable to title case.\n\n    \n  \n  \n    \n      0:32:05\n    \n    kableknitr\n    \n      Use kable with format = HTML  from the knitr package instead of DT to make turning the data into HTML much easier.\n\n    \n  \n  \n    \n      0:34:05\n    \n    paste0base\n    \n      Use paste0 from base R to bold the Volcano Name, Primary Volcano Type, and Last Eruption Year in the leaflet popup.\n\n    \n  \n  \n    \n      0:34:50\n    \n    replace_natidyr\n    \n      Use replace_na from the tidyr package to replace unknown with NA.\n\n    \n  \n  \n    \n      0:37:15\n    \n    addMeasureleaflet\n    \n      Use addMeasure from the leaflet package to add a tool to the map that allows for the measuring of distance between points.\n\n    \n  \n  \n    \n      0:39:30\n    \n    colorNumericleaflet\n    \n      Use colorNumeric from the leaflet package to color the points based on their population within 5km. To accomplish this, David creates 2 new variables: 1) transformed_pop to get the population on a log2 scale & 2) pop_color which uses the colorNumeric function to generate the color hex values based on transformed_pop.\n\n    \n  \n  \n    \n      0:46:30\n    \n    transition_timeframe_timegganimate\n    \n      Use the gganimate package to create an animated map.\n\n    \n  \n  \n    \n      0:48:45\n    \n    geom_pointggplot2\n    \n      Use geom_point from the ggplot2 package with size = .00001 * 10 ^ vei so the size of the points are then proportional to the volume metrics provided in the Volcano Eruption Index. The metrics are in Km^3.\n\n    \n  \n  \n    \n      0:50:20\n    \n    scale_size_continuousggplot2\n    \n      Use scale_size_continuous from the ggplot2 package with range = c(.1, 6) to make the smaller points smaller and larger points larger.\n\n    \n  \n  \n    \n      0:50:55\n    \n    scale_color_gradient2ggplot2\n    \n      Use scale_color_gradient2 from the ggplot2 package to apply color gradient to each point based on the volcano size and whether its low or high.\n\n    \n  \n  \n    \n      0:59:40\n    \n    transition_revealgganimate\n    \n      Summary of screencast while waiting for gganimate map to render.\nAlso, brief discussion on using transition_reveal instead of transition_time to keep the point on the map instead of replacing them in each frame."
  },
  {
    "objectID": "content_pages/wine-ratings.html",
    "href": "content_pages/wine-ratings.html",
    "title": "Wine Ratings",
    "section": "",
    "text": "Notable topics: Text mining (tidytext package), LASSO regression (glmnet package)\nRecorded on: 2019-05-30\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/wine-ratings.html#screencast",
    "href": "content_pages/wine-ratings.html#screencast",
    "title": "Wine Ratings",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/wine-ratings.html#timestamps",
    "href": "content_pages/wine-ratings.html#timestamps",
    "title": "Wine Ratings",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:3:15\n    \n    extracttidyr\n    \n      Using extract function from tidyr package to pull out year from text field\n\n    \n  \n  \n    \n      0:9:15\n    \n    extracttidyr\n    \n      Changing extract function to pull out year column more accurately\n\n    \n  \n  \n    \n      0:13:00\n    \n    \n    \n      Starting to explore prediction of points\n\n    \n  \n  \n    \n      0:17:00\n    \n    fct_lumpfct_relevel\n    \n      Using fct_lump on country variable to collapse countries into an \"Other\" category, then fct_relevel to set the baseline category for a linear model\n\n    \n  \n  \n    \n      0:21:30\n    \n    \n    \n      Investigating year as a potential confounding variable\n\n    \n  \n  \n    \n      0:24:45\n    \n    \n    \n      Investigating \"taster_name\" as a potential confounding variable\n\n    \n  \n  \n    \n      0:27:45\n    \n    tidybroom\n    \n      Coefficient (TIE fighter) plot to see effect size of terms in a linear model, using tidy function from broom package\n\n    \n  \n  \n    \n      0:30:45\n    \n    str_replace\n    \n      Polishing category names for presentation in graph using str_replace function\n\n    \n  \n  \n    \n      0:32:15\n    \n    augment\n    \n      Using augment function to add predictions of linear model to original data\n\n    \n  \n  \n    \n      0:33:30\n    \n    \n    \n      Plotting predicted points vs. actual points\n\n    \n  \n  \n    \n      0:34:45\n    \n    \n    \n      Using ANOVA to determine the amount of variation that explained by different terms\n\n    \n  \n  \n    \n      0:36:45\n    \n    tidytext\n    \n      Using tidytext package to set up wine review text for Lasso regression\n\n    \n  \n  \n    \n      0:40:00\n    \n    pairwise_corwidyr\n    \n      Setting up and using pairwise_cor function to look at words that appear in reviews together\n\n    \n  \n  \n    \n      0:45:00\n    \n    cast_sparsetidytext\n    \n      Creating sparse matrix using cast_sparse function from tidytext package; used to perform a regression on positive/negative words\n\n    \n  \n  \n    \n      0:46:45\n    \n    \n    \n      Checking if rownames of sparse matrix correspond to the wine_id values they represent\n\n    \n  \n  \n    \n      0:47:00\n    \n    glmnet\n    \n      Setting up sparse matrix for using glmnet package to do sparse regression using Lasso method\n\n    \n  \n  \n    \n      0:48:15\n    \n    glmnet\n    \n      Actually writing code for doing Lasso regression\n\n    \n  \n  \n    \n      0:49:45\n    \n    \n    \n      Basic explanation of Lasso regression\n\n    \n  \n  \n    \n      0:51:00\n    \n    tidy\n    \n      Putting Lasso model into tidy format\n\n    \n  \n  \n    \n      0:53:15\n    \n    \n    \n      Explaining how the number of terms increases as lambda (penalty parameter) decreases\n\n    \n  \n  \n    \n      0:54:00\n    \n    \n    \n      Answering how we choose a lambda value (penalty parameter) for Lasso regression\n\n    \n  \n  \n    \n      0:56:45\n    \n    \n    \n      Using parallelization for intensive computations\n\n    \n  \n  \n    \n      0:58:30\n    \n    \n    \n      Adding price (from original linear model) to Lasso regression\n\n    \n  \n  \n    \n      1:02:15\n    \n    glmnet\n    \n      Shows glmnet.fit piece of a Lasso (glmnet) model\n\n    \n  \n  \n    \n      1:03:30\n    \n    \n    \n      Picking a lambda value (penalty parameter) and explaining which one to pick\n\n    \n  \n  \n    \n      1:08:15\n    \n    \n    \n      Taking most extreme coefficients (positive and negative) by grouping theme by direction\n\n    \n  \n  \n    \n      1:10:30\n    \n    tidytext\n    \n      Demonstrating tidytext package's sentiment lexicon, then looking at individual reviews to demonstrate the model\n\n    \n  \n  \n    \n      1:17:30\n    \n    \n    \n      Visualizing each coefficient's effect on a single review\n\n    \n  \n  \n    \n      1:20:30\n    \n    str_trunc\n    \n      Using str_trunc to truncate character strings"
  },
  {
    "objectID": "content_pages/women-in-the-workplace.html",
    "href": "content_pages/women-in-the-workplace.html",
    "title": "Women in the Workplace",
    "section": "",
    "text": "Notable topics: Interactive scatterplot (plotly and shiny packages)\nRecorded on: 2019-03-04\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/women-in-the-workplace.html#screencast",
    "href": "content_pages/women-in-the-workplace.html#screencast",
    "title": "Women in the Workplace",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/women-in-the-workplace.html#timestamps",
    "href": "content_pages/women-in-the-workplace.html#timestamps",
    "title": "Women in the Workplace",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:5:50\n    \n    \n    \n      Writing a custom function that summarizes variables based on their names (then abandoning the idea)\n\n    \n  \n  \n    \n      0:9:15\n    \n    complete.cases\n    \n      Using complete.cases function to find observations that have an NA value in any variable\n\n    \n  \n  \n    \n      0:9:50\n    \n    \n    \n      Using subsetting within a summarise function to calculate a weighted mean when dealing with 0 or NA values in some observations\n\n    \n  \n  \n    \n      0:12:20\n    \n    \n    \n      Debugging what is causing NA values to appear in the summarise output (finds the error at 13:25)\n\n    \n  \n  \n    \n      0:17:50\n    \n    \n    \n      Hypothesizing about one sector illustrating a variation of Simpson's Paradox\n\n    \n  \n  \n    \n      0:25:25\n    \n    scale_colour_gradient2\n    \n      Creating a scatterplot with a logarithmic scale and using scale_colour_gradient2 function to encode data to point colour\n\n    \n  \n  \n    \n      0:30:00\n    \n    ggplotlyplotly\n    \n      Creating an interactive plot (tooltips show up on hover) using ggplotly function from plotly package\n\n    \n  \n  \n    \n      0:33:20\n    \n    \n    \n      Fiddling with scale_size_continuous function's range argument to specify point size on a scatterplot (which are encoded to total workers)\n\n    \n  \n  \n    \n      0:34:50\n    \n    \n    \n      Explanation of why healthcare sector is a good example of Simpson's Paradox\n\n    \n  \n  \n    \n      0:43:15\n    \n    shiny\n    \n      Starting to create a shiny app with \"occupation\" as only input (many tweaks in subsequent minutes to make it work)\n\n    \n  \n  \n    \n      0:47:55\n    \n    shiny\n    \n      Tweaking size (height) of graph in shiny app\n\n    \n  \n  \n    \n      0:54:05\n    \n    \n    \n      Summary of screencast"
  },
  {
    "objectID": "content_pages/womens-world-cup.html",
    "href": "content_pages/womens-world-cup.html",
    "title": "Women’s World Cup",
    "section": "",
    "text": "Notable topics: NA\nRecorded on: 2019-07-21\nTimestamps by: Alex Cookson\nView code"
  },
  {
    "objectID": "content_pages/womens-world-cup.html#screencast",
    "href": "content_pages/womens-world-cup.html#screencast",
    "title": "Women’s World Cup",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/womens-world-cup.html#timestamps",
    "href": "content_pages/womens-world-cup.html#timestamps",
    "title": "Women’s World Cup",
    "section": "Timestamps",
    "text": "Timestamps\n\n\n  \n    \n      0:2:15\n    \n    countrycode\n    \n      Adding country names using countrycode package\n\n    \n  \n  \n    \n      0:3:45\n    \n    \n    \n      Web scraping country codes from Wikipedia\n\n    \n  \n  \n    \n      0:6:00\n    \n    \n    \n      Combining tables that are separate lists into one dataframe\n\n    \n  \n  \n    \n      0:14:00\n    \n    rev\n    \n      Using rev function (reverse) to turn multiple rows of soccer match scores into one row (base team and opposing team)\n\n    \n  \n  \n    \n      0:26:30\n    \n    geom_smooth\n    \n      Applying a geom_smooth linear model line to a scatter plot, then facetting it\n\n    \n  \n  \n    \n      0:28:30\n    \n    geom_abline\n    \n      Adding a line with a slope of 1 (x = y) using geom_abline\n\n    \n  \n  \n    \n      0:40:00\n    \n    \n    \n      Pulling out elements of a list that is embedded in a dataframe\n\n    \n  \n  \n    \n      1:09:45\n    \n    glueglue\n    \n      Using glue function to add context to facet titles"
  },
  {
    "objectID": "content_pages/x-men-comics.html",
    "href": "content_pages/x-men-comics.html",
    "title": "X-Men Comics",
    "section": "",
    "text": "Notable topics: Data manipulation, Lollipop graph, Grouping using floor division\nRecorded on: 2020-06-29\nTimestamps by: Eric Fletcher\nView code"
  },
  {
    "objectID": "content_pages/x-men-comics.html#screencast",
    "href": "content_pages/x-men-comics.html#screencast",
    "title": "X-Men Comics",
    "section": "Screencast",
    "text": "Screencast"
  },
  {
    "objectID": "content_pages/x-men-comics.html#timestamps",
    "href": "content_pages/x-men-comics.html#timestamps",
    "title": "X-Men Comics",
    "section": "Timestamps",
    "text": "Timestamps\n\n  \n    \n      0:07:25\n    \n    separatetidyr\n    \n      Using separate to separate the name from secrete identity in the character column\n\n    \n  \n  \n    \n      0:09:55\n    \n    summarizeacrossdplyr\n    \n      Using summarize and across to find the frequency of the action variables and find out how many issues each action was used for each character\n\n    \n  \n  \n    \n      0:13:25\n    \n    geom_colfct_reorderggplot2forcats\n    \n      Create a geom_col chart to visualize which character speaks in the most issues\n\n    \n  \n  \n    \n      0:18:35\n    \n    geom_pointgeom_textgeom_text_repelsummarizeggplot2ggrepeldplyr\n    \n      Create a geom_point chart to visualize each character’s average lines per issue in which the character is depicted\n\n    \n  \n  \n    \n      0:22:05\n    \n    geom_pointgeom_textgeom_text_repelsummarizeggplot2ggrepeldplyr\n    \n      Create a geom_point chart to visualize each character’s average thoughts per issue in which the character is depicted\n\n    \n  \n  \n    \n      0:23:10\n    \n    geom_pointgeom_textgeom_text_repelsummarizeggplot2ggrepeldplyr\n    \n      Create a geom_point chart to visualize character’s speech versus thought ratio per issue in which the character is depicted\n\n    \n  \n  \n    \n      0:30:05\n    \n    geom_pointpivot_longerfct_reorderggplot2tidyrforcats\n    \n      Create a geom_point to visualize character’s number of lines while in costume versus not in costume\n\n    \n  \n  \n    \n      0:34:30\n    \n    geom_pointgeom_textgeom_text_repelsummarizeggplot2ggrepeldplyr\n    \n      Create a geom_point chart to visualize the lines in costume versus lines out of costume ratio\n\n    \n  \n  \n    \n      0:39:20\n    \n    geom_pointfct_reordergeom_errorbarhggplot2forcats\n    \n      Create a lollipop graph using geom_point and geom_errorbarh to visualize the lines in costume versus lines out of costume ratio and their distance from 1.0 (1 to 1)\n\n    \n  \n  \n    \n      0:45:00\n    \n    summarizegroup_byarrangedplyr\n    \n      Use summarize to find the frequency of each location and the total number of unique issues where the location is used\n\n    \n  \n  \n    \n      0:46:00\n    \n    summarizefct_lumpdplyrforcats\n    \n      Use summarize and fct_lump to count how many issues each author has written while lumping together all authors except the most frequent\n\n    \n  \n  \n    \n      0:47:25\n    \n    summarizefct_lumpdplyrforcats\n    \n      Use summarize and fct_lump to see if the authors rates of passing the Bechdel test differ from one another\n\n    \n  \n  \n    \n      0:52:45\n    \n    geom_linesummarizedplyr\n    \n      Create a geom_line chart to visualize if the rates of passing the Bechdel test changed over time and floor division %/% to generate 20 observations per group\n\n    \n  \n  \n    \n      0:54:35\n    \n    geom_colsummarizefct_lumpfacet_wrapggplotdplyrforcats\n    \n      Create a geom_col to visualize the amount of lines each character has per issue over time giving context to Bechdel test passing rates\n\n    \n  \n  \n    \n      1:00:00\n    \n    \n    \n      Summary of screencast"
  }
]